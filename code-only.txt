Directory structure:
└── dav-rob-jobchain/
    └── src/
        ├── __init__.py
        └── jobchain/
            ├── __init__.py
            ├── jc_graph.py
            ├── jc_logging.py
            ├── job.py
            ├── job_chain.py
            ├── job_loader.py
            ├── taskmanager.py
            ├── jobs/
            │   ├── __init__.py
            │   ├── default_jobs.py
            │   └── llm_jobs.py
            ├── resources/
            │   ├── __init__.py
            │   └── otel_config.yaml
            └── utils/
                ├── __init__.py
                ├── llm_utils.py
                ├── monitor_utils.py
                ├── otel_wrapper.py
                ├── print_utils.py
                └── timing.py

================================================
File: src/jobchain/__init__.py
================================================
"""
JobChain - A scalable AI job scheduling and execution platform
"""

from .job import JobABC
from .job_chain import JobChain
from . import jc_logging

__all__ = ['JobABC', 'JobChain', 'jc_logging']


================================================
File: src/jobchain/jc_graph.py
================================================
"""
JobChain Graph module for handling directed acyclic graphs with subgraphs.
Provides functionality for graph traversal, cycle detection, and validation.
"""

from typing import Any, Dict, List, Optional, Set, Tuple

from . import jc_logging

logging = jc_logging.getLogger(__name__)


def has_cycle(graph: Dict[str, Dict[str, Any]], node: str, 
             visited: Optional[Set[str]] = None, path: Optional[Set[str]] = None) -> Tuple[bool, List[str]]:
    """
    Check if the graph has a cycle starting from the given node.
    
    Args:
        graph: The graph structure to check
        node: Starting node for cycle detection
        visited: Set of all visited nodes (for recursive calls)
        path: Set of nodes in current path (for cycle detection)
    
    Returns:
        Tuple[bool, List[str]]: (has_cycle, cycle_path)
    """
    if visited is None:
        visited = set()
    if path is None:
        path = set()
    
    visited.add(node)
    path.add(node)
    
    node_obj = graph[node]
    # Check next nodes
    for next_node in node_obj.get('next', []):
        if next_node in path:  # Cycle detected
            return True, [*path, next_node]
        if next_node not in visited:
            has_cycle_result, cycle_path = has_cycle(graph, next_node, visited, path)
            if has_cycle_result:
                return True, cycle_path
    
    # Check subgraph if it exists
    if 'subgraph' in node_obj:
        subgraph = node_obj['subgraph']
        for subnode in subgraph:
            if subnode not in visited:
                has_cycle_result, cycle_path = has_cycle(subgraph, subnode, visited, path)
                if has_cycle_result:
                    return True, cycle_path
    
    path.remove(node)
    return False, []

def check_graph_for_cycles(graph: Dict[str, Dict[str, Any]], name: str = "") -> bool:
    """
    Check entire graph for cycles.
    
    Args:
        graph: The graph structure to check
        name: Optional name for the graph (for logging)
    
    Returns:
        bool: True if cycles were found, False otherwise
    """
    print(f"\nChecking {name} for cycles...")
    for node in graph:
        has_cycle_result, cycle_path = has_cycle(graph, node)
        if has_cycle_result:
            print(f"Cycle detected! Path: {' -> '.join(cycle_path)}")
            return True
    print("No cycles detected")
    return False

def find_node_and_graph(main_graph: Dict[str, Dict[str, Any]], target_node: str, 
                       current_graph: Optional[Dict[str, Dict[str, Any]]] = None) -> Tuple[Optional[Dict[str, Dict[str, Any]]], List[str]]:
    """
    Recursively finds a node and its containing graph in the graph structure.
    
    Args:
        main_graph: The root graph structure
        target_node: The node to find
        current_graph: Current graph being searched (for recursive calls)
    
    Returns:
        Tuple[Optional[Dict], List[str]]: (containing_graph, path_to_node)
    """
    if current_graph is None:
        current_graph = main_graph
        
    # Check if node is in current level
    if target_node in current_graph:
        return current_graph, []
        
    # Search in subgraphs
    for node in current_graph:
        if 'subgraph' in current_graph[node]:
            subgraph = current_graph[node]['subgraph']
            result_graph, path = find_node_and_graph(main_graph, target_node, subgraph)
            if result_graph is not None:
                return result_graph, [node] + path
                
    return None, []

def add_edge(graph: Dict[str, Dict[str, Any]], from_node: str, to_node: str) -> bool:
    """
    Attempts to add an edge between two nodes in the same graph level.
    
    Args:
        graph: The graph containing both nodes
        from_node: Source node
        to_node: Target node
    
    Returns:
        bool: True if edge was added successfully
    """
    if from_node not in graph:
        print(f"Error: Source node {from_node} not found in graph")
        return False
    
    # Initialize 'next' list if it doesn't exist
    if 'next' not in graph[from_node]:
        graph[from_node]['next'] = []
    
    # Check if edge already exists
    if to_node in graph[from_node]['next']:
        print(f"Edge {from_node} -> {to_node} already exists")
        return True
    
    # Temporarily add the edge
    graph[from_node]['next'].append(to_node)
    
    # Check for cycles
    has_cycle_result, cycle_path = has_cycle(graph, from_node)
    
    if has_cycle_result:
        # Remove the edge if it would create a cycle
        graph[from_node]['next'].remove(to_node)
        print(f"Cannot add edge {from_node} -> {to_node} as it would create a cycle")
        print(f"Cycle detected: {' -> '.join(cycle_path)}")
        return False
    
    print(f"Successfully added edge {from_node} -> {to_node}")
    return True

def add_edge_anywhere(main_graph: Dict[str, Dict[str, Any]], from_node: str, to_node: str) -> bool:
    """
    Attempts to add an edge between two nodes in the graph structure.
    
    Rules for edge addition:
    1. Both nodes must exist in the graph structure
    2. Nodes can only reference other nodes within the same graph level:
       - Main graph nodes can only reference other main graph nodes
       - Subgraph nodes can only reference nodes within the same subgraph
    3. No cycles are allowed within any graph level
    
    Args:
        main_graph: The root graph structure
        from_node: Source node
        to_node: Target node
    
    Returns:
        bool: True if edge was added successfully
    """
    # Find the containing graphs for both nodes
    from_graph, from_path = find_node_and_graph(main_graph, from_node)
    to_graph, to_path = find_node_and_graph(main_graph, to_node)
    
    if from_graph is None:
        print(f"Error: Source node {from_node} not found in graph")
        return False
        
    if to_graph is None:
        print(f"Error: Target node {to_node} not found in graph")
        return False
    
    # Check if nodes are in the same graph level
    if from_graph is not to_graph:
        print(f"Error: Cannot create edge between different graph levels")
        print(f"Source node {from_node} is in {' -> '.join(['main'] + from_path) if from_path else 'main graph'}")
        print(f"Target node {to_node} is in {' -> '.join(['main'] + to_path) if to_path else 'main graph'}")
        return False
    
    return add_edge(from_graph, from_node, to_node)

def print_graph(graph: Dict[str, Dict[str, Any]], spaces: int = 0) -> None:
    """
    Traverse and print the graph structure.
    
    Args:
        graph: The graph structure to traverse
        spaces: Number of spaces for indentation
    """
    for key in graph.keys():
        print("." * spaces + key)
        print_visit_node(graph, key, spaces)

def print_visit_node(graph: Dict[str, Dict[str, Any]], key: str, spaces: int = 0) -> None:
    """
    Visit and print a node's details.
    
    Args:
        graph: The graph containing the node
        key: The node key to visit
        spaces: Number of spaces for indentation
    """
    node_key_obj = graph[key]
    sub_graph_obj = node_key_obj.get('subgraph')
    if sub_graph_obj:
        print("-" * (spaces + 2) + "subgraph:")
        print_graph(sub_graph_obj, spaces+2)
        print("-" * (spaces + 2) + "end subgraph.")
    next_obj = node_key_obj.get('next')
    if next_obj:
        print_traverse_list(next_obj, graph, spaces+2)

def print_traverse_list(nodes: List[str], graph: Dict[str, Dict[str, Any]], spaces: int = 0) -> None:
    """
    Traverse and print a list of nodes.
    
    Args:
        nodes: List of node names
        graph: The graph containing the nodes
        spaces: Number of spaces for indentation
    """
    for node in nodes:
        print("." * spaces + " has dependent " + node)

def validate_graph_references(graph: Dict[str, Dict[str, Any]], path: Optional[List[str]] = None) -> Tuple[bool, List[str]]:
    """
    Validates that all node references in a graph structure are within their own graph level.
    This includes the main graph and all subgraphs.
    
    Args:
        graph: The graph structure to validate
        path: Current path in the graph (for error reporting)
        
    Returns:
        tuple: (is_valid, list_of_violations)
        where violations are strings describing each cross-graph reference found
    """
    if path is None:
        path = []
        
    violations = []
    graph_nodes = set(graph.keys())
    
    # Check each node's references
    for node, node_data in graph.items():
        current_path = path + [node] if path else [node]
        
        # Check 'next' references
        next_nodes = node_data.get('next', [])
        for next_node in next_nodes:
            if next_node not in graph_nodes:
                violations.append(
                    f"Node '{' -> '.join(current_path)}' references '{next_node}' "
                    f"which is not in the same graph level"
                )
        
        # Recursively check subgraphs
        if 'subgraph' in node_data:
            subgraph_valid, subgraph_violations = validate_graph_references(
                node_data['subgraph'], 
                current_path
            )
            violations.extend(subgraph_violations)
    
    return len(violations) == 0, violations

def find_head_nodes(graph: Dict[str, Dict[str, Any]]) -> Set[str]:
    """
    Find all nodes that have no incoming edges (head nodes) in a graph.
    
    Args:
        graph: The graph structure to check
        
    Returns:
        Set[str]: Set of nodes that have no incoming edges
    """
    # First collect all nodes that are destinations
    has_incoming_edges = set()
    for node in graph:
        # Check next nodes
        for next_node in graph[node].get('next', []):
            has_incoming_edges.add(next_node)
        # Check subgraph if it exists
        if 'subgraph' in graph[node]:
            subgraph = graph[node]['subgraph']
            # Recursively find head nodes in subgraph
            subgraph_heads = find_head_nodes(subgraph)
            # All nodes in subgraph are considered to have an incoming edge
            # from the parent node that contains the subgraph
            has_incoming_edges.update(subgraph.keys())
    
    # Head nodes are those that exist in the graph but have no incoming edges
    return set(graph.keys()) - has_incoming_edges

def find_tail_nodes(graph: Dict[str, Dict[str, Any]]) -> Set[str]:
    """
    Find all nodes that have no outgoing edges (tail nodes) in a graph.
    
    Args:
        graph: The graph structure to check
        
    Returns:
        Set[str]: Set of nodes that have no outgoing edges
    """
    tail_nodes = set()
    for node, node_data in graph.items():
        has_next = bool(node_data.get('next', []))
        has_subgraph = 'subgraph' in node_data
        
        if not has_next:
            if has_subgraph:
                # If node has no next but has subgraph, the tail nodes are in the subgraph
                subgraph_tails = find_tail_nodes(node_data['subgraph'])
                tail_nodes.update(subgraph_tails)
            else:
                # Node with no next and no subgraph is a tail
                tail_nodes.add(node)
    
    return tail_nodes

def validate_graph(graph: Dict[str, Dict[str, Any]], name: str = "") -> None:
    """
    Performs comprehensive validation of a graph structure.
    Checks for:
    1. Graph cycles
    2. Cross-graph reference violations
    3. Head node requirements (exactly one head node)
    4. Tail node requirements (exactly one tail node)
    
    Args:
        graph: The graph structure to validate
        name: Optional name for the graph (for logging)
        
    Raises:
        ValueError: If any validation fails, with detailed error message
    """
    errors = []
    
    # Check for cycles
    has_cycles = check_graph_for_cycles(graph, name)
    if has_cycles:
        msg = f"Graph {name} contains cycles"
        logging.error(msg)
        errors.append(msg)
    
    # Check for cross-graph reference violations
    is_valid_refs, violations = validate_graph_references(graph)
    if not is_valid_refs:
        msg = f"Graph {name} contains invalid cross-graph references:\n" + "\n".join(violations)
        logging.error(msg)
        errors.append(msg)
    
    # Check for head node requirements
    head_nodes = find_head_nodes(graph)
    if len(head_nodes) == 0:
        msg = f"Graph {name} has no head nodes (nodes with no incoming edges)"
        logging.error(msg)
        errors.append(msg)
    elif len(head_nodes) > 1:
        msg = f"Graph {name} has multiple head nodes: {head_nodes}. Exactly one head node is required."
        logging.warning(msg)
    
    # Check for tail node requirements
    tail_nodes = find_tail_nodes(graph)
    if len(tail_nodes) == 0:
        msg = f"Graph {name} has no tail nodes (nodes with no outgoing edges)"
        logging.error(msg)
        errors.append(msg)
    elif len(tail_nodes) > 1:
        msg = f"Graph {name} has multiple tail nodes: {tail_nodes}. Exactly one tail node is required."
        logging.warning(msg)
    
    # If any errors were found, raise exception with all error messages
    if errors:
        raise ValueError(f"Graph validation failed:\n" + "\n".join(errors))
    
    # Log success if no errors
    logging.info(f"Graph {name} passed all validations")


================================================
File: src/jobchain/jc_logging.py
================================================
"""
Logging configuration for JobChain.

Environment Variables:
    JOBCHAIN_LOG_LEVEL: Set the logging level (e.g., 'DEBUG', 'INFO'). Defaults to 'INFO'.
    JOBCHAIN_LOG_HANDLERS: Set logging handlers. Options:
        - Not set or 'console': Log to console only (default)
        - 'console,file': Log to both console and file
        
Example:
    To enable both console and file logging:
    $ export JOBCHAIN_LOG_HANDLERS='console,file'
    
    To set debug level logging:
    $ export JOBCHAIN_LOG_LEVEL='DEBUG'
"""


import os

# Initializing the flag here stops logging caching root levels to another value
# for reasons I'm not completely sure about.
WINDSURF_LOG_FLAG = None #None #"DEBUG"
os.environ['JOBCHAIN_LOG_LEVEL'] = WINDSURF_LOG_FLAG or os.getenv('JOBCHAIN_LOG_LEVEL', 'INFO')
import logging
from logging.config import dictConfig


def get_logging_config():
    """Get the logging configuration based on current environment variables."""
    
    
    return {
        'version': 1,
        'disable_existing_loggers': False,
        'formatters': {
            'detailed': {
                'format': '%(asctime)s [%(levelname)s] %(name)s:%(lineno)d - %(message)s'
            }
        },
        'handlers': {
            'console': {
                'class': 'logging.StreamHandler',
                'level': os.getenv('JOBCHAIN_LOG_LEVEL', 'INFO'),
                'formatter': 'detailed',
                'stream': 'ext://sys.stdout'
            },
            'file': {
                'class': 'logging.FileHandler',
                'level': os.getenv('JOBCHAIN_LOG_LEVEL', 'INFO'),
                'formatter': 'detailed',
                'filename': 'jobchain.log',
                'mode': 'a'
            }
        },
        'loggers': {
            'ExampleCustom': {
                'level': 'DEBUG',
                'handlers': ['console', 'file'],
                'propagate': False
            }
        },
        'root': {
            'level':  os.getenv('JOBCHAIN_LOG_LEVEL', 'INFO'),
            # Set JOBCHAIN_LOG_HANDLERS='console,file' to enable both console and file logging
            'handlers': os.getenv('JOBCHAIN_LOG_HANDLERS', 'console').split(',')
        }
    }

def setup_logging():
    """Setup logging with current configuration."""
    config = get_logging_config()
    
    # Always create log file with header, actual logging will only happen if handlers use it
    if not os.path.exists('jobchain.log'):
        with open('jobchain.log', 'w') as f:
            f.write('# JobChain log file - This file is created empty and will be written to only when file logging is enabled\n')
    
    print(f"Logging level: {config['root']['level']}")
    # Apply configuration
    dictConfig(config)

# Apply configuration when module is imported
setup_logging()

# Re-export everything from logging
# Constants
CRITICAL = logging.CRITICAL
ERROR = logging.ERROR
WARNING = logging.WARNING
INFO = logging.INFO
DEBUG = logging.DEBUG
NOTSET = logging.NOTSET

# Functions
getLogger = logging.getLogger
basicConfig = logging.basicConfig
shutdown = logging.shutdown
debug = logging.debug
info = logging.info
warning = logging.warning
error = logging.error
critical = logging.critical
exception = logging.exception
log = logging.log

# Classes
Logger = logging.Logger
Handler = logging.Handler
Formatter = logging.Formatter
Filter = logging.Filter
LogRecord = logging.LogRecord

# Handlers
StreamHandler = logging.StreamHandler
FileHandler = logging.FileHandler
NullHandler = logging.NullHandler

# Configuration
dictConfig = dictConfig  # Already imported from logging.config
fileConfig = logging.config.fileConfig

# Exceptions
exception = logging.exception
captureWarnings = logging.captureWarnings

# Additional utilities
getLevelName = logging.getLevelName
makeLogRecord = logging.makeLogRecord


================================================
File: src/jobchain/job.py
================================================
import asyncio
import uuid
from abc import ABC, ABCMeta, abstractmethod
from contextlib import asynccontextmanager
from contextvars import ContextVar
from typing import Any, Dict, Optional, Type, Union

from . import jc_logging as logging
from .utils.otel_wrapper import trace_function


def _is_traced(method):
    """Helper function to check if a method is traced."""
    return hasattr(method, '_is_traced') and method._is_traced


def _has_own_traced_execute(cls):
    """Helper function to check if a class has its own traced _execute (not inherited)."""
    return '_execute' in cls.__dict__ and _is_traced(cls.__dict__['_execute'])


def _mark_traced(method):
    """Helper function to mark a method as traced."""
    method._is_traced = True
    return method


def traced_job(cls: Type) -> Type:
    """
    Class decorator that ensures the execute method is traced.
    This is only applied to the JobABC class itself.
    """
    if hasattr(cls, '_execute'):
        original_execute = cls._execute
        traced_execute = trace_function(original_execute, detailed_trace=True)
        traced_execute = _mark_traced(traced_execute)
        # Store original as executeNoTrace
        cls.executeNoTrace = original_execute
        # Replace execute with traced version
        cls._execute = traced_execute
    return cls


class JobMeta(ABCMeta):
    """Metaclass that automatically applies the traced_job decorator to JobABC only."""
    def __new__(mcs, name, bases, namespace):
        cls = super().__new__(mcs, name, bases, namespace)
        if name == 'JobABC':  # Only decorate the JobABC class itself
            return traced_job(cls)
        # For subclasses, ensure they inherit JobABC's traced _execute
        if '_execute' in namespace:
            # If subclass defines its own _execute, ensure it's not traced again
            # but still inherits the tracing from JobABC
            del namespace['_execute']
            cls = super().__new__(mcs, name, bases, namespace)
        return cls


class Task(dict):
    """A task dictionary with a unique identifier.
    
    Args:
        data (Union[Dict[str, Any], str]): The task data as a dictionary or string. If a string,
                                            it will be converted to a dictionary with a 'task' key.
        job_name (Optional[str], optional): The name of the job that will process this task.
                                            Required if there is more than one job graph in the
                                            JobChain class"""
    def __init__(self, data: Union[Dict[str, Any], str], job_name: Optional[str] = None):
        # Convert string input to dict
        if isinstance(data, str):
            data = {'task': data}
        elif isinstance(data, dict):
            data = data.copy()  # Create a copy to avoid modifying the original
        else:
            data = {'task': str(data)}
        
        super().__init__(data)
        self.task_id:str = str(uuid.uuid4())
        if job_name is not None:
            self['job_name'] = job_name

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, Task):
            return NotImplemented
        return self.task_id == other.task_id

    # mypy highlights this as an error because dicts are mutable
    #   and so not hashable, but I want each Task to have a unique id
    #   so it is hashable.
    def __hash__(self) -> int:
        return hash(self.task_id)

    def __repr__(self) -> str:
        job_name = self.get('job_name', 'None')
        task_preview = str(dict(self))[:50] + '...' if len(str(dict(self))) > 50 else str(dict(self))
        return f"Task(id={self.task_id}, job_name={job_name}, data={task_preview})"

class JobState:
  def __init__(self):
      self.inputs: Dict[str, Dict[str, Any]] = {}
      self.input_event = asyncio.Event()
      self.execution_started = False

job_graph_context : ContextVar[dict] = ContextVar('job_graph_context')

@asynccontextmanager
async def job_graph_context_manager(job_set: set['JobABC']):
  """Create a new context for job execution, with a new JobState."""
  new_state = {}
  for job in job_set:
      new_state[job.name] = JobState()
  new_state[JobABC.CONTEXT] = {}
  token = job_graph_context.set(new_state)
  try:
      yield new_state
  finally:
      job_graph_context.reset(token)

class JobABC(ABC, metaclass=JobMeta):
    """
    Abstract base class for jobs. Only this class will have tracing enabled through the JobMeta metaclass.
    Subclasses will inherit the traced version of _execute but won't add additional tracing.
    """

    # class variable to keep track of instance counts for each class
    _instance_counts: Dict[Type, int] = {}
    
    # Key used to pass task metadata through the job chain
    TASK_PASSTHROUGH_KEY: str = 'task_pass_through'
    RETURN_JOB='RETURN_JOB'
    CONTEXT='CONTEXT'

    def __init__(self, name: Optional[str] = None, properties: Dict[str, Any] = {}):
        """
        Initialize an JobABC instance.

        Args:
            name (Optional[str], optional): Must be a unique identifier for this job within the context of a JobChain.
                                            If not provided, a unique name will be auto-generated.
            properties (Dict[str, Any], optional): configuration properties passed in by jobs.yaml
        """
        self.name:str = self._getUniqueName() if name is None else name
        self.properties:Dict[str, Any] = properties
        self.expected_inputs:set[str] = set()
        self.next_jobs:list[JobABC] = [] 
        self.timeout = 3000
        self.logger = logging.getLogger(self.__class__.__name__)

    @classmethod
    def parse_job_loader_name(cls, name: str) -> Dict[str, str]:
        """Parse a job loader name into its constituent parts.
        
        Args:
            name: The full job loader name string in format:
                 graph_name$$param_name$$job_name$$
                 
        Returns:
            dict: A dictionary containing graph_name, param_name, and job_name,
                 or {'parsing_message': 'UNSUPPORTED NAME FORMAT'} if invalid
        """
        try:
            parts = name.split("$$")
            if len(parts) != 4 or parts[3] != "" or not parts[0]:
                return {"parsing_message": "UNSUPPORTED NAME FORMAT"}
                
            return {
                "graph_name": parts[0],
                "param_name": parts[1],
                "job_name": parts[2]
            }
        except:
            return {"parsing_message": "UNSUPPORTED NAME FORMAT"}

    @classmethod
    def parse_graph_name(cls, name: str) -> str:
        """Parse and return the graph name from a job loader name.
        
        Args:
            name: The full job loader name string
            
        Returns:
            str: The graph name or 'UNSUPPORTED NAME FORMAT' if invalid
        """
        result = cls.parse_job_loader_name(name)
        return result.get("graph_name", "UNSUPPORTED NAME FORMAT")

    @classmethod
    def parse_param_name(cls, name: str) -> str:
        """Parse and return the parameter name from a job loader name.
        
        Args:
            name: The full job loader name string
            
        Returns:
            str: The parameter name or 'UNSUPPORTED NAME FORMAT' if invalid
        """
        result = cls.parse_job_loader_name(name)
        return result.get("param_name", "UNSUPPORTED NAME FORMAT")

    @classmethod
    def parse_job_name(cls, name: str) -> str:
        """Parse and return the job name from a job loader name.
        
        Args:
            name: The full job loader name string
            
        Returns:
            str: The job name or 'UNSUPPORTED NAME FORMAT' if invalid
        """
        result = cls.parse_job_loader_name(name)
        return result.get("job_name", "UNSUPPORTED NAME FORMAT")

    @classmethod
    def _getUniqueName(cls):
        # Increment the counter for the current class
        cls._instance_counts[cls] = cls._instance_counts.get(cls, 0) + 1
        # Return a unique name based on the current class
        return f"{cls.__name__}_{cls._instance_counts[cls]}"

    @classmethod
    def get_input_from(cls, inputs: Dict[str, Any], job_name: str) -> Dict[str, Any]:
        """Get input data from a specific job in the inputs dictionary.
        
        Args:
            inputs (Dict[str, Any]): Dictionary of inputs from various jobs
            job_name (str): Name of the job whose input we want to retrieve
            
        Returns:
            Dict[str, Any]: The input data from the specified job, or empty dict if not found
        """
        for key in inputs.keys():
            if cls.parse_job_name(key) == job_name:
                return inputs[key]
        return {}


    @classmethod
    def job_set(cls, job) -> set['JobABC']:
        """
        Returns a set of all unique job instances in the job graph by recursively traversing
        all possible paths through next_jobs.
        
        Returns:
            set[JobABC]: A set containing all unique job instances in the graph
        """
        result = {job}  # Start with current job instance
        
        # Base case: if no next jobs, return current set
        if not job.next_jobs:
            return result
            
        # Recursive case: add all jobs from each path
        for job in job.next_jobs:
            result.update(cls.job_set(job))
            
        return result

    def __repr__(self):
        next_jobs_str = [job.name for job in self.next_jobs]
        expected_inputs_str = [input_name for input_name in self.expected_inputs]
        return (f"name: {self.name}\n"
                f"next_jobs: {next_jobs_str}\n"
                f"expected_inputs: {expected_inputs_str}\n"
                f"properties: {self.properties}")

    async def _execute(self, task: Union[Task, None]) -> Dict[str, Any]:
        """ Responsible for executing the job graph, maintaining state of the graph
        by updating the JobState object and propagating the tail results back up the graph
        when a tail job is reached.

        WARNING: DO NOT OVERRIDE THIS METHOD IN CUSTOM JOB CLASSES.
        This method is part of the core JobChain execution flow and handles critical operations
        including job graph traversal, state management, and result propagation.
        
        Instead, implement the abstract 'run' method to define custom job behavior.

        Can only be used within a job_graph_context set up with:
           ```python
            async with job_graph_context_manager(job_set):
                        result = await job._execute(task)
            ```

        Args:
            task (Union[Task, None]): the input to the first (head) job of the job graph, is None in child jobs.

        Returns:
            Dict[str, Any]: The output of the job graph execution
        """
        job_state_dict:dict = job_graph_context.get()
        job_state = job_state_dict.get(self.name)
        if isinstance(task, dict):
            job_state.inputs.update(task)
            self.get_context()[JobABC.TASK_PASSTHROUGH_KEY] = task
        elif task is None:
            pass 
        else:
            job_state.inputs[self.name] = task

        if self.expected_inputs:
            if job_state.execution_started:
                return None
            
            job_state.execution_started = True
            try:
                await asyncio.wait_for(job_state.input_event.wait(), self.timeout)
            except asyncio.TimeoutError:
                job_state.execution_started = False
                raise TimeoutError(
                    f"Timeout waiting for inputs in {self.name}. "
                    f"Expected: {self.expected_inputs}, "
                    f"Received: {list(job_state.inputs.keys())}"
                )

        result = await self.run(job_state.inputs)
        self.logger.debug(f"Job {self.name} finished running")

        if not isinstance(result, dict):
            result = {'result': result}

        # if isinstance(task, dict):
        #     result[JobABC.TASK_PASSTHROUGH_KEY] = task
        # else:
        #     for input_data in job_state.inputs.values():
        #         if isinstance(input_data, dict) and JobABC.TASK_PASSTHROUGH_KEY in input_data:
        #             result[JobABC.TASK_PASSTHROUGH_KEY] = input_data[JobABC.TASK_PASSTHROUGH_KEY]
        #             break

        # Clear state for potential reuse
        job_state.inputs.clear()
        job_state.input_event.clear()
        job_state.execution_started = False

        # Store the job name that returns the result
        result[JobABC.RETURN_JOB] = self.name

        # If this is a tail job, return immediately
        if not self.next_jobs:
            self.logger.debug(f"Tail Job {self.name} returning result: {result}")
            task = self.get_context()[JobABC.TASK_PASSTHROUGH_KEY]
            result[JobABC.TASK_PASSTHROUGH_KEY] = task
            return result

        # Execute child jobs
        executing_jobs = []
        for next_job in self.next_jobs:
            input_data = result.copy()
            await next_job.receive_input(self.name, input_data)
            next_job_inputs = job_state_dict.get(next_job.name).inputs
            if next_job.expected_inputs.issubset(set(next_job_inputs.keys())):
                executing_jobs.append(next_job._execute(task=None))

        if executing_jobs:
            child_results = await asyncio.gather(*executing_jobs)
            # Find the tail job result (the one that has no next_jobs)
            tail_results = [r for r in child_results if r is not None]
            if tail_results:
                # Always return the first valid tail result
                tail_result = tail_results[0]
                self.logger.debug(f"Job {self.name} propagating tail result: {tail_result}")
                # Preserve the original tail job that generated the result
                return tail_result

        # If no child jobs executed or no tail result found, return None
        return None

    async def receive_input(self, from_job: str, data: Dict[str, Any]) -> None:
        """Receive input from a predecessor job"""
        job_state_dict:dict = job_graph_context.get()
        job_state = job_state_dict.get(self.name)
        job_state.inputs[from_job] = data
        if self.expected_inputs.issubset(set(job_state.inputs.keys())):
            job_state.input_event.set()

    def job_set_str(self) -> set[str]:
        """
        Returns a set of all unique job names in the job graph by recursively traversing
        all possible paths through next_jobs.
        
        Returns:
            set[str]: A set containing all unique job names in the graph
        """
        result = {self.name}  # Start with current job's name
        
        # Base case: if no next jobs, return current set
        if not self.next_jobs:
            return result
            
        # Recursive case: add all jobs from each path
        for job in self.next_jobs:
            result.update(job.job_set_str())
            
        return result

    def is_head_job(self) -> bool:
        """
        Check if this job is a head job (has no expected inputs).

        Returns:
            bool: True if this is a head job (no expected inputs), False otherwise
        """
        return len(self.expected_inputs) == 0

    def get_context(self) -> Dict[str, Any]:
        """A repository to store state across jobs in a graph for a single coroutine.
           can only be used within a job_graph_context set up with:
           ```python
            async with job_graph_context_manager(job_set):
                        result = await job._execute(task)
            ```
        """
        job_state_dict:dict = job_graph_context.get()
        context = job_state_dict[JobABC.CONTEXT]
        return context

    def get_task(self) -> Union[Dict[str, Any], Task]:
        """
        Get the task associated with this job.

        Returns:
            Union[Dict[str, Any], Task]: The task associated with this job.
        """
        task = self.get_context()[JobABC.TASK_PASSTHROUGH_KEY]
        return task
        
        # if not self.is_head_job(): 
        #     first_parent_result = next(iter(inputs.values()))
        #     task = first_parent_result[JobABC.TASK_PASSTHROUGH_KEY]
        # else:
        #     task = inputs
        # return task

    @abstractmethod
    async def run(self, task: Union[Dict[str, Any], Task]) -> Dict[str, Any]:
        """Execute the job on the given task. Must be implemented by subclasses."""
        pass

# SimpleJob and SimpleJobFactory have been moved to tests/test_utils/simple_job.py
# They are only used for testing purposes and not for production code.


================================================
File: src/jobchain/job_chain.py
================================================
import asyncio
# In theory it makes sense to use dill with the "multiprocess" package
# instead of pickle with "multiprocessing", but in practice it leads to 
# performance and stability issues.
import multiprocessing as mp
import pickle
import queue
from collections import OrderedDict
from multiprocessing import freeze_support, set_start_method
from typing import Any, Callable, Collection, Dict, Optional, Union

from pydantic import BaseModel

from . import jc_logging as logging
from .job import JobABC, Task, job_graph_context_manager
from .job_loader import ConfigLoader, JobFactory
from .utils.monitor_utils import should_log_task_stats


class JobChain:
    """
    JobChain executes up to thousands of tasks in parallel using one or more Jobs passed into constructor.
    Optionally passes results to a pre-existing result processing function after task completion.

    Args:
        job (Union[Dict[str, Any], JobABC, Collection[JobABC]]): If missing, jobs will be loaded from config file.
            Otherwise either a dictionary containing job configuration,
            a single JobABC instance, or a collection of JobABC instances.

        result_processing_function (Optional[Callable[[Any], None]]): Code to handle results after the Job executes its task.
            By default, this hand-off happens in parallel, immediately after a Job processes a task.
            Typically, this function is from an existing codebase that JobChain is supplementing.
            This function must be picklable, for parallel execution, see serial_processing parameter below.
            This code is not assumed to be asyncio compatible.

        serial_processing (bool, optional): Forces result_processing_function to execute only after all tasks are completed by the Job.
            Enables an unpicklable result_processing_function to be used by setting serial_processing=True.
            However, in most cases changing result_processing_function to be picklable is straightforward and should be the default.
            Defaults to False.
    """
    # Constants
    JOB_MAP_LOAD_TIME = 5  # Timeout in seconds for job map loading

    def __init__(self, job: Optional[Any] = None, result_processing_function: Optional[Callable[[Any], None]] = None, 
                 serial_processing: bool = False):
        # Get logger for JobChain
        self.logger = logging.getLogger('JobChain')
        self.logger.info("Initializing JobChain")
        if not serial_processing and result_processing_function:
            self._check_picklable(result_processing_function)
        # tasks are created by submit_task(), with ["job_name"] added to the task dict
        # tasks are then sent to queue for processing
        self._task_queue: mp.Queue[Task] = mp.Queue()  
        # INTERNAL USE ONLY. DO NOT ACCESS DIRECTLY.
        # This queue is for internal communication between the job executor and result processor.
        # To process results, use the result_processing_function parameter in the JobChain constructor.
        # See test_result_processing.py for examples of proper result handling.
        self._result_queue = mp.Queue()  # type: mp.Queue
        self.job_executor_process = None
        self.result_processor_process = None
        self._result_processing_function = result_processing_function
        self._serial_processing = serial_processing
        # This holds a map of job name to job, 
        # when _execute is called on the job, the task must have a job_name
        # associated with it, if there is more than one job in the job_map
        self.job_map: OrderedDict[str, JobABC] = OrderedDict()
        
        # Create a manager for sharing objects between processes
        self._manager = mp.Manager()
        # Create a shared dictionary for job name mapping
        self._job_name_map = self._manager.dict()
        # Create an event to signal when jobs are loaded
        self._jobs_loaded = mp.Event()

        if job:
            self.create_job_map(job)
        
        self._start()

    def create_job_map(self, job):
        if isinstance(job, Dict):
            pass # SimpleJobFactory is deprecated
            # job_context: Dict[str, Any] = job.get("job_context") or {}
            # loaded_job = SimpleJobFactory.load_job(job_context)
            # if isinstance(loaded_job, JobABC):
            #     self.job_map[loaded_job.name] = loaded_job
        elif isinstance(job, JobABC):
            self.job_map[job.name] = job
        elif isinstance(job, Collection) and not isinstance(job, (str, bytes, bytearray)):
            if not job:  # Check if collection is empty
                raise ValueError("Job collection cannot be empty")
            if not all(isinstance(j, JobABC) for j in job):
                raise TypeError("All items in job collection must be JobABC instances")
            for j in job:
                if isinstance(j, JobABC):
                    if j.name in self.job_map:
                        raise ValueError(f"Duplicate job name found: {j.name}")
                    self.job_map[j.name] = j
                else:
                    raise TypeError("Items in job collection must be JobABC instances")
        else:
            raise TypeError("job must be either Dict[str, Any], JobABC instance, or Collection[JobABC]")

        self._job_name_map.clear()
        self._job_name_map.update({job.name: job.job_set_str() for job in self.job_map.values()})

    # We will not to use context manager as it makes semantics of JobChain use less flexible
    # def __enter__(self):
    #     """Initialize resources when entering the context."""
    #     self._start()
    #     return self

    # def __exit__(self, exc_type, exc_val, exc_tb):
    #     """Clean up resources when exiting the context."""
    #     self._cleanup()

    # belt and braces, _cleanup is called by _wait_for_completion() via mark_input_completed()
    def __del__(self):
        self._cleanup

    def _cleanup(self):
        """Clean up resources when the object is destroyed."""
        self.logger.info("Cleaning up JobChain resources")
        
        if self.job_executor_process:
            if self.job_executor_process.is_alive():
                self.logger.debug("Terminating job executor process")
                self.job_executor_process.terminate()
                self.logger.debug("Joining job executor process")
                self.job_executor_process.join()
                self.logger.debug("Job executor process joined")
        
        if self.result_processor_process:
            if self.result_processor_process.is_alive():
                self.logger.debug("Terminating result processor process")
                self.result_processor_process.terminate()
                self.logger.debug("Joining result processor process")
                self.result_processor_process.join()
                self.logger.debug("Result processor process joined")
        
        if hasattr(self, '_task_queue'):
            self.logger.debug("Closing task queue")
            self._task_queue.close()
            self.logger.debug("Joining task queue thread")
            self._task_queue.join_thread()
            self.logger.debug("Task queue thread joined")
        
        if hasattr(self, '_result_queue'):
            self.logger.debug("Closing result queue")
            self._result_queue.close()
            self.logger.debug("Joining result queue thread")
            self._result_queue.join_thread()
            self.logger.debug("Result queue thread joined")
        
        self.logger.debug("Cleanup completed")

    def _check_picklable(self, result_processing_function):
        try:
            # Try to pickle just the function itself
            pickle.dumps(result_processing_function)
            
            # Try to pickle any closure variables
            if hasattr(result_processing_function, '__closure__') and result_processing_function.__closure__:
                for cell in result_processing_function.__closure__:
                    pickle.dumps(cell.cell_contents)
                    
        except Exception as e:
            self.logger.error(f"""Result processing function or its closure variables cannot be pickled: {e}.  
                              Use serial_processing=True for unpicklable functions.""")
            raise TypeError(f"Result processing function must be picklable in parallel mode: {e}")

    
    def _start(self):
        """Start the job executor and result processor processes - non-blocking."""
        self.logger.debug("Starting job executor process")
        self.job_executor_process = mp.Process(
            target=self._async_worker,
            args=(self.job_map, self._task_queue, self._result_queue, self._job_name_map, self._jobs_loaded, ConfigLoader.directories),
            name="JobExecutorProcess"
        )
        self.job_executor_process.start()
        self.logger.info(f"Job executor process started with PID {self.job_executor_process.pid}")

        if self._result_processing_function and not self._serial_processing:
            self.logger.debug("Starting result processor process")
            self.result_processor_process = mp.Process(
                target=self._result_processor,
                args=(self._result_processing_function, self._result_queue),
                name="ResultProcessorProcess"
            )
            self.result_processor_process.start()
            self.logger.info(f"Result processor process started with PID {self.result_processor_process.pid}")
    # TODO: add ability to submit a task or an iterable: Iterable
    # TODO: add resource usage monitoring which returns False if resource use is too high.
    def submit_task(self, task: Union[Dict[str, Any], str], job_name: Optional[str] = None):
        """
        Submit a task to be processed by the job.

        Args:
            task: Either a dictionary containing task data or a string that will be converted to a task.
                    if this is None then the task will be skipped.
            job_name: The name of the job to execute this task. Required if there is more than one job in the job_map,
                     unless the task is a dictionary that includes a 'job_name' key.

        Raises:
            ValueError: If job_name is required but not provided, or if the specified job cannot be found.
            TypeError: If the task is not a dictionary or string.
        """
        try:
            # Wait for jobs to be loaded
            if not self._jobs_loaded.wait(timeout=self.JOB_MAP_LOAD_TIME):
                raise TimeoutError("Timed out waiting for jobs to be loaded")

            if task is None:
                self.logger.warning("Received None task, skipping")
                return
            
            if isinstance(task, str):
                task_dict = {'task': task}
            elif isinstance(task, dict):
                task_dict = task.copy()
            else:
                self.logger.warning(f"Received invalid task type {type(task)}, converting to string")
                task_dict = {'task': str(task)}

            # If job_name parameter is provided, it takes precedence
            if job_name is not None:
                if job_name not in self._job_name_map:
                    raise ValueError(
                        f"Job '{job_name}' not found. Available jobs: {list(self._job_name_map.keys())}"
                    )
                task_dict['job_name'] = job_name
            
            # If there's more than one job, we need a valid job name
            if len(self._job_name_map) > 1:
                if 'job_name' not in task_dict or not isinstance(task_dict['job_name'], str) or not task_dict['job_name']:
                    raise ValueError(
                        "When multiple jobs are present, you must either:\n"
                        "1) Provide the job_name parameter in submit_task() OR\n"
                        "2) Include a non-empty string 'job_name' in the task dictionary"
                    )
                if task_dict['job_name'] not in self._job_name_map:
                    raise ValueError(
                        f"Job '{task_dict['job_name']}' not found. Available jobs: {list(self._job_name_map.keys())}"
                    )
            elif len(self._job_name_map) == 1:
                # If there's only one job, use its name
                task_dict['job_name'] = next(iter(self._job_name_map.keys()))
            else:
                raise ValueError("No jobs available in JobChain")

            task_obj = Task(task_dict)
            self._task_queue.put(task_obj)
        except Exception as e:
            self.logger.error(f"Error submitting task: {e}")
            self.logger.info("Detailed stack trace:", exc_info=True)

    def mark_input_completed(self):
        """Signal completion of input and wait for all processes to finish and shut down."""
        self.logger.debug("Marking input as completed")
        self.logger.info("*** task_queue ended ***")
        self._task_queue.put(None)
        self._wait_for_completion()

    # Must be static because it's passed as a target to multiprocessing.Process
    # Instance methods can't be pickled properly for multiprocessing
    # TODO: it may be necessary to put a flag to execute this using asyncio event loops
    #          for example, when handing off to an async web service
    @staticmethod
    def _result_processor(process_fn: Callable[[Any], None], result_queue: 'mp.Queue'):
        """Process that handles processing results as they arrive."""
        logger = logging.getLogger('ResultProcessor')
        logger.debug("Starting result processor")

        while True:
            try:
                result = result_queue.get()
                if result is None:
                    logger.debug("Received completion signal from result queue")
                    break
                logger.debug(f"ResultProcessor received result: {result}")
                try:
                    # Handle both dictionary and non-dictionary results
                    task_id = result.get('task', str(result)) if isinstance(result, dict) else str(result)
                    logger.debug(f"Processing result for task {task_id}")
                    process_fn(result)
                    logger.debug(f"Finished processing result for task {task_id}")
                except Exception as e:
                    logger.error(f"Error processing result: {e}")
                    logger.info("Detailed stack trace:", exc_info=True)
            except queue.Empty:
                continue

        logger.debug("Result processor shutting down")

    def _wait_for_completion(self):
        """Wait for completion of all processing."""
        self.logger.debug("Entering wait for completion")

        if self._result_processing_function and self._serial_processing:
            self._process_serial_results()
        
        # Wait for job executor to finish
        if self.job_executor_process and self.job_executor_process.is_alive():
            self.logger.debug("Waiting for job executor process")
            self.job_executor_process.join()
            self.logger.debug("Job executor process completed")

        # Wait for result processor to finish
        if self.result_processor_process and self.result_processor_process.is_alive():
            self.logger.debug("Waiting for result processor process")
            self.result_processor_process.join()
            self.logger.debug("Result processor process completed")
        
        self._cleanup()

    def _process_serial_results(self):
        while True:
            try:
                self.logger.debug("Attempting to get result from queue")
                result = self._result_queue.get(timeout=0.1)
                if result is None:
                    self.logger.debug("Received completion signal (None) from result queue")
                    self.logger.info("No more results to process.")
                    break
                if self._result_processing_function:
                    try:
                        # Handle both dictionary and non-dictionary results
                        task_id = result.get('task', str(result)) if isinstance(result, dict) else str(result)
                        self.logger.debug(f"Processing result for task {task_id}")
                        self._result_processing_function(result)
                        self.logger.debug(f"Finished processing result for task {task_id}")
                    except Exception as e:
                        self.logger.error(f"Error processing result: {e}")
                        self.logger.info("Detailed stack trace:", exc_info=True)
            except queue.Empty:
                job_executor_is_alive = self.job_executor_process and self.job_executor_process.is_alive()
                self.logger.debug(f"Queue empty, job executor process alive status = {job_executor_is_alive}")
                if not job_executor_is_alive:
                    self.logger.debug("Job executor process is not alive, breaking wait loop")
                    break
                continue

    @staticmethod
    def _replace_pydantic_models(data: Any) -> Any:
        """Recursively replace pydantic.BaseModel instances with their JSON dumps."""
        logger = logging.getLogger('JobChain')
        logger.debug(f'Processing data type: {type(data)}')

        if isinstance(data, dict):
            return {k: JobChain._replace_pydantic_models(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [JobChain._replace_pydantic_models(item) for item in data]
        elif isinstance(data, BaseModel):
            logger.info(f'Converting pydantic model {data.__class__.__name__}')
            return data.model_dump_json()
        return data

    # Must be static because it's passed as a target to multiprocessing.Process
    # Instance methods can't be pickled properly for multiprocessing
    @staticmethod
    def _async_worker(job_map: Dict[str, JobABC], task_queue: 'mp.Queue', result_queue: 'mp.Queue', 
                     job_name_map: 'mp.managers.DictProxy', jobs_loaded: 'mp.Event', 
                     directories: list[str] = []):
        """Process that handles making workflow calls using asyncio."""
        # Get logger for AsyncWorker
        logger = logging.getLogger('AsyncWorker')
        logger.debug("Starting async worker")

        # If job_map is empty, create it from SimpleJobLoader
        if not job_map:
            # logger.info("Creating job map from SimpleJobLoader")
            # job = SimpleJobFactory.load_job({"type": "file", "params": {}})
            # job_map = {job.name: job}
            logger.info("Creating job map from JobLoader")
            logger.info(f"Using directories from process: {directories}")
            ConfigLoader._set_directories(directories)
            ConfigLoader.reload_configs()
            head_jobs = JobFactory.get_head_jobs_from_config()
            job_map = {job.name: job for job in head_jobs}
            # Update the shared job_name_map with each head job's complete set of reachable jobs
            job_name_map.clear()
            job_name_map.update({job.name: job.job_set_str() for job in head_jobs})
            logger.info(f"Created job map with head jobs: {list(job_name_map.keys())}")

        # Signal that jobs are loaded
        jobs_loaded.set()

        async def process_task(task: Task):
            """Process a single task and return its result"""
            task_id = task.task_id  # task_id is not held in the dictionary itself i.e. NOT task['task_id']
            logger.debug(f"[TASK_TRACK] Starting task {task_id}")
            try:
                # If there's only one job, use it directly
                if len(job_map) == 1:
                    job = next(iter(job_map.values()))
                else:
                    # Otherwise, get the job from the map using job_name
                    job_name = task.get('job_name')
                    if not job_name:
                        raise ValueError("Task missing job_name when multiple jobs are present")
                    job = job_map[job_name]
                job_set = JobABC.job_set(job) #TODO: create a map of job to jobset in _async_worker
                async with job_graph_context_manager(job_set):
                    result = await job._execute(task)
                    processed_result = JobChain._replace_pydantic_models(result)
                    logger.info(f"[TASK_TRACK] Completed task {task_id}, returned by job {processed_result[JobABC.RETURN_JOB]}")

                    result_queue.put(processed_result)
                    logger.debug(f"[TASK_TRACK] Result queued for task {task_id}")
            except Exception as e:
                logger.error(f"[TASK_TRACK] Failed task {task_id}: {e}")
                logger.info("Detailed stack trace:", exc_info=True)
                raise

        async def queue_monitor():
            """Monitor the task queue and create tasks as they arrive"""
            logger.debug("Starting queue monitor")
            tasks = set()
            pending_tasks = []
            tasks_created = 0
            tasks_completed = 0
            end_signal_received = False

            while not end_signal_received or tasks:
                # Get all available tasks from the queue
                while True:
                    try:
                        task = task_queue.get_nowait()
                        if task is None:
                            logger.info("Received end signal in task queue")
                            end_signal_received = True
                            break
                        pending_tasks.append(task)
                    except queue.Empty:
                        break

                # Create tasks in batch if we have any pending
                if pending_tasks:
                    logger.debug(f"Creating {len(pending_tasks)} new tasks")
                    new_tasks = {asyncio.create_task(process_task(pending_tasks[i])) for i in range(len(pending_tasks))}
                    tasks.update(new_tasks)
                    tasks_created += len(new_tasks)
                    logger.debug(f"Total tasks created: {tasks_created}")
                    pending_tasks.clear()

                # Clean up completed tasks
                done_tasks = {t for t in tasks if t.done()}
                if done_tasks:
                    for done_task in done_tasks:
                        try:
                            # Check if task raised an exception
                            exc = done_task.exception()
                            if exc:
                                logger.error(f"Task failed with exception: {exc}")
                                logger.info("Detailed stack trace:", exc_info=True)
                        except asyncio.InvalidStateError:
                            pass  # Task was cancelled or not done
                    tasks_completed += len(done_tasks)
                    logger.debug(f"Cleaned up {len(done_tasks)} completed tasks. Total completed: {tasks_completed}")
                    logger.debug(f"Active tasks remaining: {len(tasks)}")
                tasks.difference_update(done_tasks)

                # Log task stats periodically
                if tasks_completed != 0 and tasks_completed % 5 == 0:
                    if should_log_task_stats(queue_monitor, tasks_created, tasks_completed):
                        logger.info(f"Tasks stats - Created: {tasks_created}, Completed: {tasks_completed}, Active: {len(tasks)}")

                # A short pause to reduce CPU usage and avoid a busy-wait state.             
                await asyncio.sleep(0.0001)

            # Wait for remaining tasks to complete
            if tasks:
                logger.debug(f"Waiting for {len(tasks)} remaining tasks")
                await asyncio.gather(*tasks)
                logger.debug("All remaining tasks completed")

            # Signal completion
            logger.debug("Sending completion signal to result queue")
            logger.debug(f"Final stats - Created: {tasks_created}, Completed: {tasks_completed}")
            logger.info("*** result_queue ended ***")
            result_queue.put(None)

        # Run the event loop
        logger.debug("Creating event loop")
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            logger.debug("Running queue monitor")
            loop.run_until_complete(queue_monitor())
        except Exception as e:
            import traceback
            logger.error(f"Error in async worker: {e}\n{traceback.format_exc()}")
            logger.info("Detailed stack trace:", exc_info=True)
        finally:
            logger.info("Closing event loop")
            loop.close()

    def get_job_names(self) -> list[str]:
        """
        Returns a list of job names after ensuring the job_name_map is loaded.

        Returns:
            list[str]: List of job names from the job_name_map

        Raises:
            TimeoutError: If waiting for jobs to be loaded exceeds timeout
        """
        self.logger.debug("Waiting for jobs to be loaded before returning job names")
        if not self._jobs_loaded.wait(timeout=self.JOB_MAP_LOAD_TIME):
            raise TimeoutError("Timed out waiting for jobs to be loaded")
        
        return list(self._job_name_map.keys())

    def get_job_graph_mapping(self) -> dict[str, set[str]]:
        """
        Returns a mapping of head job names to their complete set of job names in their graph.
        
        Returns:
            dict[str, set[str]]: Dictionary mapping each head job name to a set of all job names
                                reachable from that job (including itself).

        Raises:
            TimeoutError: If waiting for jobs to be loaded exceeds timeout
        """
        self.logger.debug("Waiting for jobs to be loaded before returning job graph mapping")
        if not self._jobs_loaded.wait(timeout=self.JOB_MAP_LOAD_TIME):
            raise TimeoutError("Timed out waiting for jobs to be loaded")
        
        return dict(self._job_name_map)

class JobChainFactory:
    _instance = None
    _job_chain = None

    def __init__(self, *args, **kwargs):
        if not JobChainFactory._instance:
            self._job_chain = JobChain(*args, **kwargs)
            JobChainFactory._instance = self

    @classmethod
    def init(cls, start_method="spawn", *args, **kwargs):
      """
      Initializes the JobChainFactory using the given start method.
      args and kwargs are passed down to the JobChain constructor.

      Args:
        start_method: The start method of multiprocessing. Defaults to "spawn".
        args: The parameters to be passed to the JobChain's constructor
        kwargs: The keyword parameters to be passed to the JobChain's constructor
        
      """
      freeze_support()
      set_start_method(start_method)
      if not cls._instance:
        cls._instance = cls(*args, **kwargs)
      return cls._instance

    @staticmethod
    def get_instance()->JobChain:
        if not JobChainFactory._instance:
            raise RuntimeError("JobChainFactory not initialized")
        return JobChainFactory._instance._job_chain


================================================
File: src/jobchain/job_loader.py
================================================
import importlib.util
import inspect
import os
import sys
from pathlib import Path
from typing import Any, Collection, Dict, List, Type, Union

import yaml
from pydantic import BaseModel

from . import jc_logging as logging
from .jc_graph import validate_graph
from .job import JobABC

logger = logging.getLogger(__name__)


class JobValidationError(Exception):
    """Raised when a custom job fails validation"""
    pass


class ConfigurationError(Exception):
    """Exception raised when configuration is malformed."""
    pass


class PythonLoader:
    JOBS = "jobs"
    PYDANTIC = "pydantic"

    @staticmethod
    def validate_pydantic_class(job_class: Type) -> bool:
        """Validate that a class meets the requirements to be a valid pydantic model:
        - Inherits from BaseModel
        """
        return inspect.isclass(job_class) and issubclass(job_class, BaseModel)

    @staticmethod
    def validate_job_class(job_class: Type) -> bool:
        """
        Validate that a class meets the requirements to be a valid job:
        - Inherits from JobABC
        - Has required methods
        - Has required attributes
        """
        # Check if it's a class and inherits from JobABC
        if not (inspect.isclass(job_class) and issubclass(job_class, JobABC)):
            return False

        # Check for required async run method
        if not hasattr(job_class, 'run'):
            return False

        # Check if run method is async
        run_method = getattr(job_class, 'run')
        if not inspect.iscoroutinefunction(run_method):
            return False

        return True

    @classmethod
    def load_python(cls, python_dir: str, type_name: str = JOBS) -> Dict[str, Union[Type[JobABC], Type[BaseModel]]]:
        """
        Load all custom job classes from the specified directory
        """
        python_classes = {}
        python_path = Path(python_dir)

        if not python_path.exists():
            logger.info(f"Python directory not found: {python_dir}")
            return python_classes

        # Add the custom jobs directory to Python path
        logger.debug(f"Python path before: {sys.path}")
        sys.path.append(str(python_path))
        logger.info(f"Added {python_path} to Python path")
        logger.debug(f"Python path after: {sys.path}")

        # Scan for Python files
        for file_path in python_path.glob("**/*.py"):
            if file_path.name.startswith("__"):
                continue

            logger.info(f"Loading python classes from {file_path}")
            try:
                # Load the module
                module_name = file_path.stem
                spec = importlib.util.spec_from_file_location(module_name, str(file_path))
                if spec is None or spec.loader is None:
                    logger.warning(f"Could not create module spec for {file_path}")
                    continue

                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)

                # Find all classes in the module that inherit from JobABC
                for name, obj in inspect.getmembers(module):
                    if inspect.isclass(obj) and obj.__module__ == module.__name__:
                        try:
                            if ((type_name == cls.JOBS and cls.validate_job_class(obj)) or
                                (type_name == cls.PYDANTIC and cls.validate_pydantic_class(obj))):
                                logger.info(f"Found valid python class: {name}")
                                python_classes[name] = obj
                        except Exception as e:
                            logger.error(f"Error validating python class {name} in {file_path}: {str(e)}")
                            raise JobValidationError(
                                f"Error validating python class {name} in {file_path}: {str(e)}"
                            )

            except Exception as e:
                logger.error(f"Error loading custom python class from {file_path}: {str(e)}")
                raise ImportError(
                    f"Error loading custom python class from {file_path}: {str(e)}"
                )

        return python_classes


class JobFactory:
    _job_types_registry: Dict[str, Type[JobABC]] = {}
    _pydantic_types_registry: Dict[str, Type[BaseModel]] = {}
    # Default jobs directory is always checked first
    _default_jobs_dir: str = os.path.join(os.path.dirname(__file__), "jobs") # site-package directory when this is a package
    _cached_job_graphs: List[JobABC] = None

    @classmethod
    def load_python_into_registries(cls, custom_python_dirs: list[str] = None):
        """
        Load and register all custom jobs from specified config directories.
        Will look for jobs in the 'jobs' subdirectory of each config directory.
        Loads jobs from all directories.

        Args:
            custom_python_dirs: List of config directory paths. Jobs will be loaded from the 'jobs' subdirectory
                            of each config directory.
        """
        loader = PythonLoader()
        # Create an iterable of job directories, including the default and any custom directories
        python_dirs = [cls._default_jobs_dir]
        if custom_python_dirs:
            # Add local jobs directories from each config directory
            for config_dir in custom_python_dirs:
                python_dir = os.path.join(config_dir, "jobs")
                if os.path.exists(python_dir):
                    python_dirs.append(python_dir)
            
        found_valid_jobs = False
        for python_dir in python_dirs:
            # Load and register jobs
            jobs = loader.load_python(python_dir, PythonLoader.JOBS)
            if jobs:
                found_valid_jobs = True
                # Register all valid custom jobs
                for job_name, job_class in jobs.items():
                    cls.register_job_type(job_name, job_class)
                    print(f"Registered custom job: {job_name}")
            
            # Load and register pydantic models
            pydantic_models = loader.load_python(python_dir, PythonLoader.PYDANTIC)
            if pydantic_models:
                for model_name, model_class in pydantic_models.items():
                    cls.register_pydantic_type(model_name, model_class)
                    print(f"Registered pydantic model: {model_name}")
            else:
                logger.info("No pydantic classes found")
        
        if not found_valid_jobs:
            # This is a critical error as we need at least one valid job directory
            raise FileNotFoundError(f"No valid jobs found in any of the directories: {python_dirs}")

    @classmethod
    def create_job(cls, name: str, job_type: str, job_def: Dict[str, Any]) -> JobABC:
        if job_type not in cls._job_types_registry:
            logger.error(f"*** Unknown job type: {job_type} ***")
            raise ValueError(f"Unknown job type: {job_type}")
        
        properties = job_def.get('properties', {})
        if not properties:
            logger.info(f"No properties specified for job {name} of type {job_type}")
            
        return cls._job_types_registry[job_type](name, properties)

    @classmethod
    def register_job_type(cls, type_name: str, job_class: Type[JobABC]):
        cls._job_types_registry[type_name] = job_class

    @classmethod
    def register_pydantic_type(cls, type_name: str, model_class: Type[BaseModel]):
        """Register a Pydantic model type with the factory"""
        cls._pydantic_types_registry[type_name] = model_class

    @classmethod
    def get_pydantic_class(cls, type_name: str) -> Type[BaseModel]:
        """Retrieve a registered Pydantic model class by its type name."""
        if type_name not in cls._pydantic_types_registry:
            raise ValueError(f"Pydantic type {type_name} not registered.")
        return cls._pydantic_types_registry[type_name]

    @classmethod
    def get_head_jobs_from_config(cls) -> Collection[JobABC]:
        JobFactory.load_python_into_registries(ConfigLoader.directories)
        """Create job graphs from configuration, using cache if available"""
        if cls._cached_job_graphs is None:
            job_graphs: list[JobABC] = []
            graphs_config = ConfigLoader.get_graphs_config()
            graph_names = list(graphs_config.keys())
            for graph_name in graph_names:
                graph_def = graphs_config[graph_name]
                job_names_in_graph = list(graph_def.keys())
                param_groups_for_graph_name = ConfigLoader.get_parameters_config().get(graph_name, {})
                if param_groups_for_graph_name:
                    param_jobs_graphs: List[JobABC] = cls.create_job_graph_using_parameters(graph_def, graph_name,
                                                                                            param_groups_for_graph_name,
                                                                                            job_names_in_graph)
                    job_graphs += param_jobs_graphs
                else:
                    job_graph_no_params: JobABC = cls.create_job_graph_no_params(graph_def, graph_name,
                                                                                 job_names_in_graph)
                    job_graphs.append(job_graph_no_params)
            cls._cached_job_graphs = job_graphs
        return cls._cached_job_graphs

    @classmethod
    def create_job_graph_using_parameters(cls, graph_def, graph_name, param_groups_for_graph_name,
                                          job_names_in_graph) -> List[JobABC]:
        job_graphs: list[JobABC] = []
        parameter_names_list = list(param_groups_for_graph_name.keys())
        for parameter_name in parameter_names_list:
            job_instances: dict[str, JobABC] = {}
            for graph_job_name in job_names_in_graph:
                raw_job_def: Dict[str, Any] = ConfigLoader.get_jobs_config()[graph_job_name]
                if ConfigLoader.is_parameterized_job(raw_job_def):
                    job_def: Dict[str, Any] = ConfigLoader.fill_job_with_parameters(raw_job_def, graph_name, parameter_name)
                else:
                    job_def = raw_job_def
                unique_job_name = graph_name + "$$" + parameter_name + "$$" + graph_job_name + "$$"
                job_type: str = job_def["type"]
                job: JobABC = cls.create_job(unique_job_name, job_type, job_def)
                job_instances[graph_job_name] = job
            job_graph: JobABC = cls.create_job_graph(graph_def, job_instances)
            job_graphs.append(job_graph)
        return job_graphs

    @classmethod
    def create_job_graph_no_params(cls, graph_def, graph_name, job_names_in_graph)-> JobABC:
        job_instances: dict[str, JobABC] = {}
        for graph_job_name in job_names_in_graph:
                job_def: Dict[str, Any] = ConfigLoader.get_jobs_config()[graph_job_name]
                unique_job_name = graph_name + "$$" + "$$" + graph_job_name +"$$"
                job_type: str = job_def["type"]
                job: JobABC = cls.create_job(unique_job_name, job_type, job_def)
                job_instances[graph_job_name] = job
        job_graph: JobABC = cls.create_job_graph(graph_def, job_instances)
        return job_graph

    @classmethod
    def create_job_graph(cls, graph_definition: dict[str, dict], job_instances: dict[str, JobABC]) -> JobABC:
        """
        graph definition defines the job graph and looks like this:

        graph_definition: dict[str, Any] = {
            "A": {"next": ["B", "C"]},
            "B": {"next": ["D"]},
            "C": {"next": ["D"]},
            "D": {"next": []},
        }

        job instances are a dictionary of job instances in the job graph and looks like this:

        job_instances: dict[str, JobABC] = {
            "A": SimpleJob("A"),
            "B": SimpleJob("B"),
            "C": SimpleJob("C"),
            "D": SimpleJob("D"),
        }
        
        """
        from jobchain.jobs.default_jobs import DefaultHeadJob
        nodes:dict[str, JobABC] = {} # nodes holds Jobs which will be hydrated with next_jobs 
                                    # and expected_inputs fields from the graph_definition.
        for job_name in graph_definition:
            job_obj = job_instances[job_name]
            nodes[job_name] = job_obj

        # determine the incoming edges i.e the Jobs that each Job depends on
        # so we can determine the head node ( which depends on no Jobs) 
        # and set the expected_inputs (i.e. the dependencies) for each Job.
        incoming_edges: dict[str, set[str]] = {job_name: set() for job_name in graph_definition}
        for job_name, config in graph_definition.items():
            for next_job in config['next']:
                incoming_edges[next_job].add(job_name)
        
        # 1) Find the head node (node with no incoming edges)
        head_jobs = [job_name for job_name, inputs in incoming_edges.items() if not inputs]
        
        if len(head_jobs) > 1:
            # Get naming from first job instance in job_instances
            sample_job = next(iter(job_instances.values()))
            sample_name = sample_job.name
            parsed = JobABC.parse_job_name(sample_name)
            
            # Debug logging for sample name and parsed name
            logger.debug(f"DEBUG - Sample name (long): {sample_name}")
            logger.debug(f"DEBUG - Parsed name (short): {parsed}")
            
            if parsed != 'UNSUPPORTED NAME FORMAT':
                # Replace the short job name with "DefaultHeadJob" while maintaining the $$ format
                # Example: "multi_head_demo$$params1$$head_job_alpha$$" -> "multi_head_demo$$params1$$DefaultHeadJob$$"
                new_name = sample_name.replace(parsed + "$$", "DefaultHeadJob$$")
                default_head = DefaultHeadJob(name=new_name)
                logger.debug(f"Constructed DefaultHeadJob name: {new_name}")
            else:
                default_head = DefaultHeadJob()
                logger.warning("Falling back to default naming for head job")
            
            logger.debug(f"Created DefaultHeadJob with name: {default_head.name}")
            job_instances[default_head.name] = default_head
            graph_definition[default_head.name] = {"next": head_jobs}
            head_job_name = default_head.name
            
            # Add the default head job to nodes dictionary
            nodes[default_head.name] = default_head
        elif len(head_jobs) == 1:
            head_job_name = head_jobs[0]
        else:
            raise ValueError("No head nodes found in graph definition")

        # 2) Set next_jobs for each node
        for job_name, config in graph_definition.items():
            nodes[job_name].next_jobs = [nodes[next_name] for next_name in config['next']]

        # 3) Set expected_inputs for each node using fully qualified names
        for job_name, input_job_names_set in incoming_edges.items():
            if input_job_names_set:  # if node has incoming edges
                # Transform short names to fully qualified names using the job_instances dictionary
                nodes[job_name].expected_inputs = {job_instances[input_name].name for input_name in input_job_names_set}

        # 4) Set reference to final node in head node -- not needed!
        # Find node with no next jobs
        # final_job_name = next(job_name for job_name, config in graph_definition.items() 
        #                    if not config['next'])
        # nodes[head_job_name].final_node = nodes[final_job_name]

        return nodes[head_job_name]


class ConfigLoader:
    # Directories are searched in order. If a valid jobchain directory is found,
    # the search stops and uses that directory.
    # TODO: Nice to have - Add support for merging configurations from multiple directories
    #       if required in the future.
    directories: List[str] = [
        os.path.join(os.getcwd(), "jobchain"),  # jobchain directory in current working directory
        os.path.join(os.path.expanduser("~"), "jobchain"),  # ~/jobchain
        "/etc/jobchain"
    ]
    _cached_configs: Dict[str, dict] = None

    @classmethod
    def _set_directories(cls, directories):
        """Set the directories and clear the cache"""
        cls.directories = directories
        cls._cached_configs = None

    @classmethod
    def __setattr__(cls, name, value):
        """Clear cache when directories are changed"""
        super().__setattr__(name, value)
        if name == 'directories':
            cls._cached_configs = None

    @classmethod
    def load_configs_from_dirs(
            cls,
            directories: List[str] = [],
            config_bases: List[str] = ['graphs', 'jobs', 'parameters', 'jobchain_all'],
            allowed_extensions: tuple = ('.yaml', '.yml', '.json')
    ) -> Dict[str, dict]:
        """
        Load configuration files from directories. Will search directories in order and stop
        at the first valid jobchain directory found.
        
        Args:
            directories: List of directory paths to search
            config_bases: List of configuration file base names to look for
            allowed_extensions: Tuple of allowed file extensions
        
        Returns:
            Dictionary with config_base as key and loaded config as value
            
        Raises:
            FileNotFoundError: If no valid jobchain directory is found in any of the directories
            ConfigurationError: If configuration files are malformed
        """
        configs: Dict[str, dict] = {}
        config_files: Dict[str, str] = {}  # Track which file each config came from

        # Convert directories to Path objects
        dir_paths = [Path(str(d)) for d in directories]
        logger.info(f"Looking for config files in directories: {dir_paths}")

        found_valid_dir = False
        for dir_path in dir_paths:
            if not dir_path.exists():
                logger.info(f"Directory not found, skipping: {dir_path}")
                continue

            # Check if any config files exist in this directory
            has_configs = False
            for config_base in config_bases:
                for ext in allowed_extensions:
                    if (dir_path / f"{config_base}{ext}").exists():
                        has_configs = True
                        break
                if has_configs:
                    break

            if has_configs:
                found_valid_dir = True
                logger.info(f"Found valid jobchain directory: {dir_path}")
                # Load configs from this directory only
                for config_base in config_bases:
                    for ext in allowed_extensions:
                        config_path = dir_path / f"{config_base}{ext}"
                        if config_path.exists():
                            try:
                                with open(config_path) as f:
                                    configs[config_base] = yaml.safe_load(f)
                                    config_files[config_base] = str(config_path)
                            except yaml.YAMLError as e:
                                error_msg = "Configuration is malformed. Unable to proceed with job execution.\n\n" \
                                        "-------------------------------------------------------\n" \
                                        "          Configuration file malformed - cannot continue\n" \
                                        "-------------------------------------------------------\n" \
                                        f"File: {config_path}\n" \
                                        f"Error details: {str(e)}\n" \
                                        "-------------------------------------------------------"
                                raise ConfigurationError(error_msg) from e
                break  # Stop searching after finding first valid directory

        if not found_valid_dir:
            raise FileNotFoundError(f"No valid jobchain directory found in search paths: {dir_paths}")

        # Store file paths in configs
        configs['__files__'] = config_files
        return configs

    @classmethod
    def _extract_config_section(cls, configs: Dict[str, dict], section_name: str) -> dict:
        """
        Extract a configuration section from either a dedicated file or jobchain_all.
        
        Args:
            configs: Dictionary containing all configurations
            section_name: Name of the section to extract (e.g., 'graphs', 'jobs', 'parameters')
            
        Returns:
            Dictionary containing the configuration section, or empty dict if not found
        """
        # Try to get from dedicated file first
        if section_name in configs:
            return configs[section_name]

        # If not found, try to get from jobchain_all
        if 'jobchain_all' in configs and isinstance(configs['jobchain_all'], dict):
            return configs['jobchain_all'].get(section_name, {})

        # If nothing found, return empty dict
        return {}

    @classmethod
    def _find_parameterized_fields(cls, job_config: dict) -> set:
        """
        Find all parameterized fields in a job configuration.
        A field is parameterized if its value starts with '$'.
        
        Args:
            job_config: Job configuration dictionary
            
        Returns:
            Set of parameterized field names
        """
        params = set()

        def search_dict(d):
            for k, v in d.items():
                if isinstance(v, dict):
                    search_dict(v)
                elif isinstance(v, str) and v.startswith('$'):
                    params.add(v[1:])  # Remove the '$' prefix

        search_dict(job_config.get('properties', {}))
        return params

    @classmethod
    def _validate_graph_structure(cls, graph_def: dict, defined_jobs: set, graph_name: str) -> None:
        """
        Validate the structure of a job graph.
        - Checks for cycles
        - Ensures all referenced jobs exist
        - Validates head/tail nodes
        
        Args:
            graph_def: Graph definition from configuration
            defined_jobs: Set of all defined job names
            graph_name: Name of the graph being validated
            
        Raises:
            ValueError: If validation fails
        """
        # First validate all jobs exist and are properly connected
        for job, job_def in graph_def.items():
            if job not in defined_jobs:
                raise ValueError(f"Job '{job}' in graph '{graph_name}' is not defined")
            for next_job in job_def.get('next', []):
                if next_job not in defined_jobs:
                    raise ValueError(f"Job '{next_job}' referenced in 'next' field of job '{job}' in graph '{graph_name}' is not defined in jobs configuration")
                    
        # Build adjacency list after validating jobs
        adjacency = {job: job_def.get('next', []) for job, job_def in graph_def.items()}
        
        # Check for cycles using DFS
        visited = set()
        rec_stack = set()
        
        def has_cycle(node):
            visited.add(node)
            rec_stack.add(node)
            
            for neighbor in adjacency[node]:
                if neighbor not in visited:
                    if has_cycle(neighbor):
                        return True
                elif neighbor in rec_stack:
                    return True
                    
            rec_stack.remove(node)
            return False
            
        # Run cycle detection from each unvisited node
        for job in adjacency:
            if job not in visited:
                if has_cycle(job):
                    raise ValueError(f"Cycle detected in graph '{graph_name}'")
                    
    @classmethod
    def validate_configs(cls, configs: Dict[str, dict]) -> None:
        """
        Validate that:
        1. All jobs referenced in graphs exist in jobs configuration
        2. All parameterized jobs have corresponding parameter values
        3. Each graph structure is valid (no cycles, proper head/tail nodes, valid references)
        
        Args:
            configs: Dictionary containing all configurations
            
        Raises:
            ValueError: If validation fails
            ConfigurationError: If configuration is malformed (e.g. wrong types, missing required fields)
        """
        try:
            graphs_config = cls._extract_config_section(configs, 'graphs')
            jobs_config = cls._extract_config_section(configs, 'jobs')
            parameters_config = cls._extract_config_section(configs, 'parameters')
            config_files = configs.get('__files__', {})

            if not graphs_config or not jobs_config:
                return

            # First validate that all jobs in graphs exist
            defined_jobs = set(jobs_config.keys())

            # Track which config we're currently validating
            current_config = 'jobs'
            
            # Validate jobs config structure
            for job_name, job_config in jobs_config.items():
                if not isinstance(job_config, dict):
                    raise TypeError(f"Job '{job_name}' configuration must be a dictionary")
                
            current_config = 'graphs'
            for graph_name, graph_definition in graphs_config.items():
                # Validate graph structure (no cycles, etc)
                print(f"\nChecking {graph_name} for cycles...")
                cls._validate_graph_structure(graph_definition, defined_jobs, graph_name)
                print("No cycles detected")
                validate_graph(graph_definition, graph_name)

                # Find all parameterized jobs in this graph
                graph_parameterized_jobs = {}
                for job_name in graph_definition.keys():
                    job_config = jobs_config[job_name]
                    params = cls._find_parameterized_fields(job_config)
                    if params:
                        graph_parameterized_jobs[job_name] = params

                # If graph has parameterized jobs, it must have parameters
                if graph_parameterized_jobs:
                    current_config = 'parameters'
                    if graph_name not in parameters_config:
                        raise ValueError(
                            f"Graph '{graph_name}' contains parameterized jobs {list(graph_parameterized_jobs.keys())} but has no entry in parameters configuration")

                    parameters_for_graph = parameters_config[graph_name]

                    # Validate parameter groups
                    for param_name in parameters_for_graph.keys():
                        if not param_name.startswith('params'):
                            raise ValueError(
                                f"Invalid parameter group name '{param_name}' in graph '{graph_name}'. Parameter groups must start with 'params'")

                    # Validate that all parameters are filled for each group
                    for param_name, parameterized_jobs in parameters_for_graph.items():
                        for job_name, required_params in graph_parameterized_jobs.items():
                            if job_name not in parameterized_jobs:
                                raise ValueError(
                                    f"Job '{job_name}' in graph '{graph_name}' requires parameters {required_params} but has no entry in parameter group '{param_name}'")

                            # Each job should have a list of parameter sets
                            job_param_sets = parameterized_jobs[job_name]
                            if not isinstance(job_param_sets, list):
                                raise ValueError(
                                    f"Parameters for job '{job_name}' in graph '{graph_name}', group '{param_name}' should be a list of parameter sets")

                            # Validate each parameter set
                            for param_set in job_param_sets:
                                missing_params = required_params - set(param_set.keys())
                                if missing_params:
                                    raise ValueError(
                                        f"Parameter set for job '{job_name}' in graph '{graph_name}', group '{param_name}' is missing required parameters: {missing_params}")

                print(f"Graph {graph_name} passed all validations")

        except (AttributeError, TypeError, KeyError) as e:
            # Get the relevant file path based on which config we were validating
            error_file = config_files.get(current_config, 'unknown file')
            error_msg = "Configuration is malformed. Unable to proceed with job execution.\n\n" \
                        "-------------------------------------------------------\n" \
                        "          Configuration file malformed - cannot continue\n" \
                        "-------------------------------------------------------\n" \
                        f"File: {error_file}\n" \
                        f"Error details: {str(e)}\n" \
                        "-------------------------------------------------------"
            raise ConfigurationError(error_msg) from e

    @classmethod
    def load_all_configs(cls) -> Dict[str, dict]:
        """Load all configurations and validate them"""
        if cls._cached_configs is None:
            cls._cached_configs = cls.load_configs_from_dirs(cls.directories)
            cls.validate_configs(cls._cached_configs)
        return cls._cached_configs

    @classmethod
    def reload_configs(cls) -> Dict[str, dict]:
        """Force a reload of all configurations."""
        logger.info("Reloading configs...")
        cls._cached_configs = None
        return cls.load_all_configs()

    @classmethod
    def get_graphs_config(cls) -> dict:
        """
        Get graphs configuration from either dedicated graphs file or jobchain_all.
        Returns empty dict if no configuration is found.
        """
        configs = cls.load_all_configs()
        return cls._extract_config_section(configs, 'graphs')

    @classmethod
    def get_jobs_config(cls) -> dict:
        """
        Get jobs configuration from either dedicated jobs file or jobchain_all.
        Returns empty dict if no configuration is found.
        """
        configs = cls.load_all_configs()
        return cls._extract_config_section(configs, 'jobs')

    @classmethod
    def get_parameters_config(cls) -> dict:
        """
        Get parameters configuration from either dedicated parameters file or jobchain_all.
        Returns empty dict if no configuration is found.
        """
        configs = cls.load_all_configs()
        return cls._extract_config_section(configs, 'parameters')

    @classmethod
    def is_parameterized_job(cls, raw_job_def):
        """
        Check if a job definition contains parameterized fields.
        
        Args:
            raw_job_def: Raw job definition from jobs.yaml
            
        Returns:
            bool: True if job has parameterized fields, False otherwise
        """
        if not isinstance(raw_job_def, dict):
            return False
            
        # Use existing method to find parameterized fields
        params = cls._find_parameterized_fields(raw_job_def)
        return len(params) > 0

    @classmethod
    def fill_job_with_parameters(cls, job_config: dict, graph_name: str, parameter_name: str) -> dict:
        """
        Fill a job configuration with parameters from parameters.yaml.
        
        Args:
            job_config: Raw job configuration from jobs.yaml
            graph_name: Name of the graph containing the job
            parameter_name: Name of the parameter group to use
            
        Returns:
            dict: Job configuration with parameters filled in
        """
        # Deep copy the job config to avoid modifying the original
        import copy
        filled_config = copy.deepcopy(job_config)
        
        # Get parameters for this job from parameters.yaml
        params_config = cls.get_parameters_config()
        if graph_name not in params_config or parameter_name not in params_config[graph_name]:
            raise ValueError(f"No parameters found for graph '{graph_name}' and parameter group '{parameter_name}'")
            
        # Get the job name by finding which job in the parameters matches this config
        job_name = None
        for job in params_config[graph_name][parameter_name].keys():
            if job_config == cls.get_jobs_config()[job]:
                job_name = job
                break
                
        if job_name is None:
            raise ValueError(f"Could not find job in parameters for graph '{graph_name}' and group '{parameter_name}'")
            
        # Get parameter values for this job
        param_sets = params_config[graph_name][parameter_name][job_name]
        if not param_sets or not isinstance(param_sets, list):
            raise ValueError(f"Invalid parameter sets for job '{job_name}' in graph '{graph_name}', group '{parameter_name}'")
            
        # Use the first parameter set (as defined in the spec)
        param_values = param_sets[0]
        
        def replace_params(obj, params):
            if isinstance(obj, dict):
                return {k: replace_params(v, params) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [replace_params(item, params) for item in obj]
            elif isinstance(obj, str) and obj.startswith('$'):
                param_name = obj[1:]  # Remove '$' prefix
                if param_name not in params:
                    raise ValueError(f"Parameter '{param_name}' not found in parameter set")
                return params[param_name]
            return obj
            
        # Replace all parameterized values in the config
        filled_config = replace_params(filled_config, param_values)
        return filled_config


================================================
File: src/jobchain/taskmanager.py
================================================
import asyncio
import threading
from collections import deque

import jobchain.jc_logging as logging

from .job_loader import JobFactory


class TaskManager:
    _instance = None
    _lock = threading.Lock()  # Class-level lock for singleton creation

    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance.__initialized = False
        return cls._instance

    def __init__(self):
        if not hasattr(self, '_TaskManager__initialized') or not self.__initialized:
            with self._lock:
                if not hasattr(self, '_TaskManager__initialized') or not self.__initialized:
                    self._initialize()
                    self.__initialized = True

    def _initialize(self):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.loop = asyncio.new_event_loop()
        self.thread = threading.Thread(target=self._run_loop, daemon=True)
        self.thread.start()
        self.head_jobs = JobFactory.get_head_jobs_from_config()

        self.submitted_count = 0
        self.completed_count = 0
        self.error_count = 0
        self.completed_results = deque()
        self.error_results = deque()

        self._data_lock = threading.Lock()

    def _run_loop(self):
        asyncio.set_event_loop(self.loop)
        self.loop.run_forever()

    def submit(self, func, *args, **kwargs):
        with self._data_lock:
            self.submitted_count += 1

        try:
            coro = func(*args, **kwargs)
        except Exception as e:
            self.logger.error(f"Error processing result: {e}")
            self.logger.info("Detailed stack trace:", exc_info=True)
            with self._data_lock:
                self.error_count += 1
                self.error_results.append(
                    (e, {'func': func, 'args': args, 'kwargs': kwargs})
                )
            return

        future = asyncio.run_coroutine_threadsafe(coro, self.loop)
        future.add_done_callback(
            lambda f: self._handle_completion(f, func, args, kwargs)
        )

    def _handle_completion(self, future, func, args, kwargs):
        try:
            result = future.result()
            with self._data_lock:
                self.completed_count += 1
                self.completed_results.append(
                    (result, {'func': func, 'args': args, 'kwargs': kwargs})
                )
        except Exception as e:
            self.logger.error(f"Error processing result: {e}")
            self.logger.info("Detailed stack trace:", exc_info=True)
            with self._data_lock:
                self.error_count += 1
                self.error_results.append(
                    (e, {'func': func, 'args': args, 'kwargs': kwargs})
                )

    def get_counts(self):
        with self._data_lock:
            return {
                'submitted': self.submitted_count,
                'completed': self.completed_count,
                'errors': self.error_count
            }

    def pop_results(self):
        with self._data_lock:
            completed = list(self.completed_results)
            errors = list(self.error_results)
            self.completed_results.clear()
            self.error_results.clear()
            return {
                'completed': completed,
                'errors': errors
            }



================================================
File: src/jobchain/jobs/__init__.py
================================================
from jobchain.jobs.llm_jobs import OpenAIJob

__all__ = ['OpenAIJob']

================================================
File: src/jobchain/jobs/default_jobs.py
================================================
from typing import Any, Dict, Union

from jobchain import jc_logging as logging
from jobchain.job import JobABC, Task

logger = logging.getLogger(__name__)


class DefaultHeadJob(JobABC):
    """A Job implementation that provides a simple default behavior."""
    
    async def run(self, task: Union[Dict[str, Any], Task]) -> Dict[str, Any]:
        """Run a simple job that logs and returns the task."""
        logger.info(f"Default head JOB for {task}")
        return {}

================================================
File: src/jobchain/jobs/llm_jobs.py
================================================
import os
from typing import Any, Dict, Optional, Union

from aiolimiter import AsyncLimiter
from dotenv import load_dotenv
from openai import AsyncOpenAI

import jobchain.jc_logging as logging
from jobchain.job import JobABC
from jobchain.job_loader import JobFactory
from jobchain.utils.llm_utils import check_response_errors, clean_prompt

logger = logging.getLogger("OpenAIJob")

class OpenAIClient:
    """
    Singleton class for AsyncOpenAI client.
    """
    _client = None

    @classmethod
    def get_client(cls, params: Dict[str, Any] = None):
        if cls._client is None:
            # Load environment variables from api.env file
            load_dotenv("api.env")

            # Initialize params if None
            params = params or {}

            # Handle special parameters
            api_key = os.getenv(params.pop("api_key", None)) if "api_key" in params else os.getenv('OPENAI_API_KEY')
            logger.info(f"Resolved API Key exists: {bool(api_key)}")
            
            # Optional: Check if the API key is not set and raise an error 
            if not api_key:
                raise ValueError("API key is not set. Please provide an API key.")
            
            # Create client with remaining params
            cls._client = AsyncOpenAI(api_key=api_key, **params)
            logger.info(f"Created client with base_url: {params.get('base_url', 'default')}")
        return cls._client

class OpenAIJob(JobABC):

    # Shared AsyncLimiter for all jobs, default to 5,000 requests per minute
    default_rate_limit = {"max_rate": 5000, "time_period": 60}

    def __init__(self, name: Optional[str] = None, properties: Dict[str, Any] = {}):
        """
        Call JobChain.submit_task({"prompt": prompt}), when submitting a task to the JobChain.
        Initialize an OpenAIJob instance with a properties dict containing three top-level keys, client, api, and rate_limit.
        All properties are optional.

        Args:
            name (Optional[str], optional): 
                A unique identifier for this job within the context of a JobChain.
                The name must be unique among all jobs in the same JobChain to ensure proper job identification 
                and dependency resolution. If not provided, a unique name will be auto-generated.

            properties (Dict[str, Any], optional): Optional properties for the job. A dictionary containing the following keys:

            {
                rate_limit: {
                    max_rate: Allow up to max_rate / time_period acquisitions before blocking.
                    time_period: duration of the time period in which to limit the rate. Note that up to max_rate acquisitions are allowed within this time period in a burst
                },
                client: {
                    api_key: str | None = None,
                    organization: str | None = None,
                    project: str | None = None,
                    base_url: str | URL | None = None,
                    websocket_base_url: str | URL | None = None,
                    timeout: float | Timeout | NotGiven | None = NOT_GIVEN,
                    max_retries: int = DEFAULT_MAX_RETRIES,
                    default_headers: Mapping[str, str] | None = None,
                    default_query: Mapping[str, object] | None = None,
                    http_client: AsyncClient | None = None,
                    _strict_response_validation: bool = False
                },
                api: {
                    messages: Iterable[ChatCompletionMessageParam],
                    model: ChatModel | str,
                    audio: ChatCompletionAudioParam | NotGiven | None = NOT_GIVEN,
                    frequency_penalty: float | NotGiven | None = NOT_GIVEN,
                    function_call: FunctionCall | NotGiven = NOT_GIVEN,
                    functions: Iterable[Function] | NotGiven = NOT_GIVEN,
                    logit_bias: Dict[str, int] | NotGiven | None = NOT_GIVEN,
                    logprobs: bool | NotGiven | None = NOT_GIVEN,
                    max_completion_tokens: int | NotGiven | None = NOT_GIVEN,
                    max_tokens: int | NotGiven | None = NOT_GIVEN,
                    metadata: Dict[str, str] | NotGiven | None = NOT_GIVEN,
                    modalities: List[ChatCompletionModality] | NotGiven | None = NOT_GIVEN,
                    n: int | NotGiven | None = NOT_GIVEN,
                    parallel_tool_calls: bool | NotGiven = NOT_GIVEN,
                    prediction: ChatCompletionPredictionContentParam | NotGiven | None = NOT_GIVEN,
                    presence_penalty: float | NotGiven | None = NOT_GIVEN,
                    reasoning_effort: ChatCompletionReasoningEffort | NotGiven = NOT_GIVEN,
                    response_format: ResponseFormat | NotGiven | None = NOT_GIVEN,
                    seed: int | NotGiven | None = NOT_GIVEN,
                    service_tier: NotGiven | Literal['auto', 'default'] | None = NOT_GIVEN,
                    stop: str | List[str] | NotGiven | None = NOT_GIVEN,
                    store: bool | NotGiven | None = NOT_GIVEN,
                    stream: NotGiven | Literal[False] | None = NOT_GIVEN,
                    stream_options: ChatCompletionStreamOptionsParam | NotGiven | None = NOT_GIVEN,
                    temperature: float | NotGiven | None = NOT_GIVEN,
                    tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven = NOT_GIVEN,
                    tools: Iterable[ChatCompletionToolParam] | NotGiven = NOT_GIVEN,
                    top_logprobs: int | NotGiven | None = NOT_GIVEN,
                    top_p: float | NotGiven | None = NOT_GIVEN,
                    user: str | NotGiven = NOT_GIVEN,
                    extra_headers: Headers | None = None,
                    extra_query: Query | None = None,
                    extra_body: Body | None = None,
                    timeout: float | Timeout | NotGiven | None = NOT_GIVEN
                }
            }
        """
        super().__init__(name, properties)
        
        # Initialize OpenAI client with properties
        self.client = OpenAIClient.get_client(self.properties.get("client", {}))
        
        # Rate limiter configuration
        rate_limit_config = self.properties.get("rate_limit", self.default_rate_limit)
        self.limiter = AsyncLimiter(**rate_limit_config)

        # Extract other relevant properties for OpenAI client
        self.api_properties = self.properties.get("api", {})

    async def run(self, task: Union[Dict[str, Any], Any]) -> Dict[str, Any]:
        """
        Perform an OpenAI API call while adhering to rate limits.
        
        Args:
            task: A dictionary containing either:
                - prompt: str - The prompt to send to the model
                - messages: list - Direct message format for the API
                Or any other valid parameters for the chat.completions.create API
        """
        # Start with default properties
        request_properties = {
            "model": "gpt-4o",
            "temperature": 0.7,
            "messages": [
                {"role": "system", "content": "You are a helpful assistant"},
                {"role": "user", "content": "You are a helpful assistant."}
            ]
        }
        
        # Add API properties from initialization
        request_properties.update(self.api_properties)

        # Check if response_format is a string and replace with the Pydantic class
        if "response_format" in request_properties and isinstance(request_properties["response_format"], str):
            try:
                response_format_name = request_properties["response_format"]
                request_properties["response_format"] = JobFactory.get_pydantic_class(response_format_name)
                logger.info(f"Successfully replaced response_format string with Pydantic class: {response_format_name}")
            except ValueError as e:
                logger.error(f"Could not find Pydantic class for response_format: {request_properties['response_format']}. Error: {e}")
            except Exception as e:
                logger.error(f"An unexpected error occurred while trying to get the Pydantic class: {e}")

        self.create_prompt(request_properties, task)

        # Acquire the rate limiter before making the request
        async with self.limiter:
            try:
                logger.info(f"{self.name} is making an OpenAI API call.")
                if "response_format" in request_properties:
                    response = await self.client.beta.chat.completions.parse(**request_properties)
                else:
                    response = await self.client.chat.completions.create(**request_properties)
                logger.info(f"{self.name} received a response.")
                
                # Handle the response
                if hasattr(response, 'choices') and response.choices:
                    if "response_format" in request_properties:
                        return response.choices[0].message.parsed
                    else:
                        return {"response": response.choices[0].message.content}
                else:
                    return {"error": "No valid response content found"}
            except Exception as e:
                logger.error(f"Error in {self.name}: {e}")
                return {"error": str(e)}

    def create_prompt(self, request_properties, task):
        # Handle the task input
        if isinstance(task, dict):
            # If task has a prompt, convert it to messages format
            if "prompt" in task:
                prompt = clean_prompt(task["prompt"])
                request_properties["messages"] = [
                    {"role": "system", "content": "You are a helpful assistant"},
                    {"role": "user", "content": prompt}
                ]
            # If task already has messages, use those
            elif "messages" in task:
                request_properties["messages"] = task["messages"]

            # Add any other valid API parameters from task
            # request_properties.update({k: v for k, v in task.items() if k not in ["prompt", "messages"]})
        elif task:  # If task is not empty and not a dict
            # If task is not a dict, treat it as the prompt
            prompt = clean_prompt(str(task))
            request_properties["messages"] = [
                {"role": "system", "content": "You are a helpful assistant"},
                {"role": "user", "content": prompt}
            ]

================================================
File: src/jobchain/resources/otel_config.yaml
================================================
exporter: file  # Default exporter is file; can be overridden by OTEL_TRACES_EXPORTER env variable.
service_name: MyService  # Can be overridden by OTEL_SERVICE_NAME env variable.
batch_processor:
  max_queue_size: 1000  # Batch processor will handle up to 1000 spans in queue.
  schedule_delay_millis: 1000  # 1-second timeout for exporting spans.
file_exporter:
  path: "~/.JobChain/otel_trace.json"  # Default path for trace export
  max_size_bytes: 5242880  # 5MB (5 * 1024 * 1024 bytes)
  rotation_time_days: 1  # Rotate daily


================================================
File: src/jobchain/utils/__init__.py
================================================
# Make utils a Python package
from .otel_wrapper import TracerFactory, trace_function
from .timing import timing_decorator

__all__ = ['TracerFactory', 'trace_function', 'timing_decorator']


================================================
File: src/jobchain/utils/llm_utils.py
================================================
import re
import string

import jobchain.jc_logging as logging

logger = logging.getLogger(__name__)

def clean_prompt(text):
    # Keep only printable characters
    return ''.join(char for char in text if char in string.printable)


def clean_prompt(text):
    if not isinstance(text, str):
        logger.error("Input must be a string")
        raise ValueError("Input must be a string")
    
    # Remove control characters but keep normal whitespace
    cleaned_1 = ''.join(char for char in text if ord(char) >= 32 or char in '\n\r\t')
    cleaned = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F]', '', cleaned_1)
    # Optional: Check if the text was modified
    if cleaned != text:
        logger.info("Characters were cleaned from the prompt")
    
    # Optional: Ensure the text isn't empty after cleaning
    if not cleaned.strip():
        logger.error("Prompt is empty after cleaning")
        raise ValueError("Prompt is empty after cleaning")
    
    return cleaned

def check_response_errors(response:dict):
    if response.get("error"):
        logger.error(f"Response has an error: {response}")
        raise ValueError("Response has an error")
    elif response.get("status"):
        status = response.get("status")
        if status == "error":
            logger.error(f"Response has an error: {response}")
            raise ValueError("Response has an error")


================================================
File: src/jobchain/utils/monitor_utils.py
================================================
"""Utilities for monitoring and logging task progress."""
import asyncio

NO_CHANGE_LOG_INTERVAL = 1.0

def should_log_task_stats(monitor_fn, tasks_created: int, tasks_completed: int) -> bool:
    """Check if task stats should be logged based on changes or time elapsed.
    
    Args:
        monitor_fn: The monitoring function to store state on
        tasks_created: Current count of created tasks
        tasks_completed: Current count of completed tasks
        
    Returns:
        bool: True if stats should be logged
    """
    if not hasattr(monitor_fn, '_last_log_time'):
        monitor_fn._last_log_time = 0
        monitor_fn._last_tasks_created = -1
        monitor_fn._last_tasks_completed = -1
    
    current_time = asyncio.get_event_loop().time()
    counts_changed = (tasks_created != monitor_fn._last_tasks_created or 
                     tasks_completed != monitor_fn._last_tasks_completed)
    
    should_log = counts_changed or (current_time - monitor_fn._last_log_time) >= NO_CHANGE_LOG_INTERVAL
    
    if should_log:
        monitor_fn._last_log_time = current_time
        monitor_fn._last_tasks_created = tasks_created
        monitor_fn._last_tasks_completed = tasks_completed
        
    return should_log


================================================
File: src/jobchain/utils/otel_wrapper.py
================================================
import inspect
import json
import os
from functools import wraps
from importlib import resources
from threading import Lock
from typing import Any, Dict, Optional, Sequence

import yaml
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import \
    OTLPSpanExporter
from opentelemetry.sdk.trace import ReadableSpan, TracerProvider
from opentelemetry.sdk.trace.export import (BatchSpanProcessor,
                                            ConsoleSpanExporter, SpanExporter,
                                            SpanExportResult)

# Explicitly define exports
__all__ = ['TracerFactory', 'trace_function', 'AsyncFileExporter']
DEFAULT_OTEL_CONFIG = "otel_config.yaml"

class AsyncFileExporter(SpanExporter):
    """Asynchronous file exporter for OpenTelemetry spans with log rotation support."""
    
    def __init__(self, filepath: str, max_size_bytes: int = None, rotation_time_days: int = None):
        """Initialize the exporter with the target file path and rotation settings.
        
        Args:
            filepath: Path to the file where spans will be exported
            max_size_bytes: Maximum file size in bytes before rotation
            rotation_time_days: Number of days before rotating file
        """
        self.filepath = os.path.expanduser(filepath)
        self.max_size_bytes = max_size_bytes
        self.rotation_time_days = rotation_time_days
        self.last_rotation_time = None
        
        # Ensure directory exists
        os.makedirs(os.path.dirname(self.filepath), exist_ok=True)
        self._export_lock = Lock()
        
        # Initialize file with empty array if it doesn't exist
        if not os.path.exists(self.filepath):
            with open(self.filepath, 'w') as f:
                json.dump([], f)
                
        # Record initial rotation time
        if self.rotation_time_days:
            self.last_rotation_time = os.path.getmtime(self.filepath)
    
    def _should_rotate(self, additional_size: int = 0) -> bool:
        """Check if file should be rotated based on size or time.
        
        Args:
            additional_size: Additional size in bytes that will be added
        """
        if not os.path.exists(self.filepath):
            return False
            
        should_rotate = False
        
        # Check size-based rotation
        if self.max_size_bytes:
            current_size = os.path.getsize(self.filepath)
            if (current_size + additional_size) >= self.max_size_bytes:
                should_rotate = True
                
        # Check time-based rotation
        if self.rotation_time_days and self.last_rotation_time:
            current_time = os.path.getmtime(self.filepath)
            days_elapsed = (current_time - self.last_rotation_time) / (24 * 3600)
            if days_elapsed >= self.rotation_time_days:
                should_rotate = True
                
        return should_rotate
    
    def _rotate_file(self):
        """Rotate the current file if it exists."""
        if not os.path.exists(self.filepath):
            return
            
        # Generate rotation suffix based on timestamp
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        rotated_path = f"{self.filepath}.{timestamp}"
        
        # Rotate the file
        os.rename(self.filepath, rotated_path)
        
        # Create new empty file
        with open(self.filepath, 'w') as f:
            json.dump([], f)
            
        # Update rotation time
        self.last_rotation_time = os.path.getmtime(self.filepath)
    
    def _serialize_span(self, span: ReadableSpan) -> dict:
        """Convert a span to a JSON-serializable dictionary.
        
        Args:
            span: The span to serialize
        Returns:
            dict: JSON-serializable representation of the span
        """
        return {
            'name': span.name,
            'context': {
                'trace_id': format(span.context.trace_id, '032x'),
                'span_id': format(span.context.span_id, '016x'),
            },
            'parent_id': format(span.parent.span_id, '016x') if span.parent else None,
            'start_time': span.start_time,
            'end_time': span.end_time,
            'attributes': dict(span.attributes),
            'events': [
                {
                    'name': event.name,
                    'timestamp': event.timestamp,
                    'attributes': dict(event.attributes)
                }
                for event in span.events
            ],
            'status': {
                'status_code': str(span.status.status_code),
                'description': span.status.description
            }
        }

    def export(self, spans: Sequence[ReadableSpan]) -> SpanExportResult:
        """Export spans to file with rotation support.
        
        Args:
            spans: Sequence of spans to export
        Returns:
            SpanExportResult indicating success or failure
        """
        try:
            with self._export_lock:
                # Create serializable span data
                span_data = [self._serialize_span(span) for span in spans]
                
                # Read existing spans
                try:
                    with open(self.filepath, 'r') as f:
                        try:
                            existing_spans = json.load(f)
                        except json.JSONDecodeError:
                            existing_spans = []
                except FileNotFoundError:
                    existing_spans = []
                
                # Calculate size of new data
                new_data = existing_spans + span_data
                new_data_str = json.dumps(new_data, indent=2)
                additional_size = len(new_data_str.encode('utf-8'))
                
                # Check rotation after calculating new size
                if self._should_rotate(additional_size - os.path.getsize(self.filepath) if os.path.exists(self.filepath) else 0):
                    self._rotate_file()
                    existing_spans = []
                
                # Append new spans
                existing_spans.extend(span_data)
                
                # Write all spans back to file
                temp_file = f"{self.filepath}.tmp"
                try:
                    with open(temp_file, 'w') as f:
                        json.dump(existing_spans, f, indent=2)
                    # Atomic replace
                    os.replace(temp_file, self.filepath)
                finally:
                    if os.path.exists(temp_file):
                        os.unlink(temp_file)
                
            return SpanExportResult.SUCCESS
        except Exception as e:
            print(f"Error exporting spans to file: {e}")
            return SpanExportResult.FAILURE

    def shutdown(self) -> None:
        """Shutdown the exporter."""
        pass

class TestTracerProvider(TracerProvider):
    _instance = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(TestTracerProvider, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if not hasattr(self, '_initialized'):
            super().__init__()
            self._initialized = True
            
    def get_tracer(
        self,
        instrumenting_module_name: str,
        instrumenting_library_version: str = None,
        schema_url: str = None,
        attributes: dict = None,
    ) -> trace.Tracer:
        """Get a tracer for use in tests.
        
        Args:
            instrumenting_module_name: The name of the instrumenting module
            instrumenting_library_version: Optional version of the instrumenting module
            schema_url: Optional URL of the OpenTelemetry schema
            attributes: Optional attributes to add to the tracer
            
        Returns:
            A tracer instance for use in tests
        """
        return super().get_tracer(
            instrumenting_module_name,
            instrumenting_library_version,
            schema_url,
            attributes,
        )

# Singleton TracerFactory
class TracerFactory:
    _instance = None
    _config = None
    _lock = Lock()
    _is_test_mode = False
    
    @classmethod
    def set_test_mode(cls, enabled: bool = True):
        """Enable or disable test mode.
        
        Args:
            enabled: Whether to enable test mode
        """
        cls._is_test_mode = enabled
        cls._instance = None  # Reset instance to force recreation with new provider
    
    @classmethod
    def _load_config(cls, yaml_file=None):
        """Load configuration from YAML file.
        
        Args:
            yaml_file: Optional path override for the YAML configuration file
        Returns:
            dict: Configuration dictionary
        """
       # First try yaml_file parameter
        config_path = yaml_file
        if not config_path:
            # Then try environment variable
            config_path = os.environ.get('JOBCHAIN_OT_CONFIG', "")
        
        if not config_path:
            # Finally use default path from package resources
            try:
                with resources.path('jobchain.resources', DEFAULT_OTEL_CONFIG) as path:
                    config_path = str(path)
            except Exception as e:
                raise RuntimeError(f"Could not find {DEFAULT_OTEL_CONFIG} in package resources: {e}")
        
        with open(config_path, 'r') as file:
                cls._config = yaml.safe_load(file)
        return cls._config
    
    @classmethod
    def get_tracer(cls, config=None):
        """Get or create the tracer instance.
        
        Args:
            config: Optional configuration override. If not provided, loads from file.
        Returns:
            Tracer instance
        """
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    # Use provided config or load from file
                    cfg = config if config is not None else cls._load_config()
                    
                    # Use TestTracerProvider in test mode
                    provider = TestTracerProvider() if cls._is_test_mode else TracerProvider()
                    
                    # Configure main exporter
                    main_exporter = cls._configure_exporter(cfg['exporter'])
                    batch_processor = BatchSpanProcessor(
                        main_exporter,
                        max_queue_size=cfg['batch_processor']['max_queue_size'],
                        schedule_delay_millis=cfg['batch_processor']['schedule_delay_millis']
                    )
                    provider.add_span_processor(batch_processor)
                    
                    trace.set_tracer_provider(provider)
                    cls._instance = trace.get_tracer(cfg["service_name"])
        return cls._instance

    @staticmethod
    def _configure_exporter(exporter_type):
        """Configure the appropriate exporter based on type.
        
        Args:
            exporter_type: Type of exporter to configure
        Returns:
            Configured exporter instance
        """
        if exporter_type == "otlp":
            return OTLPSpanExporter()  # OTEL_EXPORTER_OTLP_... environment variables apply here
        elif exporter_type == "console":
            return ConsoleSpanExporter()  # OTEL_EXPORTER_CONSOLE_... environment variables apply here
        elif exporter_type == "file":
            # Load config to get file path
            config = TracerFactory._load_config()
            file_path = config.get('file_exporter', {}).get('path', "~/.JobChain/otel_trace.json")
            max_size_bytes = config.get('file_exporter', {}).get('max_size_bytes')
            rotation_time_days = config.get('file_exporter', {}).get('rotation_time_days')
            return AsyncFileExporter(file_path, max_size_bytes, rotation_time_days)
        else:
            raise ValueError("Unsupported exporter type")

    @classmethod
    def trace(cls, message: str, detailed_trace: bool = False, attributes: Optional[Dict[str, Any]] = None):
        """Trace a message with OpenTelemetry tracing.
        
        Args:
            message: The message to trace
            detailed_trace: Whether to include detailed tracing information (args, kwargs, object fields)
            attributes: Optional dictionary of additional attributes to add to the span
        """
        tracer = cls.get_tracer()
        
        # Get the calling frame
        frame = inspect.currentframe()
        if frame:
            caller_frame = frame.f_back
            if caller_frame:
                # Get function info
                func_name = caller_frame.f_code.co_name
                module_name = inspect.getmodule(caller_frame).__name__ if inspect.getmodule(caller_frame) else "__main__"
                
                # Get local variables including 'self' if it exists
                local_vars = caller_frame.f_locals
                args = []
                kwargs = {}
                
                # If this is a method call (has 'self')
                if 'self' in local_vars:
                    args.append(local_vars['self'])
                    # Add other arguments if they exist
                    if len(local_vars) > 1:
                        # Filter out 'self' and get remaining arguments
                        args.extend([v for k, v in local_vars.items() if k != 'self'])
                
                span_name = f"{module_name}.{func_name}"
                with tracer.start_as_current_span(span_name) as span:
                    span.set_attribute("trace.message", message)
                    if detailed_trace:
                        span.set_attribute("function.args", str(tuple(args)))
                        span.set_attribute("function.kwargs", str(kwargs))
                        if args and hasattr(args[0], "__dict__"):
                            span.set_attribute("object.fields", str(vars(args[0])))
                    if attributes:
                        for key, value in attributes.items():
                            span.set_attribute(key, str(value))
                    print(message)
                
                # Clean up
                del frame
                del caller_frame
                return
        
        # Fallback if not in a function context
        with tracer.start_as_current_span("trace_message") as span:
            span.set_attribute("trace.message", message)
            if attributes:
                for key, value in attributes.items():
                    span.set_attribute(key, str(value))
            print(message)

# Decorator for OpenTelemetry tracing
def trace_function(func=None, *, detailed_trace: bool = False, attributes: Optional[Dict[str, Any]] = None):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            tracer = TracerFactory.get_tracer()
            span_name = f"{func.__module__}.{func.__name__}"
            with tracer.start_as_current_span(span_name) as span:
                # Record function arguments only if detailed_trace is True
                if detailed_trace:
                    span.set_attribute("function.args", str(args))
                    span.set_attribute("function.kwargs", str(kwargs))
                    if args and hasattr(args[0], "__dict__"):
                        span.set_attribute("object.fields", str(vars(args[0])))
                if attributes:
                    for key, value in attributes.items():
                        span.set_attribute(key, str(value))
                try:
                    result = func(*args, **kwargs)
                    return result
                except Exception as e:
                    span.record_exception(e)
                    raise
        return wrapper
    
    if func is None:
        return decorator
    return decorator(func)


================================================
File: src/jobchain/utils/print_utils.py
================================================
from .. import jc_logging as logging


def printh(text):
    """
    Log the given text surrounded by asterisks.
    """
    logger = logging.getLogger('PrintUtils')
    logger.info("*** " + text + " ***")


================================================
File: src/jobchain/utils/timing.py
================================================
import time
from functools import wraps

def timing_decorator(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.perf_counter()
        result = func(*args, **kwargs)
        end_time = time.perf_counter()
        elapsed_time = end_time - start_time
        print(f"Elapsed time: {elapsed_time:.6f} seconds")
        return result
    return wrapper


