================================================
FILE: src/__init__.py
================================================



================================================
FILE: src/flow4ai/__init__.py
================================================
"""
Flow4AI - A scalable AI job scheduling and execution platform
"""

from . import f4a_logging
from .flowmanagerMP import FlowManagerMP
from .job import JobABC

__all__ = ['JobABC', 'FlowManagerMP', 'f4a_logging']



================================================
FILE: src/flow4ai/dsl.py
================================================
from functools import reduce
from typing import Any, Dict, List, Union

from . import f4a_logging as logging
from .job import JobABC
from .jobs.wrapping_job import WrappingJob

logger = logging.getLogger(__name__)

# Type definitions for DSL components
DSLComponent = Union[JobABC, 'Parallel', 'Serial']
JobsDict = Dict[str, JobABC]

class Parallel:
    def __init__(self, *components):
        self.components = components
        self.obj = None  # No direct object for this composite

    def __or__(self, other):
        """Support chaining with | operator"""
        if not isinstance(other, (JobABC, Parallel, Serial)):
            other = WrappingJob(other)
        
        return Parallel(*list(self.components) + [other])
        
    def __rshift__(self, other):
        """Support chaining with >> operator"""
        if not isinstance(other, (JobABC, Parallel, Serial)):
            other = WrappingJob(other)
            
        return Serial(self, other)

    def __repr__(self):
        return f"parallel({', '.join(repr(c) for c in self.components)})"

class Serial:
    def __init__(self, *components):
        self.components = components
        self.obj = None  # No direct object for this composite
        
    def __or__(self, other):
        """Support chaining with | operator"""
        if not isinstance(other, (JobABC, Parallel, Serial)):
            other = WrappingJob(other)
            
        return Parallel(self, other)
        
    def __rshift__(self, other):
        """Support chaining with >> operator"""
        if not isinstance(other, (JobABC, Parallel, Serial)):
            other = WrappingJob(other)
            
        return Serial(*list(self.components) + [other])
        
    def __repr__(self):
        return f"serial({', '.join(repr(c) for c in self.components)})"


def wrap(obj=None, **kwargs):
    """
    Wrap any object to enable direct graph operations with | and >> operators.
    
    This function is the key to enabling the clean syntax:
    wrap(obj1) | wrap(obj2)  # For parallel composition
    wrap(obj1) >> wrap(obj2)  # For serial composition
    
    Enhanced functionality:
    1. Single object wrapping:
       - For JobABC instances: sets the name property and returns the instance
       - For Serial/Parallel: returns the object unchanged
       - For other objects: creates a WrappingJob with the given name
    
    2. dict (kwargs) object wrapping:
       wrap(obj_a_name=obj_a, obj_b_name=obj_b) or wrap({"obj_a_name": obj_a, "obj_b_name": obj_b})
       - Returns a collection of wrapped objects following the rules in case 1
       - If only one item, returns a dict with the name as key and the wrapped object as value
    """
    # Case 1: Only keyword arguments provided (no positional argument)
    if obj is None and kwargs:
        # Process keyword arguments
        result = {}
        for name, value in kwargs.items():
            if isinstance(value, JobABC):
                value.name = name
                result[name] = value
            elif isinstance(value, (Parallel, Serial)):
                result[name] = value
            else:
                result[name] = WrappingJob(value, name)
        
        # If only one item, return just that item
        if len(result) == 1:
            return next(iter(result.values()))
        return result
    
    # Case 2: Dictionary passed as the first argument
    if isinstance(obj, dict):
        result = {}
        for name, value in obj.items():
            if isinstance(value, JobABC):
                value.name = name
                result[name] = value
            elif isinstance(value, (Parallel, Serial)):
                result[name] = value
            else:
                result[name] = WrappingJob(value, name)
        
        # If only one item, return just that item
        if len(result) == 1:
            return next(iter(result.values()))
        return result
    
    # Case 3: Original behavior - single object
    # Handle the case where obj is None (could happen if called with wrap())
    if obj is None:
        raise ValueError("wrap() requires at least one argument")
        
    if isinstance(obj, (JobABC, Parallel, Serial)):
        return obj  # Already has the operations we need
    return WrappingJob(obj)

# Synonym for wrap
w = wrap

def parallel(*objects, **kwargs):
    """
    Create a parallel composition from multiple objects.
    
    This utility function takes objects (which can be a mix of JobABC
    instances and regular objects) and creates a parallel composition of all of them.
    
    Example:
        graph = parallel(obj1, obj2, obj3)  # Equivalent to wrap(obj1) | wrap(obj2) | wrap(obj3)
        
        # Also supports list argument for backward compatibility
        objects = [obj1, obj2, obj3]
        graph = parallel(objects)  # Still works if a single list is passed
        
        # Named objects using kwargs
        graph = parallel(object_a_name=object_a, object_b_name=object_b)
        
        # Named objects using a dictionary
        graph = parallel({"object_a_name": object_a, "object_b_name": object_b})
    """
    # Case 1: Only keyword arguments provided (no positional arguments)
    if not objects and kwargs:
        # Wrap each item with its name and then combine them with the | operator
        wrapped_items = wrap(**kwargs)
        if not isinstance(wrapped_items, dict):
            # If wrap returned a single item (not a dict), return it
            return wrapped_items
            
        if not wrapped_items:
            raise ValueError("Cannot create a parallel composition from empty arguments")
        
        # Convert dictionary to a list of items
        items = list(wrapped_items.values())
        if len(items) == 1:
            return items[0]
        
        # Combine all items with the | operator
        return reduce(lambda acc, obj: acc | obj, items[1:], items[0])
    
    # Case 2: Dictionary passed as the first argument
    if len(objects) == 1 and isinstance(objects[0], dict) and not kwargs:
        # Wrap each item with its name and then combine them with the | operator
        wrapped_items = wrap(objects[0])
        if not isinstance(wrapped_items, dict):
            # If wrap returned a single item (not a dict), return it
            return wrapped_items
            
        if not wrapped_items:
            raise ValueError("Cannot create a parallel composition from empty arguments")
        
        # Convert dictionary to a list of items
        items = list(wrapped_items.values())
        if len(items) == 1:
            return items[0]
        
        # Combine all items with the | operator
        return reduce(lambda acc, obj: acc | obj, items[1:], items[0])
    
    # Case 3: Original behavior - using positional arguments
    # Handle case where a single list is passed (for backward compatibility)
    if len(objects) == 1 and isinstance(objects[0], list):
        objects = objects[0]
        
    if not objects:
        raise ValueError("Cannot create a parallel composition from empty arguments")
    if len(objects) == 1:
        return wrap(objects[0])
    return reduce(lambda acc, obj: acc | wrap(obj), objects[1:], wrap(objects[0]))

# Synonym for parallel
p = parallel

def serial(*objects, **kwargs):
    """
    Create a serial composition from multiple objects.
    
    This utility function takes objects (which can be a mix of JobABC
    instances and regular objects) and creates a serial composition of all of them.
    
    Example:
        graph = serial(obj1, obj2, obj3)  # Equivalent to wrap(obj1) >> wrap(obj2) >> wrap(obj3)
        
        # Also supports list argument for backward compatibility
        objects = [obj1, obj2, obj3]
        graph = serial(objects)  # Still works if a single list is passed
        
        # Named objects using kwargs
        graph = serial(object_a_name=object_a, object_b_name=object_b)
        
        # Named objects using a dictionary
        graph = serial({"object_a_name": object_a, "object_b_name": object_b})
    """
    # Case 1: Dictionary argument
    if len(objects) == 1 and isinstance(objects[0], dict) and not kwargs:
        wrapped_items = {name: wrap({name: obj}) for name, obj in objects[0].items()}
        if not wrapped_items:
            raise ValueError("Cannot create a serial composition from empty arguments")
        if len(wrapped_items) == 1:
            return list(wrapped_items.values())[0]
        items = list(wrapped_items.values())
        return reduce(lambda acc, obj: acc >> obj, items[1:], items[0])
        
    # Case 2: Kwargs provided (object_name=object syntax)
    if kwargs:
        wrapped_items = {name: wrap({name: obj}) for name, obj in kwargs.items()}
        if not wrapped_items:
            raise ValueError("Cannot create a serial composition from empty arguments")
        if len(wrapped_items) == 1:
            return list(wrapped_items.values())[0]
        items = list(wrapped_items.values())
        return reduce(lambda acc, obj: acc >> obj, items[1:], items[0])
    
    # Case 3: Original behavior - using positional arguments
    # Handle case where a single list is passed (for backward compatibility)
    if len(objects) == 1 and isinstance(objects[0], list):
        objects = objects[0]
        
    if not objects:
        raise ValueError("Cannot create a serial composition from empty arguments")
    if len(objects) == 1:
        return wrap(objects[0])
    return reduce(lambda acc, obj: acc >> wrap(obj), objects[1:], wrap(objects[0]))

# Synonym for serial
s = serial

# Graph evaluation utilities have been moved to tests/test_utils/graph_evaluation.py


================================================
FILE: src/flow4ai/dsl_graph.py
================================================
from typing import Dict, List, Tuple

from . import f4a_logging as logging
from .dsl import JobsDict, Parallel, Serial
from .job import JobABC
from .jobs.wrapping_job import WrappingJob

logger = logging.getLogger(__name__)


PrecedenceGraph = Dict[str, Dict[str, List[str]]]

def dsl_to_precedence_graph(dsl_obj) -> Tuple[PrecedenceGraph, JobsDict]:
    """
    Convert a DSL object into a precedence graph with nested dictionary.
    
    Args:
        dsl_obj: A DSL object created with operators >> and |, or functions p() and s()
    
    Returns:
        Dict[str, Dict[str, List[str]]]: A graph definition in the format:
        {
            'Task A': {'next': ['Task B', 'Task C']},
            'Task B': {'next': []},
            ...
        }
        Where keys are node string representations and values are dictionaries with 'next' key
        pointing to lists of successor node representations.
    """
    # Print the DSL object structure details to help with debugging and understanding
    if logger.getEffectiveLevel() == logging.DEBUG:
        debug_dsl_structure(dsl_obj)
    
    # Extract all unique job objects from the DSL structure
    jobs = extract_jobs(dsl_obj)
    logger.info(f"Extracted {len(jobs)} jobs from DSL")
    
    # Initialize the graph with empty adjacency nested dictionaries using string representation of jobs
    graph = {job.name: {'next': []} for job in jobs}
    jobs: JobsDict = {job.name: job for job in jobs}
    
    # Build connections based on DSL structure using the new format
    build_connections(dsl_obj, graph, nested=True)
    
    return graph, jobs



def extract_jobs(dsl_obj):
    """
    Extract all individual job objects from a DSL structure.
    
    Args:
        dsl_obj: The DSL object to extract jobs from
        
    Returns:
        list: List of unique job objects
    """
    jobs = []
    
    def _extract(obj):
        if isinstance(obj, (Serial, Parallel)):
            # For compositional structures, extract jobs from each component
            for comp in obj.components:
                _extract(comp)
        elif isinstance(obj, JobABC):
            # Recursively extract jobs from wrapped compositional structures
            if isinstance(obj, WrappingJob) and isinstance(obj.callable, (Serial, Parallel)):
                _extract(obj.callable)
            # Terminal job object
            elif obj not in jobs:
                jobs.append(obj)
        else:
            # This is a primitive value that will be auto-wrapped
            wrapped = WrappingJob(obj)
            if wrapped not in jobs:
                jobs.append(wrapped)
    
    _extract(dsl_obj)
    return jobs



def build_connections(dsl_obj, graph, nested=False):
    """
    Build the connections in the graph based on the DSL structure.
    
    Args:
        dsl_obj: The DSL object to analyze
        graph: The graph to build connections in where keys and values are string representations of jobs
        nested: If True, the graph uses the nested format {'next': [...]} for edges instead of direct lists
    """
    def _process_serial(serial_obj, prev_terminals=None):
        if not serial_obj.components:
            return []
        
        # Start with the first component
        curr_terminals = []
        for i, comp in enumerate(serial_obj.components):
            # Process the current component
            if i == 0:
                # First component
                curr_terminals = _process_component(comp, prev_terminals)
            else:
                # Connect previous terminals to the current component
                curr_terminals = _process_component(comp, curr_terminals)
        
        return curr_terminals
    
    def _process_parallel(parallel_obj, prev_terminals=None):
        if not parallel_obj.components:
            return []
        
        all_terminals = []
        for comp in parallel_obj.components:
            # Process each component in parallel
            comp_terminals = _process_component(comp, prev_terminals)
            all_terminals.extend(comp_terminals)
        
        return all_terminals
    
    def _process_component(comp, prev_terminals=None):
        # Handle different types of components
        if isinstance(comp, Serial):
            return _process_serial(comp, prev_terminals)
        elif isinstance(comp, Parallel):
            return _process_parallel(comp, prev_terminals)
        elif isinstance(comp, JobABC):
            if isinstance(comp, WrappingJob):
                # Check if the wrapped object is a compositional structure
                wrapped = comp.callable
                if isinstance(wrapped, (Serial, Parallel)):
                    return _process_component(wrapped, prev_terminals)
            
            # This is a terminal job object (either WrappingJob with non-compositional object
            # or any other JobABC subclass)
            comp_str = comp.name
            
            # Connect previous terminals to this component
            if prev_terminals:
                for term in prev_terminals:
                    term_str = term.name
                    if nested:
                        if comp_str not in graph[term_str]['next']:
                            graph[term_str]['next'].append(comp_str)
                    else:
                        if comp_str not in graph[term_str]:
                            graph[term_str].append(comp_str)
            
            return [comp]
        else:
            # This is a primitive value that will be auto-wrapped
            wrapped = WrappingJob(comp)
            comp_str = wrapped.name
            
            # Connect previous terminals to this component
            if prev_terminals:
                for term in prev_terminals:
                    term_str = term.name
                    if nested:
                        if comp_str not in graph[term_str]['next']:
                            graph[term_str]['next'].append(comp_str)
                    else:
                        if comp_str not in graph[term_str]:
                            graph[term_str].append(comp_str)
            
            return [wrapped]
    
    # Start processing from the top-level DSL object
    _process_component(dsl_obj)

def debug_dsl_structure(dsl_obj, indent=0):
    """
    Print the structure of a DSL object for debugging purposes.
    
    Args:
        dsl_obj: The DSL object to analyze
        indent: Current indentation level for nested printing
    """
    indent_str = "  " * indent
    
    if isinstance(dsl_obj, Serial):
        print(f"{indent_str}Serial with {len(dsl_obj.components)} components")
        for i, comp in enumerate(dsl_obj.components):
            print(f"{indent_str}Component {i}:")
            debug_dsl_structure(comp, indent + 1)
    
    elif isinstance(dsl_obj, Parallel):
        print(f"{indent_str}Parallel with {len(dsl_obj.components)} components")
        for i, comp in enumerate(dsl_obj.components):
            print(f"{indent_str}Component {i}:")
            debug_dsl_structure(comp, indent + 1)
    
    elif isinstance(dsl_obj, WrappingJob):
        print(f"{indent_str}WrappingJob wrapping: {dsl_obj.callable}")
    
    else:
        print(f"{indent_str}Other Type: {type(dsl_obj).__name__} - Value: {dsl_obj}")



def visualize_graph(graph):
    """
    Visualize the graph structure for debugging purposes.
    Displays parent nodes before their children for better readability.
    Ensures a node is only displayed after all of its parents have been processed.
    
    Args:
        graph: A graph definition, either in:
            - Adjacency list format Dict[str, List[str]]
            - Or nested dictionary format Dict[str, Dict[str, List[str]]] with 'next' key
    """
    print("Graph Structure:")
    
    # Check if we're using the nested format
    is_nested = all(isinstance(node_data, dict) and 'next' in node_data for node_data in graph.values())
    
    # Build a reverse graph (child -> parents) to track parent relationships
    reverse_graph = {}
    for node in graph:
        reverse_graph[node] = []
    
    # Find all parent-child relationships
    for parent, node_data in graph.items():
        # Get children based on the graph format
        if is_nested:
            children = node_data['next']
        else:
            children = node_data
            
        for child in children:
            if child not in reverse_graph:
                reverse_graph[child] = []
            reverse_graph[child].append(parent)
    
    # Use Kahn's algorithm for topological sorting
    # Identify nodes with no parents (root nodes)
    root_nodes = []
    for node, parents in reverse_graph.items():
        if not parents and node in graph:  # Only include nodes that are in the original graph
            root_nodes.append(node)
    
    # Process nodes in order, ensuring a node is only processed after all its parents
    node_order = []
    while root_nodes:
        node = root_nodes.pop(0)  # Get a node with no unprocessed parents
        node_order.append(node)
        
        # Process this node's children
        if is_nested:
            children = graph.get(node, {}).get('next', [])
        else:
            children = graph.get(node, [])
            
        for child in children:
            # Remove this parent from the child's parent list
            reverse_graph[child].remove(node)
            # If the child has no more unprocessed parents, add it to the root nodes
            if not reverse_graph[child]:
                root_nodes.append(child)
    
    # Check for cycles or disconnected components
    remaining_nodes = [node for node in graph if node not in node_order]
    if remaining_nodes:
        # For disconnected components, process them separately
        for node in sorted(remaining_nodes):
            if node not in node_order:  # Skip if already added through processing
                node_order.append(node)
    
    # Print the graph in the calculated order
    for node in node_order:
        if is_nested:
            next_nodes = graph.get(node, {}).get('next', [])
            print(f"{node}: {{'next': {next_nodes}}}")
        else:
            next_nodes = graph.get(node, [])
            if next_nodes:
                print(f"{node}: {next_nodes}")
            else:
                print(f"{node}: []")



if __name__ == "__main__":
   pass


================================================
FILE: src/flow4ai/f4a_graph.py
================================================
"""
Flow4AI Graph module for handling directed acyclic graphs with subgraphs.
Provides functionality for graph traversal, cycle detection, and validation.
"""

from typing import Any, Dict, List, Optional, Set, Tuple

from . import f4a_logging

logging = f4a_logging.getLogger(__name__)


def has_cycle(graph: Dict[str, Dict[str, Any]], node: str, 
             visited: Optional[Set[str]] = None, path: Optional[Set[str]] = None) -> Tuple[bool, List[str]]:
    """
    Check if the graph has a cycle starting from the given node.
    
    Args:
        graph: The graph structure to check
        node: Starting node for cycle detection
        visited: Set of all visited nodes (for recursive calls)
        path: Set of nodes in current path (for cycle detection)
    
    Returns:
        Tuple[bool, List[str]]: (has_cycle, cycle_path)
    """
    if visited is None:
        visited = set()
    if path is None:
        path = set()
    
    visited.add(node)
    path.add(node)
    
    node_obj = graph[node]
    # Check next nodes
    for next_node in node_obj.get('next', []):
        if next_node in path:  # Cycle detected
            return True, [*path, next_node]
        if next_node not in visited:
            has_cycle_result, cycle_path = has_cycle(graph, next_node, visited, path)
            if has_cycle_result:
                return True, cycle_path
    
    # Check subgraph if it exists
    if 'subgraph' in node_obj:
        subgraph = node_obj['subgraph']
        for subnode in subgraph:
            if subnode not in visited:
                has_cycle_result, cycle_path = has_cycle(subgraph, subnode, visited, path)
                if has_cycle_result:
                    return True, cycle_path
    
    path.remove(node)
    return False, []

def check_graph_for_cycles(graph: Dict[str, Dict[str, Any]], name: str = "") -> bool:
    """
    Check entire graph for cycles.
    
    Args:
        graph: The graph structure to check
        name: Optional name for the graph (for logging)
    
    Returns:
        bool: True if cycles were found, False otherwise
    """
    print(f"\nChecking {name} for cycles...")
    for node in graph:
        has_cycle_result, cycle_path = has_cycle(graph, node)
        if has_cycle_result:
            print(f"Cycle detected! Path: {' -> '.join(cycle_path)}")
            return True
    print("No cycles detected")
    return False

def find_node_and_graph(main_graph: Dict[str, Dict[str, Any]], target_node: str, 
                       current_graph: Optional[Dict[str, Dict[str, Any]]] = None) -> Tuple[Optional[Dict[str, Dict[str, Any]]], List[str]]:
    """
    Recursively finds a node and its containing graph in the graph structure.
    
    Args:
        main_graph: The root graph structure
        target_node: The node to find
        current_graph: Current graph being searched (for recursive calls)
    
    Returns:
        Tuple[Optional[Dict], List[str]]: (containing_graph, path_to_node)
    """
    if current_graph is None:
        current_graph = main_graph
        
    # Check if node is in current level
    if target_node in current_graph:
        return current_graph, []
        
    # Search in subgraphs
    for node in current_graph:
        if 'subgraph' in current_graph[node]:
            subgraph = current_graph[node]['subgraph']
            result_graph, path = find_node_and_graph(main_graph, target_node, subgraph)
            if result_graph is not None:
                return result_graph, [node] + path
                
    return None, []

def add_edge(graph: Dict[str, Dict[str, Any]], from_node: str, to_node: str) -> bool:
    """
    Attempts to add an edge between two nodes in the same graph level.
    
    Args:
        graph: The graph containing both nodes
        from_node: Source node
        to_node: Target node
    
    Returns:
        bool: True if edge was added successfully
    """
    if from_node not in graph:
        print(f"Error: Source node {from_node} not found in graph")
        return False
    
    # Initialize 'next' list if it doesn't exist
    if 'next' not in graph[from_node]:
        graph[from_node]['next'] = []
    
    # Check if edge already exists
    if to_node in graph[from_node]['next']:
        print(f"Edge {from_node} -> {to_node} already exists")
        return True
    
    # Temporarily add the edge
    graph[from_node]['next'].append(to_node)
    
    # Check for cycles
    has_cycle_result, cycle_path = has_cycle(graph, from_node)
    
    if has_cycle_result:
        # Remove the edge if it would create a cycle
        graph[from_node]['next'].remove(to_node)
        print(f"Cannot add edge {from_node} -> {to_node} as it would create a cycle")
        print(f"Cycle detected: {' -> '.join(cycle_path)}")
        return False
    
    print(f"Successfully added edge {from_node} -> {to_node}")
    return True

def add_edge_anywhere(main_graph: Dict[str, Dict[str, Any]], from_node: str, to_node: str) -> bool:
    """
    Attempts to add an edge between two nodes in the graph structure.
    
    Rules for edge addition:
    1. Both nodes must exist in the graph structure
    2. Nodes can only reference other nodes within the same graph level:
       - Main graph nodes can only reference other main graph nodes
       - Subgraph nodes can only reference nodes within the same subgraph
    3. No cycles are allowed within any graph level
    
    Args:
        main_graph: The root graph structure
        from_node: Source node
        to_node: Target node
    
    Returns:
        bool: True if edge was added successfully
    """
    # Find the containing graphs for both nodes
    from_graph, from_path = find_node_and_graph(main_graph, from_node)
    to_graph, to_path = find_node_and_graph(main_graph, to_node)
    
    if from_graph is None:
        print(f"Error: Source node {from_node} not found in graph")
        return False
        
    if to_graph is None:
        print(f"Error: Target node {to_node} not found in graph")
        return False
    
    # Check if nodes are in the same graph level
    if from_graph is not to_graph:
        print(f"Error: Cannot create edge between different graph levels")
        print(f"Source node {from_node} is in {' -> '.join(['main'] + from_path) if from_path else 'main graph'}")
        print(f"Target node {to_node} is in {' -> '.join(['main'] + to_path) if to_path else 'main graph'}")
        return False
    
    return add_edge(from_graph, from_node, to_node)

def print_graph(graph: Dict[str, Dict[str, Any]], spaces: int = 0) -> None:
    """
    Traverse and print the graph structure.
    
    Args:
        graph: The graph structure to traverse
        spaces: Number of spaces for indentation
    """
    for key in graph.keys():
        print("." * spaces + key)
        print_visit_node(graph, key, spaces)

def print_visit_node(graph: Dict[str, Dict[str, Any]], key: str, spaces: int = 0) -> None:
    """
    Visit and print a node's details.
    
    Args:
        graph: The graph containing the node
        key: The node key to visit
        spaces: Number of spaces for indentation
    """
    node_key_obj = graph[key]
    sub_graph_obj = node_key_obj.get('subgraph')
    if sub_graph_obj:
        print("-" * (spaces + 2) + "subgraph:")
        print_graph(sub_graph_obj, spaces+2)
        print("-" * (spaces + 2) + "end subgraph.")
    next_obj = node_key_obj.get('next')
    if next_obj:
        print_traverse_list(next_obj, graph, spaces+2)

def print_traverse_list(nodes: List[str], graph: Dict[str, Dict[str, Any]], spaces: int = 0) -> None:
    """
    Traverse and print a list of nodes.
    
    Args:
        nodes: List of node names
        graph: The graph containing the nodes
        spaces: Number of spaces for indentation
    """
    for node in nodes:
        print("." * spaces + " has dependent " + node)

def validate_graph_references(graph: Dict[str, Dict[str, Any]], path: Optional[List[str]] = None) -> Tuple[bool, List[str]]:
    """
    Validates that all node references in a graph structure are within their own graph level.
    This includes the main graph and all subgraphs.
    
    Args:
        graph: The graph structure to validate
        path: Current path in the graph (for error reporting)
        
    Returns:
        tuple: (is_valid, list_of_violations)
        where violations are strings describing each cross-graph reference found
    """
    if path is None:
        path = []
        
    violations = []
    graph_nodes = set(graph.keys())
    
    # Check each node's references
    for node, node_data in graph.items():
        current_path = path + [node] if path else [node]
        
        # Check 'next' references
        next_nodes = node_data.get('next', [])
        for next_node in next_nodes:
            if next_node not in graph_nodes:
                violations.append(
                    f"Node '{' -> '.join(current_path)}' references '{next_node}' "
                    f"which is not in the same graph level"
                )
        
        # Recursively check subgraphs
        if 'subgraph' in node_data:
            subgraph_valid, subgraph_violations = validate_graph_references(
                node_data['subgraph'], 
                current_path
            )
            violations.extend(subgraph_violations)
    
    return len(violations) == 0, violations

def find_head_nodes(graph: Dict[str, Dict[str, Any]]) -> Set[str]:
    """
    Find all nodes that have no incoming edges (head nodes) in a graph.
    
    Args:
        graph: The graph structure to check
        
    Returns:
        Set[str]: Set of nodes that have no incoming edges
    """
    # First collect all nodes that are destinations
    has_incoming_edges = set()
    for node in graph:
        # Check next nodes
        for next_node in graph[node].get('next', []):
            has_incoming_edges.add(next_node)
        # Check subgraph if it exists
        if 'subgraph' in graph[node]:
            subgraph = graph[node]['subgraph']
            # Recursively find head nodes in subgraph
            subgraph_heads = find_head_nodes(subgraph)
            # All nodes in subgraph are considered to have an incoming edge
            # from the parent node that contains the subgraph
            has_incoming_edges.update(subgraph.keys())
    
    # Head nodes are those that exist in the graph but have no incoming edges
    return set(graph.keys()) - has_incoming_edges

def find_tail_nodes(graph: Dict[str, Dict[str, Any]]) -> Set[str]:
    """
    Find all nodes that have no outgoing edges (tail nodes) in a graph.
    
    Args:
        graph: The graph structure to check
        
    Returns:
        Set[str]: Set of nodes that have no outgoing edges
    """
    tail_nodes = set()
    for node, node_data in graph.items():
        has_next = bool(node_data.get('next', []))
        has_subgraph = 'subgraph' in node_data
        
        if not has_next:
            if has_subgraph:
                # If node has no next but has subgraph, the tail nodes are in the subgraph
                subgraph_tails = find_tail_nodes(node_data['subgraph'])
                tail_nodes.update(subgraph_tails)
            else:
                # Node with no next and no subgraph is a tail
                tail_nodes.add(node)
    
    return tail_nodes

def validate_graph(graph: Dict[str, Dict[str, Any]], name: str = "") -> None:
    """
    Performs comprehensive validation of a graph structure.
    Checks for:
    1. Graph cycles
    2. Cross-graph reference violations
    3. Head node requirements (exactly one head node)
    4. Tail node requirements (exactly one tail node)
    
    Args:
        graph: The graph structure to validate
        name: Optional name for the graph (for logging)
        
    Raises:
        ValueError: If any validation fails, with detailed error message
    """
    errors = []
    
    # Check for cycles
    has_cycles = check_graph_for_cycles(graph, name)
    if has_cycles:
        msg = f"Graph {name} contains cycles"
        logging.error(msg)
        errors.append(msg)
    
    # Check for cross-graph reference violations
    is_valid_refs, violations = validate_graph_references(graph)
    if not is_valid_refs:
        msg = f"Graph {name} contains invalid cross-graph references:\n" + "\n".join(violations)
        logging.error(msg)
        errors.append(msg)
    
    # Check for head node requirements
    head_nodes = find_head_nodes(graph)
    if len(head_nodes) == 0:
        msg = f"Graph {name} has no head nodes (nodes with no incoming edges)"
        logging.error(msg)
        errors.append(msg)
    elif len(head_nodes) > 1:
        msg = f"Graph {name} has multiple head nodes: {head_nodes}. Exactly one head node is required."
        logging.warning(msg)
    
    # Check for tail node requirements
    tail_nodes = find_tail_nodes(graph)
    if len(tail_nodes) == 0:
        msg = f"Graph {name} has no tail nodes (nodes with no outgoing edges)"
        logging.error(msg)
        errors.append(msg)
    elif len(tail_nodes) > 1:
        msg = f"Graph {name} has multiple tail nodes: {tail_nodes}. Exactly one tail node is required."
        logging.warning(msg)
    
    # If any errors were found, raise exception with all error messages
    if errors:
        raise ValueError(f"Graph validation failed:\n" + "\n".join(errors))
    
    # Log success if no errors
    logging.info(f"Graph {name} passed all validations")



================================================
FILE: src/flow4ai/f4a_logging.py
================================================
"""
Logging configuration for Flow4AI.

Environment Variables:
    FLOW4AI_LOG_LEVEL: Set the logging level (e.g., 'DEBUG', 'INFO'). Defaults to 'INFO'.
    FLOW4AI_LOG_HANDLERS: Set logging handlers. Options:
        - Not set or 'console': Log to console only (default)
        - 'console,file': Log to both console and file
        
Example:
    To enable both console and file logging:
    $ export FLOW4AI_LOG_HANDLERS='console,file'
    
    To set debug level logging:
    $ export FLOW4AI_LOG_LEVEL='DEBUG'
"""


import os

# Initializing the flag here stops logging caching root levels to another value
# for reasons I'm not completely sure about.
WINDSURF_LOG_FLAG = None #None #"DEBUG"
os.environ['FLOW4AI_LOG_LEVEL'] = WINDSURF_LOG_FLAG or os.getenv('FLOW4AI_LOG_LEVEL', 'INFO')
import logging
from logging.config import dictConfig


def get_logging_config():
    """Get the logging configuration based on current environment variables."""
    
    
    return {
        'version': 1,
        'disable_existing_loggers': False,
        'formatters': {
            'detailed': {
                'format': '%(asctime)s [%(levelname)s] %(name)s:%(lineno)d - %(message)s'
            }
        },
        'handlers': {
            'console': {
                'class': 'logging.StreamHandler',
                'level': os.getenv('FLOW4AI_LOG_LEVEL', 'INFO'),
                'formatter': 'detailed',
                'stream': 'ext://sys.stdout'
            },
            'file': {
                'class': 'logging.FileHandler',
                'level': os.getenv('FLOW4AI_LOG_LEVEL', 'INFO'),
                'formatter': 'detailed',
                'filename': 'flow4ai.log',
                'mode': 'a'
            }
        },
        'loggers': {
            'ExampleCustom': {
                'level': 'DEBUG',
                'handlers': ['console', 'file'],
                'propagate': False
            }
        },
        'root': {
            'level':  os.getenv('FLOW4AI_LOG_LEVEL', 'INFO'),
            # Set FLOW4AI_LOG_HANDLERS='console,file' to enable both console and file logging
            'handlers': os.getenv('FLOW4AI_LOG_HANDLERS', 'console').split(',')
        }
    }

def setup_logging():
    """Setup logging with current configuration."""
    config = get_logging_config()
    
    # Always create log file with header, actual logging will only happen if handlers use it
    if not os.path.exists('flow4ai.log'):
        with open('flow4ai.log', 'w') as f:
            f.write('# Flow4AI log file - This file is created empty and will be written to only when file logging is enabled\n')
    
    print(f"Logging level: {config['root']['level']}")
    # Apply configuration
    dictConfig(config)

# Apply configuration when module is imported
setup_logging()

# Re-export everything from logging
# Constants
CRITICAL = logging.CRITICAL
ERROR = logging.ERROR
WARNING = logging.WARNING
INFO = logging.INFO
DEBUG = logging.DEBUG
NOTSET = logging.NOTSET

# Functions
getLogger = logging.getLogger
basicConfig = logging.basicConfig
shutdown = logging.shutdown
debug = logging.debug
info = logging.info
warning = logging.warning
error = logging.error
critical = logging.critical
exception = logging.exception
log = logging.log

# Classes
Logger = logging.Logger
Handler = logging.Handler
Formatter = logging.Formatter
Filter = logging.Filter
LogRecord = logging.LogRecord

# Handlers
StreamHandler = logging.StreamHandler
FileHandler = logging.FileHandler
NullHandler = logging.NullHandler

# Configuration
dictConfig = dictConfig  # Already imported from logging.config
fileConfig = logging.config.fileConfig

# Exceptions
exception = logging.exception
captureWarnings = logging.captureWarnings

# Additional utilities
getLevelName = logging.getLevelName
makeLogRecord = logging.makeLogRecord



================================================
FILE: src/flow4ai/flowmanager.py
================================================
import asyncio
import threading
from collections import defaultdict, deque
from typing import Any, Callable, Dict, List, Optional, Union

from flow4ai.flowmanager_utils import find_unique_variant_suffix

from . import JobABC
from . import f4a_logging as logging
from .dsl import DSLComponent, JobsDict
from .dsl_graph import PrecedenceGraph, dsl_to_precedence_graph
from .f4a_graph import validate_graph
from .job import SPLIT_STR, Task, job_graph_context_manager
from .job_loader import JobFactory


class FlowManager:
    _instance = None
    _lock = threading.Lock()  # Class-level lock for singleton creation

    def __new__(cls, *args, **kwargs):  # Accept arbitrary arguments
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance.__initialized = False
        return cls._instance

    def __init__(self, dsl_dict=None, jobs_dir_mode=False, on_complete: Optional[Callable[[Any], None]] = None):
        self.jobs_dir_mode = jobs_dir_mode
        self.on_complete = on_complete
        if not hasattr(self, '_TaskManager__initialized') or not self.__initialized:
            with self._lock:
                if not hasattr(self, '_TaskManager__initialized') or not self.__initialized:
                    self._initialize()
                    self.__initialized = True
        
        # Add DSL dictionary if provided
        if dsl_dict:
            self.add_dsl_dict(dsl_dict)

    def _initialize(self):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.loop = asyncio.new_event_loop()
        self.thread = threading.Thread(target=self._run_loop, daemon=True)
        self.thread.start()
        self.job_map: Dict[str, JobABC] = {}
        self.head_jobs: List[JobABC] = []
        if self.jobs_dir_mode:
            self.head_jobs = JobFactory.get_head_jobs_from_config()
            self.job_map = {job.name: job for job in self.head_jobs}
        self.submitted_count = 0
        self.completed_count = 0
        self.error_count = 0
        self.completed_results = defaultdict(list)
        self.error_results = defaultdict(list)

        self._data_lock = threading.Lock()

    def _run_loop(self):
        asyncio.set_event_loop(self.loop)
        self.loop.run_forever()

    async def _execute_with_context(self, job: JobABC, task: Task):
        """Execute a job with the job graph context manager.
        
        This ensures that the local async context variables are reset for each
        coroutine that is executed.

        Args:
            job: The job to execute
            task: The task to process
            
        Returns:
            The result of the job execution
        """
        # Create a job set for this job
        job_set = JobABC.job_set(job)
        
        # Execute the job within the context manager
        async with job_graph_context_manager(job_set):
            return await job._execute(task)
            
    def submit(self, task: Union[Task, List[Task]], fq_name: str = None):
        """
        Submit a task or list of tasks to the FlowManager.
        
        Args:
            task: A Task or list of Tasks to submit
            fq_name: Fully qualified name of the job graph to submit to, optional, if there is
            only one job graph in job_map, it will be used automatically without an fq_name
            lookup.
            
        Returns:
            None
        """
        # Check that job_map is not None or empty
        if not self.job_map:
            self.logger.error("job_map is None or empty")
            return
        
        # If fq_name is None and there's only one job graph in job_map, use that one
        if fq_name is None:
            if len(self.job_map) == 1:
                fq_name = next(iter(self.job_map))
                self.logger.debug(f"Using the only available job graph: {fq_name}")
            else:
                error_msg = "fq_name must be specified when multiple job graphs are available"
                self.logger.error(error_msg)
                # Handle error for single task or list of tasks
                job_key = "unknown"
                if isinstance(task, list):
                    with self._data_lock:
                        self.error_count += len(task)
                        for single_task in task:
                            self.error_results[job_key].append({
                                "error": ValueError(error_msg),
                                "task": single_task
                            })
                else:
                    with self._data_lock:
                        self.error_count += 1
                        self.error_results[job_key].append({
                            "error": ValueError(error_msg),
                            "task": task
                        })
                return
    
        # Get the JobABC instance from the job_map
        job_key = fq_name
        job = None
        
        # Find the job by matching the start of the name
        if fq_name in self.job_map:
            job = self.job_map[fq_name]
                
        if job is None:
            self.logger.error(f"No job found for graph_name: {fq_name}")
            # Handle error for single task or list of tasks
            if isinstance(task, list):
                with self._data_lock:
                    self.error_count += len(task)
                    for single_task in task:
                        self.error_results[job_key].append({
                            "error": ValueError(f"No job found for graph_name: {fq_name}"),
                            "task": single_task
                        })
            else:
                with self._data_lock:
                    self.error_count += 1
                    self.error_results[job_key].append({
                        "error": ValueError(f"No job found for graph_name: {fq_name}"),
                        "task": task
                    })
            return

        # Handle single task or list of tasks
        if isinstance(task, list):
            for single_task in task:
                self._submit_single_task(job, single_task, job_key)
        else:
            self._submit_single_task(job, task, job_key)

    def _submit_single_task(self, job, task: Task, job_key: str):
        """Helper method to submit a single task to the job.
        
        Args:
            job: The job to execute
            task: The task to submit
            job_key: The job key for tracking
        """
        with self._data_lock:
            self.submitted_count += 1

        coro = self._execute_with_context(job, task)
        future = asyncio.run_coroutine_threadsafe(coro, self.loop)
        future.add_done_callback(
            lambda f: self._handle_completion(f, job, task)
        )

    def _handle_completion(self, future, job: JobABC, task: Task):
        result = None
        exception = None
        try:
            result = future.result()
            with self._data_lock:
                self.completed_count += 1
                self.completed_results[job.name].append(result)
                
            if self.on_complete:
                #try: don't catch the exception let it bubble up
                self.on_complete(result)
       
        except Exception as e:
            exception = e
            self.logger.error(f"Error processing result: {e}")
            self.logger.info("Detailed stack trace:", exc_info=True)
            with self._data_lock:
                self.error_count += 1
                self.error_results[job.name].append({
                    "error": e,
                    "task": task
                })


    def add_dsl(self, dsl: DSLComponent, graph_name: str, variant: str = "") -> str:
        """
        Adds a DSL component to the FlowManager. Each DSL component should only be added once
        as it is modified during the process by dsl_to_precedence_graph.
        
        Args:
            dsl: The DSL component defining the data flow between jobs.
                This DSL will be modified by being converted to a precedence graph.
            graph_name: The name of the graph. Used to generate fully qualified job names.
            variant: The variant of the graph e.g. "dev", "prod"
        
        Returns:
            str: The fully qualified name of the head job in the graph, to be used with submit().
        
        Warning:
            Do not add the same DSL object multiple times, as it will be modified each time.
            Instead, get the fully qualified name (FQ name) from the first call and reuse it.
        """
        if not graph_name:
            raise ValueError("graph_name cannot be None or empty")
        if dsl is None:
            raise ValueError("graph cannot be None")
        
        # Check if this DSL has a tracking attribute to prevent multiple additions
        if hasattr(dsl, "_f4a_already_added") and dsl._f4a_already_added:
            # Try to find the existing graph for this DSL
            for job in self.head_jobs:
                # Check if this job's associated DSL is the same object
                if hasattr(job, "_f4a_source_dsl") and job._f4a_source_dsl is dsl:
                    self.logger.info(f"DSL already added, returning existing FQ name: {job.name}")
                    return job.name
                
            self.logger.warning(
                f"DSL appears to have been added already but existing graph not found. "
                f"Creating a new graph, which may lead to duplicate processing."
            )
        
        # Transform the DSL into a precedence graph (this modifies the DSL)
        graph, jobs = dsl_to_precedence_graph(dsl)
        
        # Mark this DSL as added to prevent multiple additions
        setattr(dsl, "_f4a_already_added", True)
        
        # Check for FQ name collisions from different DSL objects with same structure
        # Create a base name prefix to check for collisions
        base_name_prefix = f"{graph_name}{SPLIT_STR}{variant}"
        
        # If there's already a job in job_map that would lead to a collision, 
        # we need to make this variant name unique by adding a suffix
        variant_suffix = find_unique_variant_suffix(self.job_map, base_name_prefix)
        
        # Add the suffix to the variant if needed
        if variant_suffix:
            self.logger.info(f"Detected potential FQ name collision, adding suffix '{variant_suffix}' to variant")
            enhanced_variant = f"{variant}{variant_suffix}"
        else:
            enhanced_variant = variant
        
        # Add the graph to our job map with potentially modified variant name
        fq_name = self.add_graph(graph, jobs, graph_name, enhanced_variant)
        
        # Store the reference to the source DSL in the head job
        head_job = self.job_map[fq_name]
        setattr(head_job, "_f4a_source_dsl", dsl)
        
        return fq_name
        
    def add_dsl_dict(self, dsl_dict: Dict) -> List[str]:
        """
        Adds multiple graphs to the task manager from a dictionary structure.
        
        Args:
            dsl_dict: A dictionary containing graph definitions, with optional variants.
                Format can be either:
                {
                    "graph1": {
                        "dev": dsl1d,
                        "prod": dsl1p
                    }
                    "graph2": {
                        "dev": dsl2d,
                        "prod": dsl2p
                    }
                }
                Or without variants:
                {
                    "graph1": dsl1,
                    "graph2": dsl2
                }
        
        Returns:
            List[str]: The fully qualified names of all added graphs.
        
        Raises:
            ValueError: If the dictionary structure is invalid or missing required components.
        """
        if not dsl_dict:
            raise ValueError("dsl_dict cannot be None or empty")
        
        fq_names = []
        
        for graph_name, graph_data in dsl_dict.items():
            # Check if graph_data is a DSL component directly (no variants)
            if not isinstance(graph_data, dict):
                # No variants, graph_data is the DSL directly
                dsl = graph_data
                
                fq_name = self.add_dsl(dsl, graph_name)
                fq_names.append(fq_name)
            else:
                # Check if this is a variant structure or old-style direct dsl structure
                if "dsl" in graph_data:
                    # Old format - no variants, direct dsl
                    dsl = graph_data.get("dsl")
                    
                    if dsl is None:
                        raise ValueError(f"Graph '{graph_name}' is missing required 'dsl'")
                        
                    fq_name = self.add_dsl(dsl, graph_name)
                    fq_names.append(fq_name)
                else:
                    # With variants - each key is a variant name, value is the DSL
                    for variant, variant_data in graph_data.items():
                        # Check if variant_data is a dict with 'dsl' key (old format)
                        if isinstance(variant_data, dict) and "dsl" in variant_data:
                            # Old format with nested 'dsl' key
                            dsl = variant_data.get("dsl")
                            
                            if dsl is None:
                                raise ValueError(f"Graph '{graph_name}' variant '{variant}' is missing required 'dsl'")
                        else:
                            # New format - variant_data is the DSL directly
                            dsl = variant_data
                            
                        fq_name = self.add_dsl(dsl, graph_name, variant)
                        fq_names.append(fq_name)
        
        return fq_names
        
    def get_fq_names_by_graph(self, graph_name, variant=""):
        """
        Get the fully qualified names for a specific graph and variant.
        
        This handles cases where multiple DSLs with the same structure 
        might have been added with the same graph_name and variant, 
        but received different FQ names due to collision handling.
        
        Args:
            graph_name: The name of the graph
            variant: The variant name, defaults to empty string
            
        Returns:
            The list of matching fully qualified names, or empty list if none found
        """
        matching_names = []
        base_prefix = f"{graph_name}{SPLIT_STR}{variant}"
        
        # Import re for regex pattern matching
        import re
        
        # Find exact match first
        for job_name in self.job_map.keys():
            if job_name.startswith(base_prefix + SPLIT_STR):
                matching_names.append(job_name)
                
        # Also look for variants with numeric suffixes (added by collision handling)
        pattern = re.compile(re.escape(graph_name + SPLIT_STR) + 
                           re.escape(variant) + r'_\d+' + 
                           re.escape(SPLIT_STR))
        
        for job_name in self.job_map.keys():
            if pattern.match(job_name):
                # Only add if not already added (could happen if exact match already found)
                if job_name not in matching_names:
                    matching_names.append(job_name)
                    
        return matching_names
    
    def submit_by_graph(self, task, graph_name, variant=""):
        """
        Submit task(s) to a specific graph and variant.
        
        Args:
            task: The task or list of tasks to submit
            graph_name: The name of the graph
            variant: The variant name, defaults to empty string
            
        Returns:
            None
            
        Raises:
            ValueError: If no matching graph is found or if multiple matches found
        """
        matching_fq_names = self.get_fq_names_by_graph(graph_name, variant)
        
        if not matching_fq_names:
            error_msg = f"No graph found with name '{graph_name}' and variant '{variant}'"
            self.logger.error(error_msg)
            raise ValueError(error_msg)
        
        if len(matching_fq_names) > 1:
            # If multiple matches, log them all and raise an error
            self.logger.error(f"Multiple matching graphs found for '{graph_name}' variant '{variant}': {matching_fq_names}")
            error_msg = (f"Multiple matching graphs found. Please use submit() with the specific FQ name "
                        f"from these options: {matching_fq_names}")
            raise ValueError(error_msg)
        
        # If exactly one match, use it
        fq_name = matching_fq_names[0]
        return self.submit(task, fq_name)

    def add_graph(self, precedence_graph: PrecedenceGraph, jobs: JobsDict, graph_name: str, variant: str = "") -> str:
        """
        Adds a graph to the task manager.
        
        Args:
            precedence_graph: A precedence graph that defines the data flow between jobs.
            jobs: A dictionary of jobs.
            graph_name: The name of the graph.
            variant: The variant of the graph e.g. "dev", "pr
        
        Returns:
            str: The fully qualified name of the graph.
        """
        if not graph_name:
            raise ValueError("graph_name cannot be None or empty")
        if not jobs:
            raise ValueError("jobs cannot be None or empty")
        if precedence_graph is None:
            raise ValueError("precedence_graph cannot be None")
        validate_graph(precedence_graph)
        for (short_job_name, job) in jobs.items():
            job.name = JobABC.create_FQName(graph_name, variant, short_job_name)
        head_job: JobABC = JobFactory.create_job_graph(precedence_graph, jobs)
        self.head_jobs.append(head_job)
        self.job_map.update({job.name: job for job in self.head_jobs})
        return head_job.name



    def get_counts(self):
        with self._data_lock:
            return {
                'submitted': self.submitted_count,
                'completed': self.completed_count,
                'errors': self.error_count
            }

    def pop_results(self):
        with self._data_lock:
            completed = dict(self.completed_results)
            errors = dict(self.error_results)
            self.completed_results.clear()
            self.error_results.clear()
            return {
                'completed': completed,
                'errors': errors
            }
            
    def wait_for_completion(self, timeout=10, check_interval=0.1):
        """
        Wait for all submitted tasks to complete or error out.
        
        Args:
            timeout: Maximum time to wait in seconds. Defaults to 10 seconds.
            check_interval: How often to check for completion in seconds. Defaults to 0.1 seconds.
            
        Returns:
            bool: True if all tasks completed or errored, False if timed out
        """
        import time
        start_time = time.time()
        
        while (time.time() - start_time) < timeout:
            counts = self.get_counts()
            if counts['submitted'] > 0 and counts['submitted'] == (counts['completed'] + counts['errors']):
                return True
            time.sleep(check_interval)
            
        # Check one last time before returning
        counts = self.get_counts()
        return counts['submitted'] == (counts['completed'] + counts['errors'])
        
    def execute(self, task, dsl=None, graph_name=None, fq_name=None, timeout=10):
        """
        Simplified execution method that handles the entire workflow.
        
        Args:
            task: The task to process
            dsl: Optional DSL component (if not using an existing graph)
            graph_name: Name for the graph if providing a DSL
            fq_name: Fully qualified name for an existing graph
            timeout: Maximum time to wait for completion
            
        Returns:
            A tuple of (errors, result) where errors is a list of error dictionaries and
            result is the result dictionary of the completed job(s)
        
        Raises:
            ValueError: If required parameters are missing
            TimeoutError: If tasks don't complete within the timeout period
            Exception: If any errors occurred during execution
        """
        if dsl and graph_name:
            fq_name = self.add_dsl(dsl, graph_name)
            
        if not fq_name:
            raise ValueError("Either provide both dsl and graph_name or an fq_name")
            
        self.submit(task, fq_name)
        success = self.wait_for_completion(timeout=timeout)
        
        if not success:
            raise TimeoutError(f"Timed out waiting for tasks to complete after {timeout} seconds")
            
        results = self.pop_results()
        
        # Check for errors and raise if present
        if results["errors"]:
            error_messages = []
            for job_name, job_errors in results["errors"].items():
                for error_data in job_errors:
                    error_msg = f"{job_name}: {error_data['error']}"
                    error_messages.append(error_msg)
            
            raise Exception("Errors occurred during job execution:\n" + "\n".join(error_messages))
            
        # Extract the result directly using the known fq_name
        result = None
        if results["completed"] and fq_name in results["completed"] and results["completed"][fq_name]:
            result = results["completed"][fq_name][0]
            
        return (results["errors"], result)
        
    def get_result(self, results=None, job_name_filter=None):
        """
        Extract a specific result from the completed results.
        
        Args:
            results: Results dictionary from pop_results() or None to use latest results
            job_name_filter: Optional string to filter job names (e.g., "add" to match jobs containing "add")
            
        Returns:
            The result data dictionary for the matched job, or None if not found
        """
        if results is None:
            results = self.pop_results()
            
        # Simple implementation - just find the job with the matching name
        for job_name, job_results in results["completed"].items():
            if job_name_filter is None or job_name_filter in job_name:
                if job_results:  # Make sure there's at least one result
                    return job_results[0]
        return None
        
    def get_result_value(self, results=None, job_name_filter=None):
        """
        Extract just the result value from a specific job result.
        
        Args:
            results: Results dictionary from pop_results() or None to use latest results
            job_name_filter: Optional string to filter job names
            
        Returns:
            The value of the "result" key for the matched job, or None if not found
        """
        result = self.get_result(results, job_name_filter)
        if result and "result" in result:
            return result["result"]
        return None
        
    def get_result_by_graph_name(self, graph_name, results=None):
        """
        Get the result dictionary for a specific graph by name.
        
        This simplifies result extraction by handling the lookup of the fully qualified name.
        Note: This method is primarily for backward compatibility - new code should use execute
        which directly returns the result.
        
        Args:
            graph_name: The graph name used when adding the DSL or executing the task
            results: Results dictionary from pop_results() or None to use latest results
            
        Returns:
            The result dictionary for the matched graph, or None if not found
        """
        if results is None:
            results = self.pop_results()
            
        # Find the fully qualified name based on the graph name
        fq_name = None
        for key in results["completed"].keys():
            if graph_name in key:
                fq_name = key
                break
                
        if fq_name is None:
            return None
            
        # Get the result dictionary
        if results["completed"][fq_name]:
            return results["completed"][fq_name][0]
            
        return None
        
    def get_fq_names(self):
        """
        Returns a list of all fully qualified names of graphs added to the FlowManager.
        
        Returns:
            List[str]: List of fully qualified names
        """
        return [job.name for job in self.head_jobs]

    @classmethod
    def run(cls, dsl, task, graph_name="default_graph", timeout=10):
        """
        Static helper method for one-line execution of a DSL graph with a task.
        
        Args:
            dsl: The DSL component defining the job graph
            task: Task dictionary to process
            graph_name: Name for the graph
            timeout: Maximum time to wait for completion
            
        Returns:
            A tuple of (errors, result) where errors is a list of error dictionaries and
            result is the result dictionary of the completed job(s)
        
        Raises:
            TimeoutError: If tasks don't complete within the timeout period
            Exception: If any errors occurred during execution
        """
        tm = cls()
        return tm.execute(task, dsl=dsl, graph_name=graph_name, timeout=timeout)

    def display_results(self, results=None):
        """
        Display results in a Jupyter/Colab-friendly format with rich formatting.
        
        Args:
            results: Results dictionary from pop_results() or None to use latest results
            
        Returns:
            The results dictionary for chaining
        """
        try:
            from IPython.display import HTML, Markdown, display
            jupyter_available = True
        except ImportError:
            jupyter_available = False
            
        if results is None:
            results = self.pop_results()
        
        if jupyter_available:
            # Display in rich Jupyter format
            if results["completed"]:
                display(Markdown("## Completed Tasks"))
                for job_name, job_results in results["completed"].items():
                    display(Markdown(f"### {job_name}"))
                    for result_data in job_results:
                        display(result_data["result"])
            
            if results["errors"]:
                display(Markdown("## Errors"))
                for job_name, job_errors in results["errors"].items():
                    display(Markdown(f"### {job_name}"))
                    for error_data in job_errors:
                        display(HTML(f"<div style='color:red'>{error_data['error']}</div>"))
        else:
            # Fallback to plain text output
            if results["completed"]:
                print("\nCompleted tasks:")
                for job_name, job_results in results["completed"].items():
                    for result_data in job_results:
                        print(f"- {job_name}: {result_data['result']}")
            
            if results["errors"]:
                print("\nErrors:")
                for job_name, job_errors in results["errors"].items():
                    for error_data in job_errors:
                        print(f"- {job_name}: {error_data['error']}")
        
        return results



================================================
FILE: src/flow4ai/flowmanager_utils.py
================================================
"""Utility functions for FlowManager to support unique FQ name generation."""

from typing import Dict, List

from flow4ai.dsl import DSLComponent


def find_unique_variant_suffix(job_map: Dict, base_name_prefix: str) -> str:
    """
    Find a unique numeric suffix to append to a variant name to avoid FQ name collisions.
    
    This function checks for existing keys in job_map that start with the given base_name_prefix
    and returns a suffix that will make the name unique when appended to the variant name.
    
    Args:
        job_map: The job map dictionary containing existing job FQ names as keys
        base_name_prefix: The prefix of the FQ name to check for collisions (graph_name$$variant)
        
    Returns:
        str: A numeric suffix (empty string if no collision found, or "_1", "_2", etc.)
    """
    # If no collision in job_map, no suffix needed
    collision_found = False
    for existing_key in job_map.keys():
        if existing_key.startswith(base_name_prefix):
            collision_found = True
            break
            
    if not collision_found:
        return ""
        
    # Find existing suffixes by looking at keys with the same base name prefix
    # Extract suffix numbers from variants like "graph_name$$_1$$job_name$$"
    existing_suffixes = set()
    import re
    # Match variants with numeric suffixes in the format "prefix_N$$"
    suffix_pattern = re.compile(re.escape(base_name_prefix) + r'_([0-9]+)\$\$')
    
    for existing_key in job_map.keys():
        match = suffix_pattern.match(existing_key)
        if match and match.group(1).isdigit():
            existing_suffixes.add(int(match.group(1)))
    
    # Find the next available suffix number
    suffix_num = 1
    while suffix_num in existing_suffixes:
        suffix_num += 1
    
    return f"_{suffix_num}"



================================================
FILE: src/flow4ai/flowmanagerMP.py
================================================
import asyncio
# In theory it makes sense to use dill with the "multiprocess" package
# instead of pickle with "multiprocessing", but in practice it leads to 
# performance and stability issues.
import multiprocessing as mp
import pickle
import queue
from collections import OrderedDict
from multiprocessing import freeze_support, set_start_method
from typing import Any, Callable, Collection, Dict, Optional, Union

from pydantic import BaseModel

from . import f4a_logging as logging
from .job import JobABC, Task, job_graph_context_manager
from .job_loader import ConfigLoader, JobFactory
from .utils.monitor_utils import should_log_task_stats


class FlowManagerMP:
    """
    FlowManagerMP executes up to thousands of tasks in parallel using one or more Jobs passed into constructor.
    FlowManagerMP is a multiprocessing implementation of FlowManager, so tasks and results are passed between entirely
    different processes, which means that results and tasks must be picklable.
    Optionally passes results to a pre-existing result processing function after task completion.

    Args:
        job (Union[Dict[str, Any], JobABC, Collection[JobABC]]): If missing, jobs will be loaded from config file.
            Otherwise either a dictionary containing job configuration,
            a single JobABC instance, or a collection of JobABC instances.

        result_processing_function (Optional[Callable[[Any], None]]): Code to handle results after the Job executes its task.
            By default, this hand-off happens in parallel, immediately after a Job processes a task.
            Typically, this function is from an existing codebase that FlowManagerMP is supplementing.
            This function must be picklable, for parallel execution, see serial_processing parameter below.
            This code is not assumed to be asyncio compatible.

        serial_processing (bool, optional): Forces result_processing_function to execute only after all tasks are completed by the Job.
            Enables an unpicklable result_processing_function to be used by setting serial_processing=True.
            However, in most cases changing result_processing_function to be picklable is straightforward and should be the default.
            Defaults to False.
    """
    # Constants
    JOB_MAP_LOAD_TIME = 5  # Timeout in seconds for job map loading

    def __init__(self, job: Optional[Any] = None, result_processing_function: Optional[Callable[[Any], None]] = None, 
                 serial_processing: bool = False):
        # Get logger for FlowManagerMP
        self.logger = logging.getLogger('FlowManagerMP')
        self.logger.info("Initializing FlowManagerMP")
        if not serial_processing and result_processing_function:
            self._check_picklable(result_processing_function)
        # tasks are created by submit_task(), with ["job_name"] added to the task dict
        # tasks are then sent to queue for processing
        self._task_queue: mp.Queue[Task] = mp.Queue()  
        # INTERNAL USE ONLY. DO NOT ACCESS DIRECTLY.
        # This queue is for internal communication between the job executor and result processor.
        # To process results, use the result_processing_function parameter in the FlowManagerMP constructor.
        # See test_result_processing.py for examples of proper result handling.
        self._result_queue = mp.Queue()  # type: mp.Queue
        self.job_executor_process = None
        self.result_processor_process = None
        self._result_processing_function = result_processing_function
        self._serial_processing = serial_processing
        # This holds a map of job name to job, 
        # when _execute is called on the job, the task must have a job_name
        # associated with it, if there is more than one job in the job_map
        self.job_map: OrderedDict[str, JobABC] = OrderedDict()
        
        # Create a manager for sharing objects between processes
        self._manager = mp.Manager()
        # Create a shared dictionary for job name mapping
        self._job_name_map = self._manager.dict()
        # Create an event to signal when jobs are loaded
        self._jobs_loaded = mp.Event()

        if job:
            self.create_job_map(job)
        
        self._start()

    def create_job_map(self, job):
        if isinstance(job, Dict):
            pass # SimpleJobFactory is deprecated
            # job_context: Dict[str, Any] = job.get("job_context") or {}
            # loaded_job = SimpleJobFactory.load_job(job_context)
            # if isinstance(loaded_job, JobABC):
            #     self.job_map[loaded_job.name] = loaded_job
        elif isinstance(job, JobABC):
            self.job_map[job.name] = job
        elif isinstance(job, Collection) and not isinstance(job, (str, bytes, bytearray)):
            if not job:  # Check if collection is empty
                raise ValueError("Job collection cannot be empty")
            if not all(isinstance(j, JobABC) for j in job):
                raise TypeError("All items in job collection must be JobABC instances")
            for j in job:
                if isinstance(j, JobABC):
                    if j.name in self.job_map:
                        raise ValueError(f"Duplicate job name found: {j.name}")
                    self.job_map[j.name] = j
                else:
                    raise TypeError("Items in job collection must be JobABC instances")
        else:
            raise TypeError("job must be either Dict[str, Any], JobABC instance, or Collection[JobABC]")

        self._job_name_map.clear()
        self._job_name_map.update({job.name: job.job_set_str() for job in self.job_map.values()})

    # We will not to use context manager as it makes semantics of FlowManagerMP use less flexible
    # def __enter__(self):
    #     """Initialize resources when entering the context."""
    #     self._start()
    #     return self

    # def __exit__(self, exc_type, exc_val, exc_tb):
    #     """Clean up resources when exiting the context."""
    #     self._cleanup()

    # belt and braces, _cleanup is called by _wait_for_completion() via mark_input_completed()
    def __del__(self):
        self._cleanup

    def _cleanup(self):
        """Clean up resources when the object is destroyed."""
        self.logger.info("Cleaning up FlowManagerMP resources")
        
        if self.job_executor_process:
            if self.job_executor_process.is_alive():
                self.logger.debug("Terminating job executor process")
                self.job_executor_process.terminate()
                self.logger.debug("Joining job executor process")
                self.job_executor_process.join()
                self.logger.debug("Job executor process joined")
        
        if self.result_processor_process:
            if self.result_processor_process.is_alive():
                self.logger.debug("Terminating result processor process")
                self.result_processor_process.terminate()
                self.logger.debug("Joining result processor process")
                self.result_processor_process.join()
                self.logger.debug("Result processor process joined")
        
        if hasattr(self, '_task_queue'):
            self.logger.debug("Closing task queue")
            self._task_queue.close()
            self.logger.debug("Joining task queue thread")
            self._task_queue.join_thread()
            self.logger.debug("Task queue thread joined")
        
        if hasattr(self, '_result_queue'):
            self.logger.debug("Closing result queue")
            self._result_queue.close()
            self.logger.debug("Joining result queue thread")
            self._result_queue.join_thread()
            self.logger.debug("Result queue thread joined")
        
        self.logger.debug("Cleanup completed")

    def _check_picklable(self, result_processing_function):
        try:
            # Try to pickle just the function itself
            pickle.dumps(result_processing_function)
            
            # Try to pickle any closure variables
            if hasattr(result_processing_function, '__closure__') and result_processing_function.__closure__:
                for cell in result_processing_function.__closure__:
                    pickle.dumps(cell.cell_contents)
                    
        except Exception as e:
            self.logger.error(f"""Result processing function or its closure variables cannot be pickled: {e}.  
                              Use serial_processing=True for unpicklable functions.""")
            raise TypeError(f"Result processing function must be picklable in parallel mode: {e}")

    
    def _start(self):
        """Start the job executor and result processor processes - non-blocking."""
        self.logger.debug("Starting job executor process")
        self.job_executor_process = mp.Process(
            target=self._async_worker,
            args=(self.job_map, self._task_queue, self._result_queue, self._job_name_map, self._jobs_loaded, ConfigLoader.directories),
            name="JobExecutorProcess"
        )
        self.job_executor_process.start()
        self.logger.info(f"Job executor process started with PID {self.job_executor_process.pid}")

        if self._result_processing_function and not self._serial_processing:
            self.logger.debug("Starting result processor process")
            self.result_processor_process = mp.Process(
                target=self._result_processor,
                args=(self._result_processing_function, self._result_queue),
                name="ResultProcessorProcess"
            )
            self.result_processor_process.start()
            self.logger.info(f"Result processor process started with PID {self.result_processor_process.pid}")
    # TODO: add ability to submit a task or an iterable: Iterable
    # TODO: add resource usage monitoring which returns False if resource use is too high.
    def submit_task(self, task: Union[Dict[str, Any], str], job_name: Optional[str] = None):
        """
        Submit a task to be processed by the job.

        Args:
            task: Either a dictionary containing task data or a string that will be converted to a task.
                    if this is None then the task will be skipped.
            job_name: The name of the job to execute this task. Required if there is more than one job in the job_map,
                     unless the task is a dictionary that includes a 'job_name' key.

        Raises:
            ValueError: If job_name is required but not provided, or if the specified job cannot be found.
            TypeError: If the task is not a dictionary or string.
        """
        try:
            # Wait for jobs to be loaded
            if not self._jobs_loaded.wait(timeout=self.JOB_MAP_LOAD_TIME):
                raise TimeoutError("Timed out waiting for jobs to be loaded")

            if task is None:
                self.logger.warning("Received None task, skipping")
                return
            
            if isinstance(task, str):
                task_dict = {'task': task}
            elif isinstance(task, dict):
                task_dict = task.copy()
            else:
                self.logger.warning(f"Received invalid task type {type(task)}, converting to string")
                task_dict = {'task': str(task)}

            # If job_name parameter is provided, it takes precedence
            if job_name is not None:
                if job_name not in self._job_name_map:
                    raise ValueError(
                        f"Job '{job_name}' not found. Available jobs: {list(self._job_name_map.keys())}"
                    )
                task_dict['job_name'] = job_name
            
            # If there's more than one job, we need a valid job name
            if len(self._job_name_map) > 1:
                if 'job_name' not in task_dict or not isinstance(task_dict['job_name'], str) or not task_dict['job_name']:
                    raise ValueError(
                        "When multiple jobs are present, you must either:\n"
                        "1) Provide the job_name parameter in submit_task() OR\n"
                        "2) Include a non-empty string 'job_name' in the task dictionary"
                    )
                if task_dict['job_name'] not in self._job_name_map:
                    raise ValueError(
                        f"Job '{task_dict['job_name']}' not found. Available jobs: {list(self._job_name_map.keys())}"
                    )
            elif len(self._job_name_map) == 1:
                # If there's only one job, use its name
                task_dict['job_name'] = next(iter(self._job_name_map.keys()))
            else:
                raise ValueError("No jobs available in FlowManagerMP")

            task_obj = Task(task_dict)
            self._task_queue.put(task_obj)
        except Exception as e:
            self.logger.error(f"Error submitting task: {e}")
            self.logger.info("Detailed stack trace:", exc_info=True)

    def mark_input_completed(self):
        """Signal completion of input and wait for all processes to finish and shut down."""
        self.logger.debug("Marking input as completed")
        self.logger.info("*** task_queue ended ***")
        self._task_queue.put(None)
        self._wait_for_completion()

    # Must be static because it's passed as a target to multiprocessing.Process
    # Instance methods can't be pickled properly for multiprocessing
    # TODO: it may be necessary to put a flag to execute this using asyncio event loops
    #          for example, when handing off to an async web service
    @staticmethod
    def _result_processor(process_fn: Callable[[Any], None], result_queue: 'mp.Queue'):
        """Process that handles processing results as they arrive."""
        logger = logging.getLogger('ResultProcessor')
        logger.debug("Starting result processor")

        while True:
            try:
                result = result_queue.get()
                if result is None:
                    logger.debug("Received completion signal from result queue")
                    break
                logger.debug(f"ResultProcessor received result: {result}")
                try:
                    # Handle both dictionary and non-dictionary results
                    task_id = result.get('task', str(result)) if isinstance(result, dict) else str(result)
                    logger.debug(f"Processing result for task {task_id}")
                    process_fn(result)
                    logger.debug(f"Finished processing result for task {task_id}")
                except Exception as e:
                    logger.error(f"Error processing result: {e}")
                    logger.info("Detailed stack trace:", exc_info=True)
            except queue.Empty:
                continue

        logger.debug("Result processor shutting down")

    def _wait_for_completion(self):
        """Wait for completion of all processing."""
        self.logger.debug("Entering wait for completion")

        if self._result_processing_function and self._serial_processing:
            self._process_serial_results()
        
        # Wait for job executor to finish
        if self.job_executor_process and self.job_executor_process.is_alive():
            self.logger.debug("Waiting for job executor process")
            self.job_executor_process.join()
            self.logger.debug("Job executor process completed")

        # Wait for result processor to finish
        if self.result_processor_process and self.result_processor_process.is_alive():
            self.logger.debug("Waiting for result processor process")
            self.result_processor_process.join()
            self.logger.debug("Result processor process completed")
        
        self._cleanup()

    def _process_serial_results(self):
        while True:
            try:
                self.logger.debug("Attempting to get result from queue")
                result = self._result_queue.get(timeout=0.1)
                if result is None:
                    self.logger.debug("Received completion signal (None) from result queue")
                    self.logger.info("No more results to process.")
                    break
                if self._result_processing_function:
                    try:
                        # Handle both dictionary and non-dictionary results
                        task_id = result.get('task', str(result)) if isinstance(result, dict) else str(result)
                        self.logger.debug(f"Processing result for task {task_id}")
                        self._result_processing_function(result)
                        self.logger.debug(f"Finished processing result for task {task_id}")
                    except Exception as e:
                        self.logger.error(f"Error processing result: {e}")
                        self.logger.info("Detailed stack trace:", exc_info=True)
            except queue.Empty:
                job_executor_is_alive = self.job_executor_process and self.job_executor_process.is_alive()
                self.logger.debug(f"Queue empty, job executor process alive status = {job_executor_is_alive}")
                if not job_executor_is_alive:
                    self.logger.debug("Job executor process is not alive, breaking wait loop")
                    break
                continue

    @staticmethod
    def _replace_pydantic_models(data: Any) -> Any:
        """Recursively replace pydantic.BaseModel instances with their JSON dumps."""
        logger = logging.getLogger('FlowManagerMP')
        logger.debug(f'Processing data type: {type(data)}')

        if isinstance(data, dict):
            return {k: FlowManagerMP._replace_pydantic_models(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [FlowManagerMP._replace_pydantic_models(item) for item in data]
        elif isinstance(data, BaseModel):
            logger.info(f'Converting pydantic model {data.__class__.__name__}')
            return data.model_dump_json()
        return data

    # Must be static because it's passed as a target to multiprocessing.Process
    # Instance methods can't be pickled properly for multiprocessing
    @staticmethod
    def _async_worker(job_map: Dict[str, JobABC], task_queue: 'mp.Queue', result_queue: 'mp.Queue', 
                     job_name_map: 'mp.managers.DictProxy', jobs_loaded: 'mp.Event', 
                     directories: list[str] = []):
        """Process that handles making workflow calls using asyncio."""
        # Get logger for AsyncWorker
        logger = logging.getLogger('AsyncWorker')
        logger.debug("Starting async worker")

        # If job_map is empty, create it from SimpleJobLoader
        if not job_map:
            # logger.info("Creating job map from SimpleJobLoader")
            # job = SimpleJobFactory.load_job({"type": "file", "params": {}})
            # job_map = {job.name: job}
            logger.info("Creating job map from JobLoader")
            logger.info(f"Using directories from process: {directories}")
            ConfigLoader._set_directories(directories)
            ConfigLoader.reload_configs()
            head_jobs = JobFactory.get_head_jobs_from_config()
            job_map = {job.name: job for job in head_jobs}
            # Update the shared job_name_map with each head job's complete set of reachable jobs
            job_name_map.clear()
            job_name_map.update({job.name: job.job_set_str() for job in head_jobs})
            logger.info(f"Created job map with head jobs: {list(job_name_map.keys())}")

        # Signal that jobs are loaded
        jobs_loaded.set()

        async def process_task(task: Task):
            """Process a single task and return its result"""
            task_id = task.task_id  # task_id is not held in the dictionary itself i.e. NOT task['task_id']
            logger.debug(f"[TASK_TRACK] Starting task {task_id}")
            try:
                # If there's only one job, use it directly
                if len(job_map) == 1:
                    job = next(iter(job_map.values()))
                else:
                    # Otherwise, get the job from the map using job_name
                    job_name = task.get('job_name')
                    if not job_name:
                        raise ValueError("Task missing job_name when multiple jobs are present")
                    job = job_map[job_name]
                job_set = JobABC.job_set(job) #TODO: create a map of job to jobset in _async_worker
                async with job_graph_context_manager(job_set):
                    result = await job._execute(task)
                    processed_result = FlowManagerMP._replace_pydantic_models(result)
                    logger.info(f"[TASK_TRACK] Completed task {task_id}, returned by job {processed_result[JobABC.RETURN_JOB]}")

                    result_queue.put(processed_result)
                    logger.debug(f"[TASK_TRACK] Result queued for task {task_id}")
            except Exception as e:
                logger.error(f"[TASK_TRACK] Failed task {task_id}: {e}")
                logger.info("Detailed stack trace:", exc_info=True)
                raise

        async def queue_monitor():
            """Monitor the task queue and create tasks as they arrive"""
            logger.debug("Starting queue monitor")
            tasks = set()
            pending_tasks = []
            tasks_created = 0
            tasks_completed = 0
            end_signal_received = False

            while not end_signal_received or tasks:
                # Get all available tasks from the queue
                while True:
                    try:
                        task = task_queue.get_nowait()
                        if task is None:
                            logger.info("Received end signal in task queue")
                            end_signal_received = True
                            break
                        pending_tasks.append(task)
                    except queue.Empty:
                        break

                # Create tasks in batch if we have any pending
                if pending_tasks:
                    logger.debug(f"Creating {len(pending_tasks)} new tasks")
                    new_tasks = {asyncio.create_task(process_task(pending_tasks[i])) for i in range(len(pending_tasks))}
                    tasks.update(new_tasks)
                    tasks_created += len(new_tasks)
                    logger.debug(f"Total tasks created: {tasks_created}")
                    pending_tasks.clear()

                # Clean up completed tasks
                done_tasks = {t for t in tasks if t.done()}
                if done_tasks:
                    for done_task in done_tasks:
                        try:
                            # Check if task raised an exception
                            exc = done_task.exception()
                            if exc:
                                logger.error(f"Task failed with exception: {exc}")
                                logger.info("Detailed stack trace:", exc_info=True)
                        except asyncio.InvalidStateError:
                            pass  # Task was cancelled or not done
                    tasks_completed += len(done_tasks)
                    logger.debug(f"Cleaned up {len(done_tasks)} completed tasks. Total completed: {tasks_completed}")
                    logger.debug(f"Active tasks remaining: {len(tasks)}")
                tasks.difference_update(done_tasks)

                # Log task stats periodically
                if tasks_completed != 0 and tasks_completed % 5 == 0:
                    if should_log_task_stats(queue_monitor, tasks_created, tasks_completed):
                        logger.info(f"Tasks stats - Created: {tasks_created}, Completed: {tasks_completed}, Active: {len(tasks)}")

                # A short pause to reduce CPU usage and avoid a busy-wait state.             
                await asyncio.sleep(0.0001)

            # Wait for remaining tasks to complete
            if tasks:
                logger.debug(f"Waiting for {len(tasks)} remaining tasks")
                await asyncio.gather(*tasks)
                logger.debug("All remaining tasks completed")

            # Signal completion
            logger.debug("Sending completion signal to result queue")
            logger.debug(f"Final stats - Created: {tasks_created}, Completed: {tasks_completed}")
            logger.info("*** result_queue ended ***")
            result_queue.put(None)

        # Run the event loop
        logger.debug("Creating event loop")
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            logger.debug("Running queue monitor")
            loop.run_until_complete(queue_monitor())
        except Exception as e:
            import traceback
            logger.error(f"Error in async worker: {e}\n{traceback.format_exc()}")
            logger.info("Detailed stack trace:", exc_info=True)
        finally:
            logger.info("Closing event loop")
            loop.close()

    def get_job_names(self) -> list[str]:
        """
        Returns a list of job names after ensuring the job_name_map is loaded.

        Returns:
            list[str]: List of job names from the job_name_map

        Raises:
            TimeoutError: If waiting for jobs to be loaded exceeds timeout
        """
        self.logger.debug("Waiting for jobs to be loaded before returning job names")
        if not self._jobs_loaded.wait(timeout=self.JOB_MAP_LOAD_TIME):
            raise TimeoutError("Timed out waiting for jobs to be loaded")
        
        return list(self._job_name_map.keys())

    def get_job_graph_mapping(self) -> dict[str, set[str]]:
        """
        Returns a mapping of head job names to their complete set of job names in their graph.
        
        Returns:
            dict[str, set[str]]: Dictionary mapping each head job name to a set of all job names
                                reachable from that job (including itself).

        Raises:
            TimeoutError: If waiting for jobs to be loaded exceeds timeout
        """
        self.logger.debug("Waiting for jobs to be loaded before returning job graph mapping")
        if not self._jobs_loaded.wait(timeout=self.JOB_MAP_LOAD_TIME):
            raise TimeoutError("Timed out waiting for jobs to be loaded")
        
        return dict(self._job_name_map)

class FlowManagerMPFactory:
    _instance = None
    _flowmanagerMP = None

    def __init__(self, *args, **kwargs):
        if not FlowManagerMPFactory._instance:
            self._flowmanagerMP = FlowManagerMP(*args, **kwargs)
            FlowManagerMPFactory._instance = self

    @classmethod
    def init(cls, start_method="spawn", *args, **kwargs):
      """
      Initializes the FlowManagerMPFactory using the given start method.
      args and kwargs are passed down to the FlowManagerMP constructor.

      Args:
        start_method: The start method of multiprocessing. Defaults to "spawn".
        args: The parameters to be passed to the FlowManagerMP's constructor
        kwargs: The keyword parameters to be passed to the FlowManagerMP's constructor
        
      """
      freeze_support()
      set_start_method(start_method)
      if not cls._instance:
        cls._instance = cls(*args, **kwargs)
      return cls._instance

    @staticmethod
    def get_instance()->FlowManagerMP:
        if not FlowManagerMPFactory._instance:
            raise RuntimeError("FlowManagerMPFactory not initialized")
        return FlowManagerMPFactory._instance._flowmanagerMP



================================================
FILE: src/flow4ai/graph_pic.py
================================================
"""
Graph visualization module for Flow4AI.

This module provides functionality to visualize job graphs from adjacency list
representations using NetworkX and Matplotlib. It can generate visual representations
of job dependencies and workflows.
"""

import os
import tempfile
from collections import defaultdict
from typing import Any, Dict, List, Optional, Set, Tuple, Union

import matplotlib.pyplot as plt
import networkx as nx
import numpy as np

# Define some color schemes for the graphs
COLOR_SCHEMES = {
    "default": {
        "node_color": "#1f78b4",  # Blue
        "edge_color": "#333333",  # Dark Gray
        "font_color": "black",
        "border_color": "#000000",  # Black
    },
    "light": {
        "node_color": "#a6cee3",  # Light Blue
        "edge_color": "#666666",  # Gray
        "font_color": "black",
        "border_color": "#000000",  # Black
    },
    "dark": {
        "node_color": "#1f78b4",  # Blue
        "edge_color": "#cccccc",  # Light Gray
        "font_color": "white",
        "border_color": "#ffffff",  # White
        "background": "#333333",  # Dark Gray
    },
    "colorful": {
        "node_color": "#b2df8a",  # Light Green
        "edge_color": "#33a02c",  # Green
        "font_color": "black",
        "border_color": "#000000",  # Black
    }
}


def adjacency_to_nx_graph(graph_definition: Dict[str, Any]) -> nx.DiGraph:
    """
    Convert a Flow4AI adjacency list to a NetworkX directed graph.
    
    Args:
        graph_definition: Dictionary representing the graph as an adjacency list.
                         Format: {node_id: {"next": [list_of_next_node_ids]}}
    
    Returns:
        A NetworkX DiGraph object representing the graph.
    """
    G = nx.DiGraph()
    
    # Add all nodes first
    for node_id in graph_definition:
        G.add_node(node_id)
    
    # Add all edges
    for node_id, edges in graph_definition.items():
        next_nodes = edges.get("next", [])
        for next_node in next_nodes:
            G.add_edge(node_id, next_node)
    
    return G


def get_topological_generations(G: nx.DiGraph) -> List[List[str]]:
    """
    Get nodes arranged in topological generations (levels).
    Each generation contains nodes that have the same "distance" from the source nodes.
    
    Args:
        G: A NetworkX DiGraph object
        
    Returns:
        List of lists where each inner list contains nodes in the same generation
    """
    # Find root nodes (nodes with no incoming edges)
    root_nodes = [n for n, d in G.in_degree() if d == 0]
    
    if not root_nodes:
        # If no root found, just pick a random node
        root_nodes = [list(G.nodes())[0]]
    
    # Initialize generations
    generations = [root_nodes]
    visited = set(root_nodes)
    
    # Continue until all nodes are assigned to a generation
    while len(visited) < len(G.nodes()):
        # Get the last generation
        last_gen = generations[-1]
        
        # Find all successors of nodes in the last generation
        # that haven't been visited yet
        next_gen = []
        for node in last_gen:
            for succ in G.successors(node):
                if succ not in visited:
                    next_gen.append(succ)
                    visited.add(succ)
        
        # If we found new nodes, add them to the generations
        if next_gen:
            generations.append(next_gen)
        else:
            # Find any remaining unvisited nodes and add them to a new generation
            # This handles disconnected components
            remaining = [n for n in G.nodes() if n not in visited]
            if remaining:
                generations.append(remaining)
                visited.update(remaining)
            else:
                break
    
    return generations


def identify_paths(G: nx.DiGraph) -> Dict[str, List[List[str]]]:
    """
    Identify all paths from source nodes to sink nodes in the graph.
    This helps in maintaining straight-line paths in the visualization.
    
    Args:
        G: A NetworkX DiGraph object
        
    Returns:
        Dictionary mapping node IDs to the paths they belong to
    """
    # Find source nodes (nodes with no incoming edges)
    sources = [n for n, d in G.in_degree() if d == 0]
    # Find sink nodes (nodes with no outgoing edges)
    sinks = [n for n, d in G.out_degree() if d == 0]
    
    # Get all paths from each source to each sink
    all_paths = []
    for source in sources:
        for sink in sinks:
            try:
                # Find all simple paths between source and sink
                paths = list(nx.all_simple_paths(G, source, sink))
                all_paths.extend(paths)
            except nx.NetworkXNoPath:
                continue
    
    # If we couldn't find any paths, return an empty dictionary
    if not all_paths:
        return {}
    
    # Map each node to the paths it belongs to
    node_paths = {}
    for i, path in enumerate(all_paths):
        for node in path:
            if node not in node_paths:
                node_paths[node] = []
            node_paths[node].append((i, path))
    
    return node_paths

def custom_hierarchical_layout(G: nx.DiGraph) -> Dict[str, Tuple[float, float]]:
    """
    A custom hierarchical layout that arranges nodes in levels based on topology,
    with special attention to maintaining straight paths, minimizing edge crossings,
    and creating visually balanced layouts.
    
    Key features:
    - Head nodes are positioned equidistant between their child nodes
    - Tail nodes are positioned at the rightmost side
    - The algorithm minimizes edge crossings using node relationships and path analysis
    - Nodes that belong to the same path are vertically aligned when possible
    - Nodes are positioned based on the graph structure, not the arbitrary dict order
    
    Args:
        G: A NetworkX DiGraph object
        
    Returns:
        Dictionary mapping node names to (x, y) coordinates
    """
    # Get nodes arranged in topological generations
    generations = get_topological_generations(G)
    
    # Identify source nodes (no incoming edges) and sink nodes (no outgoing edges)
    source_nodes = [n for n, d in G.in_degree() if d == 0]
    sink_nodes = [n for n, d in G.out_degree() if d == 0]
    
    # Calculate positions
    pos = {}
    
    # First pass: assign x positions based on generation
    for i, gen in enumerate(generations):
        for node in gen:
            # Standard x position is determined by generation index (level)
            x = i
            
            # Special case: ensure sink nodes are positioned at the rightmost side
            if node in sink_nodes:
                x = len(generations) # Put at the far right
                
            pos[node] = (x, 0)  # Y will be assigned later
    
    # Group nodes by their x position/level
    x_groups = {}
    for node, (x, _) in pos.items():
        if x not in x_groups:
            x_groups[x] = []
        x_groups[x].append(node)
    
    # Calculate the longest path through each node (used for vertical ordering)
    path_lengths = {}
    for node in G.nodes():
        # Calculate distance from source
        if node in source_nodes:
            path_lengths[node] = 0
        else:
            # Find the maximum path length from any source to this node
            pred_lengths = [path_lengths.get(p, 0) + 1 for p in G.predecessors(node) if p in path_lengths]
            path_lengths[node] = max(pred_lengths) if pred_lengths else 0
    
    # Sort nodes within each level based on their network position and connectivity patterns
    for level in sorted(x_groups.keys()):
        nodes = x_groups[level]
        
        # For the first level (source nodes), use a special ordering to create visual balance
        if level == 0:
            # Sort by number of outgoing connections (more central nodes in the middle)
            source_nodes_sorted = sorted(source_nodes, key=lambda n: (len(list(G.successors(n))), str(n)))
            
            # Apply a more balanced ordering - place nodes with more connections in the middle
            # This creates a more aesthetically pleasing and balanced layout
            balanced_nodes = []
            left = 0
            right = len(source_nodes_sorted) - 1
            position = 0  # 0 = middle, 1 = left, 2 = right, and alternating
            
            while left <= right:
                if position == 0:  # Middle, take from right (higher connection count)
                    balanced_nodes.append(source_nodes_sorted[right])
                    right -= 1
                    position = 1
                elif position == 1:  # Left
                    balanced_nodes.append(source_nodes_sorted[left])
                    left += 1
                    position = 2
                else:  # Right
                    balanced_nodes.append(source_nodes_sorted[right])
                    right -= 1
                    position = 1
            
            # Update the nodes in this level with the balanced ordering
            x_groups[level] = balanced_nodes
            continue
            
        # For other levels, use a more sophisticated sorting approach to minimize crossings
        def node_sort_key(node):
            # Get predecessors and successors
            preds = list(G.predecessors(node))
            succs = list(G.successors(node))
            
            # Calculate average positions of predecessors (for vertical alignment)
            pred_positions = []
            for p in preds:
                if p in pos:
                    # Get position of predecessor in its group
                    p_level = pos[p][0]
                    if p_level in x_groups and p in x_groups[p_level]:
                        # Use normalized position (0 to 1 range)
                        p_idx = x_groups[p_level].index(p) 
                        p_norm_pos = p_idx / max(1, len(x_groups[p_level]) - 1)
                        pred_positions.append(p_norm_pos)
            
            # If no predecessor positions, use path length for ordering
            if not pred_positions:
                # Handle edge case where all nodes are both source and sink (all path lengths are 0)
                max_path_value = max(path_lengths.values()) if path_lengths else 0
                pred_position = 0 if max_path_value == 0 else path_lengths.get(node, 0) / max_path_value
            else:
                pred_position = sum(pred_positions) / len(pred_positions)
                
            # Return sorting criteria tuple
            return (pred_position, path_lengths.get(node, 0), len(succs), len(preds), str(node))
        
        # Sort the nodes in this level
        x_groups[level] = sorted(nodes, key=node_sort_key)
    
    # Second pass: assign y positions
    # Track vertical slots for each column to prevent overlap
    x_slots = {x: set() for x in x_groups}
    
    # Process source nodes specially to center them between their children
    for node in source_nodes:
        # Find direct children
        children = list(G.successors(node))
        if children:
            # If this source node has children, we position it equidistant from them
            # First, make sure the children have y-positions assigned
            child_level = pos[children[0]][0]  # All children should be at the same level
            
            # Get existing y-positions of the children, if any
            child_ys = [pos[child][1] for child in children if pos[child][1] != 0]
            
            if child_ys:
                # Center the source node between its children
                center_y = sum(child_ys) / len(child_ys)
            else:
                # If children don't have positions yet, use a centered position
                # This will be adjusted later when child positions are calculated
                center_y = 0
                
            # Assign the position
            x, _ = pos[node]
            pos[node] = (x, center_y)
    
    # Assign y positions to nodes based on sorted order within levels
    for level, nodes in x_groups.items():
        # Set y-coordinates based on the index within sorted nodes
        for i, node in enumerate(nodes):
            x, _ = pos[node]
            pos[node] = (x, i)
    
    # Refine positions to create better vertical alignment and distribution
    # Create a map to track used Y positions in each column
    x_slots = {x: set() for x in x_groups}
    
    # Second pass: refine y positions to create better alignment
    for level in sorted(x_groups.keys()):
        for node in x_groups[level]:
            # Skip nodes that already have been processed
            if node in source_nodes and level == 0:
                continue
            
            # Try to align with predecessors
            aligned_y = None
            preds = list(G.predecessors(node))
            
            if preds:
                # Try to use the average y position of predecessors
                pred_ys = [pos[p][1] for p in preds if p in pos]
                if pred_ys:
                    aligned_y = sum(pred_ys) / len(pred_ys)
            
            # If unable to align with predecessors, try with successors
            if aligned_y is None:
                succs = list(G.successors(node))
                succ_ys = [pos[s][1] for s in succs if s in pos and pos[s][1] != 0]
                if succ_ys:
                    aligned_y = sum(succ_ys) / len(succ_ys)
            
            # If we still don't have a position, keep the original one
            if aligned_y is None:
                aligned_y = pos[node][1]
            
            # Check for conflicts with existing positions and adjust
            while any(abs(aligned_y - existing) < 0.8 for existing in x_slots[level]):
                aligned_y += 0.8
            
            # Mark this position as used
            x_slots[level].add(aligned_y)
            
            # Update the position
            x, _ = pos[node]
            pos[node] = (x, aligned_y)
    
    # Special pass for source nodes: center them between their children
    for node in source_nodes:
        children = list(G.successors(node))
        if children:
            # Get positions of all children with valid y-coordinates
            child_ys = [pos[child][1] for child in children if child in pos]
            
            if child_ys:
                # Center the source node between its children
                center_y = sum(child_ys) / len(child_ys)
                x, _ = pos[node]
                pos[node] = (x, center_y)
    
    # Normalize the positions
    x_values = [p[0] for p in pos.values()]
    y_values = [p[1] for p in pos.values()]
    
    # Get min/max for normalization
    x_min, x_max = min(x_values), max(x_values)
    y_min, y_max = min(y_values), max(y_values)
    
    # Avoid division by zero
    x_range = x_max - x_min if x_max > x_min else 1
    y_range = y_max - y_min if y_max > y_min else 1
    
    # Normalize to [0,1] range and apply spacing
    normalized_pos = {}
    for node, (x, y) in pos.items():
        norm_x = (x - x_min) / x_range if x_range else 0.5
        norm_y = (y - y_min) / y_range if y_range else 0.5
        normalized_pos[node] = (norm_x * 1.5, norm_y * 1.5)  # Add scaling for clarity
    
    return normalized_pos


def visualize_graph(graph_definition: Dict[str, Any],
                   title: Optional[str] = None,
                   save_path: Optional[str] = None) -> Union[plt.Figure, str]:
    """
    Visualize a graph with automatically tuned parameters based on graph characteristics.
    This function analyzes the graph structure and selects optimal visualization parameters.
    
    Key features:
    - Automatically selects appropriate node size, edge width, and font size based on graph size
    - Uses the hierarchical layout by default for clear visualization of process flow
    - Adjusts figure size based on graph complexity and node count
    
    Args:
        graph_definition: Dictionary representing the graph as an adjacency list.
                         Format: {node_id: {"next": [list_of_next_node_ids]}}
        title: Optional title for the plot
        save_path: Optional path to save the figure
        
    Returns:
        If show is True, returns the figure. If save_path is provided, returns the save path.
    """
    # Create a NetworkX graph to analyze structure
    G = adjacency_to_nx_graph(graph_definition)
    
    # Analyze graph characteristics to determine optimal parameters
    node_count = len(G.nodes())
    edge_count = len(G.edges())
    max_connections = max([max(G.out_degree(n), G.in_degree(n)) for n in G.nodes()]) if G.nodes() else 0
    
    # Auto-tune parameters based on graph size and complexity
    if node_count <= 5:  # Small graph
        node_size = 1800
        edge_width = 2.0
        font_size = 14
        figsize = (10, 7)
    elif node_count <= 10:  # Medium graph
        node_size = 1500
        edge_width = 1.8
        font_size = 12
        figsize = (12, 8)
    elif node_count <= 20:  # Large graph
        node_size = 1200
        edge_width = 1.5
        font_size = 11
        figsize = (14, 9)
    else:  # Very large graph
        node_size = 800
        edge_width = 1.0
        font_size = 10
        figsize = (16, 10)
    
    # Adjust for graphs with many connections
    if max_connections > 5:
        node_size = max(600, node_size - 200)  # Reduce node size for highly connected graphs
        figsize = (figsize[0] + 2, figsize[1] + 1)  # Increase figure size
    
    # For single-level graphs with no connections, adjust layout
    if edge_count == 0:
        node_size = 1500  # Larger nodes for better visibility
        figsize = (10, 6)  # Smaller figure (less space needed)
    
    # Call the detailed function with optimized parameters
    return visualize_graph_detail(
        graph_definition=graph_definition,
        layout="hierarchical",  # Always use hierarchical layout for best results
        node_size=node_size,
        edge_width=edge_width,
        font_size=font_size,
        figsize=figsize,
        title=title,
        save_path=save_path,
        show=True  # Always show by default
    )


def visualize_graph_detail(graph_definition: Dict[str, Any], 
                        layout: str = "hierarchical", 
                        color_scheme: str = "default",
                        node_size: int = 1500,
                        title: Optional[str] = None,
                        figsize: Tuple[int, int] = (12, 8),
                        dpi: int = 100,
                        node_shape: str = 'o',
                        show_labels: bool = True,
                        font_size: int = 12,
                        edge_width: float = 1.5,
                        save_path: Optional[str] = None,
                        show: bool = True) -> Union[plt.Figure, str]:
    """
    Visualize a graph defined by an adjacency list using NetworkX and Matplotlib.
    
    Args:
        graph_definition: Dictionary representing the graph as an adjacency list.
                         Format: {node_id: {"next": [list_of_next_node_ids]}}
        layout: Layout algorithm to use:
                - 'hierarchical' (default): Custom hierarchical layout with left-to-right flow
                - 'dot', 'neato', 'fdp', 'sfdp', 'twopi', 'circo': Graphviz layouts
                - 'spring', 'circular', 'random', 'shell', 'spectral': NetworkX layouts
        color_scheme: Color scheme to use ('default', 'light', 'dark', 'colorful')
        node_size: Size of the nodes in the visualization
        title: Optional title for the plot
        figsize: Figure size as a tuple (width, height)
        dpi: DPI for the figure
        node_shape: Shape of the nodes ('o', 's', 'D', 'v', '^', '<', '>', 'p', 'h', '8')
        show_labels: Whether to show labels on nodes
        font_size: Font size for node labels
        edge_width: Width of the edges
        save_path: Optional path to save the figure
        show: Whether to display the figure
        
    Returns:
        If show is True, returns the figure. If save_path is provided, returns the save path.
    """
    # Create a NetworkX graph from the adjacency list
    G = adjacency_to_nx_graph(graph_definition)
    
    # If there are no nodes, return early
    if not G.nodes():
        print("Graph has no nodes to visualize.")
        return plt.figure(figsize=figsize, dpi=dpi)
    
    # Get the color scheme
    colors = COLOR_SCHEMES.get(color_scheme, COLOR_SCHEMES["default"])
    
    # Create figure
    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)
    
    # Set background color if specified
    if "background" in colors:
        ax.set_facecolor(colors["background"])
        fig.set_facecolor(colors["background"])
    
    # Get the layout positions
    if layout == 'hierarchical':
        # Use our custom hierarchical layout
        pos = custom_hierarchical_layout(G)
    elif layout in ['dot', 'neato', 'fdp', 'sfdp', 'twopi', 'circo']:
        # Use graphviz layout if available
        try:
            pos = nx.nx_agraph.graphviz_layout(G, prog=layout)
        except (ImportError, Exception) as e:
            print(f"Could not use {layout} layout, using custom hierarchical layout instead: {e}")
            pos = custom_hierarchical_layout(G)
    else:
        # Use NetworkX layout
        if layout == 'spring':
            pos = nx.spring_layout(G, seed=42)
        elif layout == 'circular':
            pos = nx.circular_layout(G)
        elif layout == 'random':
            pos = nx.random_layout(G, seed=42)
        elif layout == 'shell':
            pos = nx.shell_layout(G)
        elif layout == 'spectral':
            pos = nx.spectral_layout(G)
        else:
            # Default to our custom hierarchical layout
            pos = custom_hierarchical_layout(G)
    
    # Draw nodes
    nx.draw_networkx_nodes(
        G, pos,
        node_color=colors["node_color"],
        node_size=node_size,
        node_shape=node_shape,
        edgecolors=colors["border_color"],
        linewidths=2,
        ax=ax,
    )
    
    # Draw edges with more prominent arrows
    nx.draw_networkx_edges(
        G, pos,
        edge_color=colors["edge_color"],
        width=edge_width,
        arrowsize=30,  # Larger arrows for better visibility
        arrowstyle='-|>', 
        connectionstyle='arc3,rad=0.0',  # Straight connection lines
        min_source_margin=10,  # Margin from source node
        min_target_margin=15,  # Margin from target node to better show arrows
        ax=ax,
    )
    
    # Draw labels if requested
    if show_labels:
        nx.draw_networkx_labels(
            G, pos,
            font_size=font_size,
            font_color=colors["font_color"],
            ax=ax,
        )
    
    # Set the title if provided
    if title:
        plt.title(title, color=colors.get("font_color", "black"), fontsize=font_size + 4)
    
    # Remove axes
    plt.axis('off')
    
    # Tight layout for better spacing
    plt.tight_layout()
    
    # Save the figure if requested
    if save_path:
        plt.savefig(save_path, dpi=dpi, bbox_inches='tight', facecolor=fig.get_facecolor())
        print(f"Graph saved to {save_path}")
        
    # Show the figure if requested
    if show:
        plt.show()
    
    # Return the figure or save path
    if save_path:
        return save_path
    return fig


def save_graph_as_temp_image(graph_definition: Dict[str, Any], 
                            format: str = 'png', 
                            **kwargs) -> str:
    """
    Save a graph as a temporary image file and return the path.
    
    Args:
        graph_definition: Dictionary representing the graph as an adjacency list.
        format: Image format ('png', 'svg', 'pdf', 'jpg')
        **kwargs: Additional arguments to pass to visualize_graph
        
    Returns:
        Path to the temporary image file
    """
    # Create a temporary file
    fd, path = tempfile.mkstemp(suffix=f'.{format}')
    os.close(fd)  # Close the file descriptor
    
    # Set show to False to prevent displaying the graph
    kwargs['show'] = False
    kwargs['save_path'] = path
    
    # Visualize the graph and save it
    visualize_graph(graph_definition, **kwargs)
    
    return path


def visualize_to_display(graph_definition: Dict[str, Any],
                        width: int = 800,
                        height: int = 600,
                        **kwargs) -> None:
    """
    Visualize a graph and display it using IPython display (for Jupyter notebooks).
    
    Args:
        graph_definition: Dictionary representing the graph as an adjacency list.
        width: Width of the displayed image
        height: Height of the displayed image
        **kwargs: Additional arguments to pass to visualize_graph
    """
    try:
        from IPython.display import Image, display

        # Save the graph as a temporary image
        path = save_graph_as_temp_image(graph_definition, **kwargs)
        
        # Display the image
        display(Image(filename=path, width=width, height=height))
        
        # Clean up the temporary file
        try:
            os.unlink(path)
        except:
            pass
            
    except ImportError:
        print("IPython not available. Use visualize_graph() instead.")
        visualize_graph(graph_definition, **kwargs)


def compare_layouts(graph_definition: Dict[str, Any],
                   layouts: List[str] = None,
                   color_scheme: str = "default",
                   node_size: int = 1000,
                   figsize: Tuple[int, int] = (15, 10),
                   save_path: Optional[str] = None) -> plt.Figure:
    """
    Compare different layouts for the same graph.
    
    Args:
        graph_definition: Dictionary representing the graph as an adjacency list.
        layouts: List of layout algorithms to compare
        color_scheme: Color scheme to use
        node_size: Size of the nodes
        figsize: Figure size
        save_path: Optional path to save the figure
        
    Returns:
        Matplotlib figure with subplots for each layout
    """
    if layouts is None:
        layouts = ['hierarchical', 'dot', 'spring', 'circular', 'spectral']
    
    # Create figure
    num_layouts = len(layouts)
    rows = (num_layouts + 2) // 3  # 3 per row, rounded up
    cols = min(3, num_layouts)
    
    fig, axes = plt.subplots(rows, cols, figsize=figsize)
    if rows * cols == 1:
        axes = np.array([[axes]])
    elif rows == 1 or cols == 1:
        axes = axes.reshape(-1, 1) if cols == 1 else axes.reshape(1, -1)
        
    # Flatten axes for easier indexing
    axes_flat = axes.flatten()
    
    # Get the color scheme
    colors = COLOR_SCHEMES.get(color_scheme, COLOR_SCHEMES["default"])
    
    # Create the graph
    G = adjacency_to_nx_graph(graph_definition)
    
    # Draw each layout
    for i, layout in enumerate(layouts):
        if i < len(axes_flat):
            ax = axes_flat[i]
            
            # Get layout positions
            try:
                if layout == 'hierarchical':
                    pos = custom_hierarchical_layout(G)
                elif layout in ['dot', 'neato', 'fdp', 'sfdp', 'twopi', 'circo']:
                    pos = nx.nx_agraph.graphviz_layout(G, prog=layout)
                else:
                    if layout == 'spring':
                        pos = nx.spring_layout(G, seed=42)
                    elif layout == 'circular':
                        pos = nx.circular_layout(G)
                    elif layout == 'random':
                        pos = nx.random_layout(G, seed=42)
                    elif layout == 'shell':
                        pos = nx.shell_layout(G)
                    elif layout == 'spectral':
                        pos = nx.spectral_layout(G)
                    else:
                        pos = custom_hierarchical_layout(G)
            except Exception as e:
                print(f"Layout '{layout}' failed: {e}. Using custom hierarchical layout.")
                pos = custom_hierarchical_layout(G)
                
            # Draw graph with more prominent arrows
            nx.draw_networkx(
                G, pos,
                node_color=colors["node_color"],
                node_size=node_size,
                edge_color=colors["edge_color"],
                font_color=colors["font_color"],
                ax=ax,
                arrows=True,
                arrowstyle='-|>',
                arrowsize=20,  # Larger arrowsize for better visibility
                font_size=10,
                font_weight='bold',
                connectionstyle='arc3,rad=0.0',  # Straight connection lines
                min_source_margin=15,  # Margin from source node
                min_target_margin=15,  # Margin from target node
            )
            
            # Set title
            ax.set_title(layout.capitalize(), fontsize=12, fontweight='bold')
            
            # Turn off axis
            ax.axis('off')
    
    # Hide any unused subplots
    for i in range(len(layouts), len(axes_flat)):
        axes_flat[i].axis('off')
        
    # Adjust layout
    plt.tight_layout()
    
    # Save if requested
    if save_path:
        plt.savefig(save_path, bbox_inches='tight')
        print(f"Comparison saved to {save_path}")
    
    # Show the plot
    plt.show()
    
    return fig


# Example usage function
def example_usage():
    """Example of how to use the graph_pic module."""
    # Example graph definition
    graph_definition = {
        "1": {"next": ["2", "3", "4", "5"]},
        "2": {"next": ["6"]},
        "3": {"next": ["7"]},
        "4": {"next": ["7"]},
        "5": {"next": ["7"]},
        "6": {"next": ["8"]},
        "7": {"next": ["9"]},
        "8": {"next": ["10"]},
        "9": {"next": ["11"]},
        "10": {"next": ["11"]},
        "11": {"next": []}
    }
    
    # Basic visualization using our custom hierarchical layout
    visualize_graph(graph_definition, 
                   layout="hierarchical",
                   title="Precedence Graph", 
                   node_size=1200,
                   edge_width=2.0)
    
    # Compare different layouts
    compare_layouts(graph_definition, 
                   layouts=['hierarchical', 'spring', 'circular', 'spectral'])
    
    # For Jupyter Notebooks
    # visualize_to_display(graph_definition, layout='hierarchical', color_scheme='default')
    
    return "Example completed"


if __name__ == "__main__":
    example_usage()



================================================
FILE: src/flow4ai/job.py
================================================
import asyncio
import uuid
from abc import ABC, ABCMeta, abstractmethod
from contextlib import asynccontextmanager
from contextvars import ContextVar
from typing import Any, Dict, Optional, Type, Union

from . import f4a_logging as logging
from .utils.otel_wrapper import trace_function

SPLIT_STR = "$$"


# DSL imports moved inline to avoid circular imports


def _is_traced(method):
    """Helper function to check if a method is traced."""
    return hasattr(method, '_is_traced') and method._is_traced


def _has_own_traced_execute(cls):
    """Helper function to check if a class has its own traced _execute (not inherited)."""
    return '_execute' in cls.__dict__ and _is_traced(cls.__dict__['_execute'])


def _mark_traced(method):
    """Helper function to mark a method as traced."""
    method._is_traced = True
    return method


def traced_job(cls: Type) -> Type:
    """
    Class decorator that ensures the execute method is traced.
    This is only applied to the JobABC class itself.
    """
    if hasattr(cls, '_execute'):
        original_execute = cls._execute
        traced_execute = trace_function(original_execute, detailed_trace=True)
        traced_execute = _mark_traced(traced_execute)
        # Store original as executeNoTrace
        cls.executeNoTrace = original_execute
        # Replace execute with traced version
        cls._execute = traced_execute
    return cls


class JobMeta(ABCMeta):
    """Metaclass that automatically applies the traced_job decorator to JobABC only."""
    def __new__(mcs, name, bases, namespace):
        cls = super().__new__(mcs, name, bases, namespace)
        if name == 'JobABC':  # Only decorate the JobABC class itself
            return traced_job(cls)
        # For subclasses, ensure they inherit JobABC's traced _execute
        if '_execute' in namespace:
            # If subclass defines its own _execute, ensure it's not traced again
            # but still inherits the tracing from JobABC
            del namespace['_execute']
            cls = super().__new__(mcs, name, bases, namespace)
        return cls


class Task(dict):
    """A task dictionary with a unique identifier.
    
    Args:
        data (Union[Dict[str, Any], str]): The task data as a dictionary or string. If a string,
                                            it will be converted to a dictionary with a 'task' key.
        job_name (Optional[str], optional): The name of the job that will process this task.
                                            Required if there is more than one job graph in the
                                            FlowManagerMP class"""
    def __init__(self, data: Union[Dict[str, Any], str], job_name: Optional[str] = None):
        # Convert string input to dict
        if isinstance(data, str):
            data = {'task': data}
        elif isinstance(data, dict):
            data = data.copy()  # Create a copy to avoid modifying the original
        else:
            data = {'task': str(data)}
        
        super().__init__(data)
        self.task_id:str = str(uuid.uuid4())
        if job_name is not None:
            self['job_name'] = job_name

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, Task):
            return NotImplemented
        return self.task_id == other.task_id

    # mypy highlights this as an error because dicts are mutable
    #   and so not hashable, but I want each Task to have a unique id
    #   so it is hashable.
    def __hash__(self) -> int:
        return hash(self.task_id)

    def __repr__(self) -> str:
        job_name = self.get('job_name', 'None')
        task_preview = str(dict(self))[:50] + '...' if len(str(dict(self))) > 50 else str(dict(self))
        return f"Task(id={self.task_id}, job_name={job_name}, data={task_preview})"

class JobState:
  def __init__(self):
      self.inputs: Dict[str, Dict[str, Any]] = {}
      self.input_event = asyncio.Event()
      self.execution_started = False

job_graph_context : ContextVar[dict] = ContextVar('job_graph_context')

@asynccontextmanager
async def job_graph_context_manager(job_set: set['JobABC']):
  """Create a new context for job execution, with a new JobState."""
  new_state = {}
  for job in job_set:
      new_state[job.name] = JobState()
  new_state[JobABC.CONTEXT] = {}
  new_state[JobABC.CONTEXT][JobABC.SAVED_RESULTS] = {}
  token = job_graph_context.set(new_state)
  try:
      yield new_state
  finally:
      job_graph_context.reset(token)

class JobABC(ABC, metaclass=JobMeta):
    """
    Abstract base class for jobs. Only this class will have tracing enabled through the JobMeta metaclass.
    Subclasses will inherit the traced version of _execute but won't add additional tracing.
    """

    # class variable to keep track of instance counts for each class
    _instance_counts: Dict[Type, int] = {}
    
    # Key used to pass task metadata through the job graph
    TASK_PASSTHROUGH_KEY: str = 'task_pass_through'
    RETURN_JOB='RETURN_JOB'
    CONTEXT='CONTEXT'
    SAVED_RESULTS='SAVED_RESULTS'

    def __init__(self, name: Optional[str] = None, properties: Dict[str, Any] = {}):
        """
        Initialize an JobABC instance.

        Args:
            name (Optional[str], optional): Must be a unique identifier for this job within the context of a FlowManager.
                                            If not provided, a unique name will be auto-generated.
            properties (Dict[str, Any], optional): configuration properties passed in by jobs.yaml
        """
        self.name:str = self._getUniqueName() if name is None else name
        self.save_result: bool = bool(properties.get("save_result", False))
        self.properties:Dict[str, Any] = properties
        self.expected_inputs:set[str] = set()
        self.next_jobs:list[JobABC] = [] 
        self.timeout = 3000
        self.logger = logging.getLogger(self.__class__.__name__)
        self.global_ctx = None

    def __or__(self, other):
        """Implements the | operator for parallel composition"""
        # Import DSL classes inline to avoid circular imports
        from .dsl import Parallel, Serial
        from .jobs.wrapping_job import WrappingJob

        if isinstance(other, Parallel):
            # If right side is already a parallel component, add to its components
            return Parallel(*([self] + other.components))
        elif isinstance(other, JobABC):
            return Parallel(self, other)
        elif isinstance(other, Serial):
            return Parallel(self, other)
        else:
            # If other is a raw object, wrap it first
            return Parallel(self, WrappingJob(other))
    
    def __rshift__(self, other):
        """Implements the >> operator for serial composition"""
        # Import DSL classes inline to avoid circular imports
        from .dsl import Parallel, Serial
        from .jobs.wrapping_job import WrappingJob

        if isinstance(other, Serial):
            # If right side is already a serial component, add to its components
            return Serial(*([self] + other.components))
        elif isinstance(other, JobABC):
            return Serial(self, other)
        elif isinstance(other, Parallel):
            return Serial(self, other)
        else:
            # If other is a raw object, wrap it first
            return Serial(self, WrappingJob(other))
    
    @classmethod
    def create_FQName(cls, graph_name, parameter_name, short_graph_job_name, dsl_id=None):
        """
        Creates a unique, fully qualified name from the graph, parameter and job names.
        
        Args:
            graph_name: Name of the graph
            parameter_name: Parameter name (usually variant)
            short_graph_job_name: The job name (short form)
            dsl_id: Optional unique identifier for the source DSL to prevent FQ name collisions
            
        Returns:
            str: A fully qualified name in format graph_name$$parameter_name$$short_graph_job_name$$
                 or graph_name$$parameter_name-dsl_id$$short_graph_job_name$$ if dsl_id is provided
        """
        # If a DSL identifier is provided, incorporate it into the parameter name
        # This ensures unique FQ names while maintaining compatibility with existing parsing logic
        if dsl_id:
            # Embed the DSL ID in the parameter name to maintain compatibility with parsers
            if parameter_name:
                enhanced_param_name = f"{parameter_name}-{dsl_id}"
            else:
                enhanced_param_name = dsl_id
        else:
            enhanced_param_name = parameter_name
            
        unique_job_name = graph_name + SPLIT_STR + enhanced_param_name + SPLIT_STR + short_graph_job_name + SPLIT_STR
        return unique_job_name

    @classmethod
    def parse_job_loader_name(cls, name: str) -> Dict[str, str]:
        """Parse a job loader name into its constituent parts.
        
        Args:
            name: The full job loader name string in format:
                 graph_name$$param_name$$job_name$$
                 
        Returns:
            dict: A dictionary containing graph_name, param_name, and job_name,
                 or {'parsing_message': 'UNSUPPORTED NAME FORMAT'} if invalid
        """
        try:
            parts = name.split(SPLIT_STR)
            if len(parts) != 4 or parts[3] != "" or not parts[0]:
                return {"parsing_message": "UNSUPPORTED NAME FORMAT"}
                
            return {
                "graph_name": parts[0],
                "param_name": parts[1],
                "job_name": parts[2]
            }
        except:
            return {"parsing_message": "UNSUPPORTED NAME FORMAT"}

    @classmethod
    def parse_graph_name(cls, name: str) -> str:
        """Parse and return the graph name from a job loader name.
        
        Args:
            name: The full job loader name string
            
        Returns:
            str: The graph name or 'UNSUPPORTED NAME FORMAT' if invalid
        """
        result = cls.parse_job_loader_name(name)
        return result.get("graph_name", "UNSUPPORTED NAME FORMAT")

    @classmethod
    def parse_param_name(cls, name: str) -> str:
        """Parse and return the parameter name from a job loader name.
        
        Args:
            name: The full job loader name string
            
        Returns:
            str: The parameter name or 'UNSUPPORTED NAME FORMAT' if invalid
        """
        result = cls.parse_job_loader_name(name)
        return result.get("param_name", "UNSUPPORTED NAME FORMAT")

    @classmethod
    def parse_job_name(cls, name: str) -> str:
        """Parse and return the job name from a job loader name.
        
        Args:
            name: The full job loader name string
            
        Returns:
            str: The job name or 'UNSUPPORTED NAME FORMAT' if invalid
        """
        result = cls.parse_job_loader_name(name)
        return result.get("job_name", "UNSUPPORTED NAME FORMAT")

    @classmethod
    def _getUniqueName(cls):
        # Increment the counter for the current class
        cls._instance_counts[cls] = cls._instance_counts.get(cls, 0) + 1
        # Return a unique name based on the current class
        return f"{cls.__name__}_{cls._instance_counts[cls]}"

    @classmethod
    def get_input_from(cls, inputs: Dict[str, Any], job_name: str) -> Dict[str, Any]:
        """Get input data from a specific job in the inputs dictionary.
        
        Args:
            inputs (Dict[str, Any]): Dictionary of inputs from various jobs
            job_name (str): Name of the job whose input we want to retrieve
            
        Returns:
            Dict[str, Any]: The input data from the specified job, or empty dict if not found
        """
        for key in inputs.keys():
            if cls.parse_job_name(key) == job_name:
                return inputs[key]
        return {}


    @classmethod
    def job_set(cls, job) -> set['JobABC']:
        """
        Returns a set of all unique job instances in the job graph by recursively traversing
        all possible paths through next_jobs.
        
        Returns:
            set[JobABC]: A set containing all unique job instances in the graph
        """
        result = {job}  # Start with current job instance
        
        # Base case: if no next jobs, return current set
        if not job.next_jobs:
            return result
            
        # Recursive case: add all jobs from each path
        for job in job.next_jobs:
            result.update(cls.job_set(job))
            
        return result

    def __repr__(self):
        next_jobs_str = [job.name for job in self.next_jobs]
        expected_inputs_str = [input_name for input_name in self.expected_inputs]
        return (f"name: {self.name}\n"
                f"next_jobs: {next_jobs_str}\n"
                f"expected_inputs: {expected_inputs_str}\n"
                f"properties: {self.properties}")

    async def _execute(self, task: Union[Task, None]) -> Dict[str, Any]:
        """ Responsible for executing the job graph, maintaining state of the graph
        by updating the JobState object and propagating the tail results back up the graph
        when a tail job is reached.

        This is a classic dataflow execution model, where computation proceeds based on data 
        availability rather than a predetermined sequence.

        WARNING: DO NOT OVERRIDE THIS METHOD IN CUSTOM JOB CLASSES.
        This method is part of the core Flow4AI execution flow and handles critical operations
        including job graph traversal, state management, and result propagation.
        
        Instead, implement the abstract 'run' method to define custom job behavior.

        Can only be used within a job_graph_context set up with:
           ```python
            async with job_graph_context_manager(job_set):
                        result = await job._execute(task)
            ```
        The recursive nature of the algorithm means that results flow upwards, with the head job appearing
        to return the result of the tail job.

        Args:
            task (Union[Task, None]): the input to the first (head) job of the job graph, is None in child jobs.

        Returns:
            Dict[str, Any]: The output of the job graph execution
        """
        job_state_dict:dict = job_graph_context.get()
        job_state = job_state_dict.get(self.name)
        if self.is_head_job() and  isinstance(task, dict):
            job_state.inputs.update(task)
            self.get_context()[JobABC.TASK_PASSTHROUGH_KEY] = task
        elif task is None:
            pass 
        else:
            job_state.inputs[self.name] = task

        if self.expected_inputs:
            if job_state.execution_started:
                return None
            
            job_state.execution_started = True
            try:
                await asyncio.wait_for(job_state.input_event.wait(), self.timeout)
            except asyncio.TimeoutError:
                job_state.execution_started = False
                raise TimeoutError(
                    f"Timeout waiting for inputs in {self.name}. "
                    f"Expected: {self.expected_inputs}, "
                    f"Received: {list(job_state.inputs.keys())}"
                )

        result = await self.run(task)
        self.logger.debug(f"Job {self.name} finished running")

        if self.save_result:
            saved_results = self.get_context()[JobABC.SAVED_RESULTS]
            saved_results[self.name] = result

        if not isinstance(result, dict):
            result = {'result': result}

        # Clear state for potential reuse
        job_state.inputs.clear()
        job_state.input_event.clear()
        job_state.execution_started = False

        # Store the job name that returns the result
        result[JobABC.RETURN_JOB] = self.name

        # If this is a tail job, return immediately
        if not self.next_jobs:
            self.logger.debug(f"Tail Job {self.name} returning result: {result}")
            task = self.get_context()[JobABC.TASK_PASSTHROUGH_KEY]
            result[JobABC.TASK_PASSTHROUGH_KEY] = task
            saved_results = self.get_context().get(JobABC.SAVED_RESULTS, {})
            if saved_results:
                result[JobABC.SAVED_RESULTS] = {JobABC.parse_job_name(k): v for k, v in saved_results.items()}
            return result

        # Check if any child jobs are ready to execute once given this result as input
        executing_jobs = []
        for next_job in self.next_jobs:
            input_data = result.copy()
            # add result data from this job as an input to the next job
            await next_job.receive_input(self.name, input_data)
            next_job_inputs = job_state_dict.get(next_job.name).inputs
            # if the next job has all its inputs add coroutine to list to execute
            if next_job.expected_inputs.issubset(set(next_job_inputs.keys())):
                the_task = self.get_task()
                executing_jobs.append(next_job._execute(task=the_task))

        # If there are any child jobs ready to execute, execute them, else return None 
        if executing_jobs:
            # await for all futures to return results
            child_results = await asyncio.gather(*executing_jobs)
            not_none_results = [r for r in child_results if r is not None]
            if not_none_results:
                # Return the first valid result.
                # The recursive nature of the algorithm means that results flow upwards, with the head job appearing
                # to return the result of the tail job, so returning the first valid result will return the tail job
                # result up the stack.
                first_valid_result = not_none_results[0]
                self.logger.debug(f"Job {self.name} propagating first valid result: {first_valid_result}")
                return first_valid_result

        # If no child jobs executed or no valid result found, return None
        return None

    async def receive_input(self, from_job: str, data: Dict[str, Any]) -> None:
        """Receive input from a predecessor job"""
        job_state_dict:dict = job_graph_context.get()
        job_state = job_state_dict.get(self.name)
        job_state.inputs[from_job] = data
        if self.expected_inputs.issubset(set(job_state.inputs.keys())):
            job_state.input_event.set()

    def job_set_str(self) -> set[str]:
        """
        Returns a set of all unique job names in the job graph by recursively traversing
        all possible paths through next_jobs.
        
        Returns:
            set[str]: A set containing all unique job names in the graph
        """
        result = {self.name}  # Start with current job's name
        
        # Base case: if no next jobs, return current set
        if not self.next_jobs:
            return result
            
        # Recursive case: add all jobs from each path
        for job in self.next_jobs:
            result.update(job.job_set_str())
            
        return result

    def is_head_job(self) -> bool:
        """
        Check if this job is a head job (has no expected inputs).

        Returns:
            bool: True if this is a head job (no expected inputs), False otherwise
        """
        return len(self.expected_inputs) == 0

    def get_context(self) -> Dict[str, Any]:
        """
        Returns an object that can be used to store context across jobs in a graph for a single coroutine.
           can only be used within a job_graph_context set up with:
           ```python
            async with job_graph_context_manager(job_set):
                        result = await job._execute(task)
        """
        job_state_dict:dict = job_graph_context.get()
        context = job_state_dict[JobABC.CONTEXT]
        return context

    def _get_long_name_inputs(self) -> Dict[str, Any]:
        """
        Returns the inputs for this job.

        Returns:
            Dict[str, Any]: Returns the inputs to this job with long fully qualified job names as keys
        """
        job_state_dict:dict = job_graph_context.get()
        jobstate:JobState = job_state_dict[self.name]
        inputs: Dict[str, Dict[str, Any]] = jobstate.inputs
        return inputs   
    
    def get_inputs(self) -> Dict[str, Dict[str, Any]]:
        """
        Returns the inputs to this job with short job names as keys.

        Returns:
            Dict[str, Dict[str, Any]]: The inputs to this job.
        """
        inputs: Dict[str, Dict[str, Any]] = self._get_long_name_inputs()
        inputs_with_short_job_name = {JobABC.parse_job_name(k): v for k, v in inputs.items()}
        self.logger.debug(f"Returning inputs: {inputs_with_short_job_name}")
        return inputs_with_short_job_name

    def get_task(self) -> Union[Dict[str, Any], Task]:
        """
        Get the task associated with this job.

        Returns:
            Union[Dict[str, Any], Task]: The task associated with this job.
        """
        task = self.get_context()[JobABC.TASK_PASSTHROUGH_KEY]
        return task
        
        # if not self.is_head_job(): 
        #     first_parent_result = next(iter(inputs.values()))
        #     task = first_parent_result[JobABC.TASK_PASSTHROUGH_KEY]
        # else:
        #     task = inputs
        # return task

    def update_context(self, new_context: Dict[str, Any]) -> None:
        """
        Update the context dictionary with new values.
        
        Args:
            new_context: Dictionary with new context values
        """
        self.global_ctx.update(new_context)

    @abstractmethod
    async def run(self, task: Union[Dict[str, Any], Task]) -> Dict[str, Any]:
        """Execute the job on the given task. Must be implemented by subclasses."""
        pass

# SimpleJob and SimpleJobFactory have been moved to tests/test_utils/simple_job.py
# They are only used for testing purposes and not for production code.



================================================
FILE: src/flow4ai/job_loader.py
================================================
import copy
import importlib.util
import inspect
import os
import sys
from pathlib import Path
from typing import Any, Collection, Dict, List, Type, Union

import yaml
from pydantic import BaseModel

from . import f4a_logging as logging
from .f4a_graph import validate_graph
from .job import JobABC

logger = logging.getLogger(__name__)


class JobValidationError(Exception):
    """Raised when a custom job fails validation"""
    pass


class ConfigurationError(Exception):
    """Exception raised when configuration is malformed."""
    pass


class PythonLoader:
    JOBS = "jobs"
    PYDANTIC = "pydantic"

    @staticmethod
    def validate_pydantic_class(job_class: Type) -> bool:
        """Validate that a class meets the requirements to be a valid pydantic model:
        - Inherits from BaseModel
        """
        return inspect.isclass(job_class) and issubclass(job_class, BaseModel)

    @staticmethod
    def validate_job_class(job_class: Type) -> bool:
        """
        Validate that a class meets the requirements to be a valid job:
        - Inherits from JobABC
        - Has required methods
        - Has required attributes
        """
        # Check if it's a class and inherits from JobABC
        if not (inspect.isclass(job_class) and issubclass(job_class, JobABC)):
            return False

        # Check for required async run method
        if not hasattr(job_class, 'run'):
            return False

        # Check if run method is async
        run_method = getattr(job_class, 'run')
        if not inspect.iscoroutinefunction(run_method):
            return False

        return True

    @classmethod
    def load_python(cls, python_dir: str, type_name: str = JOBS) -> Dict[str, Union[Type[JobABC], Type[BaseModel]]]:
        """
        Load all custom job classes from the specified directory
        """
        python_classes = {}
        python_path = Path(python_dir)

        if not python_path.exists():
            logger.info(f"Python directory not found: {python_dir}")
            return python_classes

        # Add the custom jobs directory to Python path
        logger.debug(f"Python path before: {sys.path}")
        sys.path.append(str(python_path))
        logger.info(f"Added {python_path} to Python path")
        logger.debug(f"Python path after: {sys.path}")

        # Scan for Python files
        for file_path in python_path.glob("**/*.py"):
            if file_path.name.startswith("__"):
                continue

            logger.info(f"Loading python classes from {file_path}")
            try:
                # Load the module
                module_name = file_path.stem
                spec = importlib.util.spec_from_file_location(module_name, str(file_path))
                if spec is None or spec.loader is None:
                    logger.warning(f"Could not create module spec for {file_path}")
                    continue

                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)

                # Find all classes in the module that inherit from JobABC
                for name, obj in inspect.getmembers(module):
                    if inspect.isclass(obj) and obj.__module__ == module.__name__:
                        try:
                            if ((type_name == cls.JOBS and cls.validate_job_class(obj)) or
                                (type_name == cls.PYDANTIC and cls.validate_pydantic_class(obj))):
                                logger.info(f"Found valid python class: {name}")
                                python_classes[name] = obj
                        except Exception as e:
                            logger.error(f"Error validating python class {name} in {file_path}: {str(e)}")
                            raise JobValidationError(
                                f"Error validating python class {name} in {file_path}: {str(e)}"
                            )

            except Exception as e:
                logger.error(f"Error loading custom python class from {file_path}: {str(e)}")
                raise ImportError(
                    f"Error loading custom python class from {file_path}: {str(e)}"
                )

        return python_classes


class JobFactory:
    _job_types_registry: Dict[str, Type[JobABC]] = {}
    _pydantic_types_registry: Dict[str, Type[BaseModel]] = {}
    # Default jobs directory is always checked first
    _default_jobs_dir: str = os.path.join(os.path.dirname(__file__), "jobs") # site-package directory when this is a package
    _cached_job_graphs: List[JobABC] = None

    @classmethod
    def load_python_into_registries(cls, custom_python_dirs: list[str] = None):
        """
        Load and register all custom jobs from specified config directories.
        Will look for jobs in the 'jobs' subdirectory of each config directory.
        Loads jobs from all directories.

        Args:
            custom_python_dirs: List of config directory paths. Jobs will be loaded from the 'jobs' subdirectory
                            of each config directory.
        """
        loader = PythonLoader()
        # Create an iterable of job directories, including the default and any custom directories
        python_dirs = [cls._default_jobs_dir]
        if custom_python_dirs:
            # Add local jobs directories from each config directory
            for config_dir in custom_python_dirs:
                python_dir = os.path.join(config_dir, "jobs")
                if os.path.exists(python_dir):
                    python_dirs.append(python_dir)
            
        found_valid_jobs = False
        for python_dir in python_dirs:
            # Load and register jobs
            jobs = loader.load_python(python_dir, PythonLoader.JOBS)
            if jobs:
                found_valid_jobs = True
                # Register all valid custom jobs
                for job_name, job_class in jobs.items():
                    cls.register_job_type(job_name, job_class)
                    print(f"Registered custom job: {job_name}")
            
            # Load and register pydantic models
            pydantic_models = loader.load_python(python_dir, PythonLoader.PYDANTIC)
            if pydantic_models:
                for model_name, model_class in pydantic_models.items():
                    cls.register_pydantic_type(model_name, model_class)
                    print(f"Registered pydantic model: {model_name}")
            else:
                logger.info("No pydantic classes found")
        
        if not found_valid_jobs:
            # This is a critical error as we need at least one valid job directory
            raise FileNotFoundError(f"No valid jobs found in any of the directories: {python_dirs}")

    @classmethod
    def create_job(cls, name: str, job_type: str, job_def: Dict[str, Any]) -> JobABC:
        if job_type not in cls._job_types_registry:
            logger.error(f"*** Unknown job type: {job_type} ***")
            raise ValueError(f"Unknown job type: {job_type}")
        
        properties = job_def.get('properties', {})
        if not properties:
            logger.info(f"No properties specified for job {name} of type {job_type}")
            
        return cls._job_types_registry[job_type](name, properties)

    @classmethod
    def register_job_type(cls, type_name: str, job_class: Type[JobABC]):
        cls._job_types_registry[type_name] = job_class

    @classmethod
    def register_pydantic_type(cls, type_name: str, model_class: Type[BaseModel]):
        """Register a Pydantic model type with the factory"""
        cls._pydantic_types_registry[type_name] = model_class

    @classmethod
    def get_pydantic_class(cls, type_name: str) -> Type[BaseModel]:
        """Retrieve a registered Pydantic model class by its type name."""
        if type_name not in cls._pydantic_types_registry:
            raise ValueError(f"Pydantic type {type_name} not registered.")
        return cls._pydantic_types_registry[type_name]

    @classmethod
    def get_head_jobs_from_config(cls) -> Collection[JobABC]:
        JobFactory.load_python_into_registries(ConfigLoader.directories)
        """Create job graphs from configuration, using cache if available"""
        if cls._cached_job_graphs is None:
            job_graphs: list[JobABC] = []
            graphs_config = ConfigLoader.get_graphs_config()
            graph_names = list(graphs_config.keys())
            for graph_name in graph_names:
                graph_def = graphs_config[graph_name]
                job_names_in_graph = list(graph_def.keys())
                param_groups_for_graph_name = ConfigLoader.get_parameters_config().get(graph_name, {})
                if param_groups_for_graph_name:
                    param_jobs_graphs: List[JobABC] = cls.create_job_graph_using_parameters(graph_def, graph_name,
                                                                                            param_groups_for_graph_name,
                                                                                            job_names_in_graph)
                    job_graphs += param_jobs_graphs
                else:
                    job_graph_no_params: JobABC = cls.create_job_graph_no_params(graph_def, graph_name,
                                                                                 job_names_in_graph)
                    job_graphs.append(job_graph_no_params)
            cls._cached_job_graphs = job_graphs
        return cls._cached_job_graphs

    @classmethod
    def create_job_graph_using_parameters(cls, graph_def, graph_name, param_groups_for_graph_name,
                                           job_names_in_graph) -> List[JobABC]:
        job_graphs: list[JobABC] = []
        parameter_names_list = list(param_groups_for_graph_name.keys())
        for parameter_name in parameter_names_list:
            job_instances: dict[str, JobABC] = {}
            # Create a copy of graph_def for this parameter group
            # This ensures each parameter group has its own graph definition
            param_graph_def = copy.deepcopy(graph_def)
            
            # Create job instances for this parameter group
            for short_graph_job_name in job_names_in_graph:
                raw_job_def: Dict[str, Any] = ConfigLoader.get_jobs_config()[short_graph_job_name]
                if ConfigLoader.is_parameterized_job(raw_job_def):
                    job_def: Dict[str, Any] = ConfigLoader.fill_job_with_parameters(raw_job_def, graph_name, parameter_name)
                else:
                    job_def = raw_job_def
                unique_job_name = JobABC.create_FQName(graph_name, parameter_name, short_graph_job_name)
                job_type: str = job_def["type"]
                job: JobABC = cls.create_job(unique_job_name, job_type, job_def)
                job_instances[short_graph_job_name] = job
                
            # Create the job graph for this parameter group
            job_graph: JobABC = cls.create_job_graph(param_graph_def, job_instances)
            job_graphs.append(job_graph)
        return job_graphs



    @classmethod
    def create_job_graph_no_params(cls, graph_def, graph_name, job_names_in_graph)-> JobABC:
        job_instances: dict[str, JobABC] = {}
        for short_graph_job_name in job_names_in_graph:
                job_def: Dict[str, Any] = ConfigLoader.get_jobs_config()[short_graph_job_name]
                unique_job_name = JobABC.create_FQName(graph_name, "", short_graph_job_name)
                job_type: str = job_def["type"]
                job: JobABC = cls.create_job(unique_job_name, job_type, job_def)
                job_instances[short_graph_job_name] = job
        job_graph: JobABC = cls.create_job_graph(graph_def, job_instances)
        return job_graph

    @classmethod
    def create_job_graph(cls, graph_definition: dict[str, dict], job_instances: dict[str, JobABC]) -> JobABC:
        """
        graph definition defines the job graph and looks like this:

        graph_definition: dict[str, Any] = {
            "A": {"next": ["B", "C"]},
            "B": {"next": ["D"]},
            "C": {"next": ["D"]},
            "D": {"next": []},
        }

        job instances are a dictionary of job instances in the job graph and looks like this:

        job_instances: dict[str, JobABC] = {
            "A": SimpleJob("A"),
            "B": SimpleJob("B"),
            "C": SimpleJob("C"),
            "D": SimpleJob("D"),
        }
        
        """
        nodes:dict[str, JobABC] = {} # nodes holds Jobs which will be hydrated with next_jobs 
                                    # and expected_inputs fields from the graph_definition.
        
        # determine the incoming edges i.e the Jobs that each Job depends on
        # so we can determine the head node ( which depends on no Jobs) 
        # and set the expected_inputs (i.e. the dependencies) for each Job.
        incoming_edges: dict[str, set[str]] = {job_name: set() for job_name in graph_definition}
        for job_name, config in graph_definition.items():
            for next_job in config['next']:
                incoming_edges[next_job].add(job_name)
        
        # 1) Find the head node (node with no incoming edges)
        head_jobs = [job_name for job_name, inputs in incoming_edges.items() if not inputs]
        
        # Handle multiple head jobs before creating nodes dictionary
        if len(head_jobs) > 1:
            head_job_name = cls.add_default_head(graph_definition, head_jobs, job_instances, nodes)
        elif len(head_jobs) == 1:
            head_job_name = head_jobs[0]
        else:
            raise ValueError("No head nodes found in graph definition")
            
        # Now populate the nodes dictionary after potential DefaultHeadJob addition
        for job_name in graph_definition:
            try:
                job_obj = job_instances[job_name]
            except KeyError:
                # Fallback mechanism for parameterized job names
                # Try to find the job by parsing the job name and looking for the base job name
                parsed_name = JobABC.parse_job_name(job_name)
                if parsed_name != "UNSUPPORTED NAME FORMAT" and parsed_name in job_instances:
                    job_obj = job_instances[parsed_name]
                    logger.debug(f"Found job {job_name} using parsed name {parsed_name}")
                else:
                    raise KeyError(f"Job {job_name} not found in job_instances and no fallback available")
            nodes[job_name] = job_obj
            
        # 1.5) Find the tail nodes (nodes with no outgoing edges)
        tail_jobs = [job_name for job_name, config in graph_definition.items() if not config['next']]
        
        if len(tail_jobs) > 1:
            # Add a default tail node if there are multiple tail nodes
            cls.add_default_tail(graph_definition, tail_jobs, job_instances, nodes)

        # 2) Set next_jobs for each node
        for job_name, config in graph_definition.items():
            nodes[job_name].next_jobs = [nodes[next_name] for next_name in config['next']]

        # 3) Set expected_inputs for each node using fully qualified names
        for job_name, input_job_names_set in incoming_edges.items():
            if input_job_names_set:  # if node has incoming edges
                # Transform short names to fully qualified names using the job_instances dictionary
                nodes[job_name].expected_inputs = {job_instances[input_name].name for input_name in input_job_names_set}

        # 4) Set reference to final node in head node -- not needed!
        # Find node with no next jobs
        # final_job_name = next(job_name for job_name, config in graph_definition.items() 
        #                    if not config['next'])
        # nodes[head_job_name].final_node = nodes[final_job_name]

        return nodes[head_job_name]

    @classmethod
    def add_default_head(cls, graph_definition, head_jobs, job_instances, nodes):
        from flow4ai.jobs.default_jobs import DefaultHeadJob

        # Get naming from first job instance in job_instances
        sample_job = next(iter(job_instances.values()))
        sample_name = sample_job.name
        parsed = JobABC.parse_job_name(sample_name)
        # Debug logging for sample name and parsed name
        logger.debug(f"DEBUG - Sample name (long): {sample_name}")
        logger.debug(f"DEBUG - Parsed name (short): {parsed}")
        
        # For each parameter group, we need to create a unique DefaultHeadJob
        # Extract parameter group from sample name
        parsed_parts = JobABC.parse_job_loader_name(sample_name)
        graph_name = parsed_parts.get("graph_name", "")
        param_name = parsed_parts.get("param_name", "")
        
        # Create a unique name for this DefaultHeadJob based on the parameter group
        if graph_name and param_name:
            # Use the same format as in create_job_graph_using_parameters
            unique_job_name = f"{graph_name}$${param_name}$$DefaultHeadJob$$"
            default_head = DefaultHeadJob(name=unique_job_name)
            logger.debug(f"Constructed DefaultHeadJob name: {unique_job_name}")
        elif graph_name:
            unique_job_name = f"{graph_name}$$$$DefaultHeadJob$$"
            default_head = DefaultHeadJob(name=unique_job_name)
            logger.debug(f"Constructed DefaultHeadJob name: {unique_job_name}")
        else:
            default_head = DefaultHeadJob()
            logger.warning("Falling back to default naming for head job")
        
        logger.debug(f"Created DefaultHeadJob with name: {default_head.name}")
        
        # Use 'DefaultHeadJob' as the key in graph_definition and job_instances
        # This is the key that will be used in the graph definition
        head_job_key = default_head.name #"DefaultHeadJob"
        
        # Add the job to job_instances with the DefaultHeadJob key
        job_instances[head_job_key] = default_head
        
        # Add to graph_definition with the same key
        graph_definition[head_job_key] = {"next": head_jobs}
        
        # Add the default head job to nodes dictionary
        nodes[head_job_key] = default_head
        
        return head_job_key
        
    @classmethod
    def add_default_tail(cls, graph_definition, tail_jobs, job_instances, nodes):
        """Add a default tail node to the graph when multiple tail nodes are detected.
        
        Args:
            graph_definition: The graph definition dictionary to modify
            tail_jobs: List of job names that are tail nodes (no 'next' nodes)
            job_instances: Dictionary of job instances
            nodes: Dictionary of nodes in the graph
            
        Returns:
            The name of the default tail job
        """
        from flow4ai.jobs.default_jobs import DefaultTailJob

        # Get naming from first job instance in job_instances
        sample_job = next(iter(job_instances.values()))
        sample_name = sample_job.name
        parsed = JobABC.parse_job_name(sample_name)
        
        # Debug logging for sample name and parsed name
        logger.debug(f"DEBUG - Sample name (long): {sample_name}")
        logger.debug(f"DEBUG - Parsed name (short): {parsed}")
        
        if parsed != 'UNSUPPORTED NAME FORMAT':
            # Replace the short job name with "DefaultTailJob" while maintaining the $$ format
            # Example: "multi_tail_demo$$params1$$tail_job_alpha$$" -> "multi_tail_demo$$params1$$DefaultTailJob$$"
            new_name = sample_name.replace(parsed + "$$", "DefaultTailJob$$")
            default_tail = DefaultTailJob(name=new_name)
            logger.debug(f"Constructed DefaultTailJob name: {new_name}")
        else:
            default_tail = DefaultTailJob()
            logger.warning("Falling back to default naming for tail job")
            
        logger.debug(f"Created DefaultTailJob with name: {default_tail.name}")
        
        # Add the default tail job to job_instances and nodes dictionaries
        job_instances[default_tail.name] = default_tail
        nodes[default_tail.name] = default_tail
        
        # Update the graph definition to make all tail nodes point to the default tail
        graph_definition[default_tail.name] = {"next": []}
        
        # Update all tail nodes to point to the default tail
        for tail_job in tail_jobs:
            graph_definition[tail_job]["next"] = [default_tail.name]
            
        return default_tail.name


class ConfigLoader:
    # Directories are searched in order. If a valid flow4ai directory is found,
    # the search stops and uses that directory.
    # TODO: Nice to have - Add support for merging configurations from multiple directories
    #       if required in the future.
    directories: List[str] = [
        os.path.join(os.getcwd(), "flow4ai"),  # flow4ai directory in current working directory
        os.path.join(os.path.expanduser("~"), "flow4ai"),  # ~/flow4ai
        "/etc/flow4ai"
    ]
    _cached_configs: Dict[str, dict] = None

    @classmethod
    def _set_directories(cls, directories):
        """Set the directories and clear the cache"""
        cls.directories = directories
        cls._cached_configs = None

    @classmethod
    def __setattr__(cls, name, value):
        """Clear cache when directories are changed"""
        super().__setattr__(name, value)
        if name == 'directories':
            cls._cached_configs = None

    @classmethod
    def load_configs_from_dirs(
            cls,
            directories: List[str] = [],
            config_bases: List[str] = ['graphs', 'jobs', 'parameters', 'flow4ai_all'],
            allowed_extensions: tuple = ('.yaml', '.yml', '.json')
    ) -> Dict[str, dict]:
        """
        Load configuration files from directories. Will search directories in order and stop
        at the first valid flow4ai directory found.
        
        Args:
            directories: List of directory paths to search
            config_bases: List of configuration file base names to look for
            allowed_extensions: Tuple of allowed file extensions
        
        Returns:
            Dictionary with config_base as key and loaded config as value
            
        Raises:
            FileNotFoundError: If no valid flow4ai directory is found in any of the directories
            ConfigurationError: If configuration files are malformed
        """
        configs: Dict[str, dict] = {}
        config_files: Dict[str, str] = {}  # Track which file each config came from

        # Convert directories to Path objects
        dir_paths = [Path(str(d)) for d in directories]
        logger.info(f"Looking for config files in directories: {dir_paths}")

        found_valid_dir = False
        for dir_path in dir_paths:
            if not dir_path.exists():
                logger.info(f"Directory not found, skipping: {dir_path}")
                continue

            # Check if any config files exist in this directory
            has_configs = False
            for config_base in config_bases:
                for ext in allowed_extensions:
                    if (dir_path / f"{config_base}{ext}").exists():
                        has_configs = True
                        break
                if has_configs:
                    break

            if has_configs:
                found_valid_dir = True
                logger.info(f"Found valid flow4ai directory: {dir_path}")
                # Load configs from this directory only
                for config_base in config_bases:
                    for ext in allowed_extensions:
                        config_path = dir_path / f"{config_base}{ext}"
                        if config_path.exists():
                            try:
                                with open(config_path) as f:
                                    configs[config_base] = yaml.safe_load(f)
                                    config_files[config_base] = str(config_path)
                            except yaml.YAMLError as e:
                                error_msg = "Configuration is malformed. Unable to proceed with job execution.\n\n" \
                                        "-------------------------------------------------------\n" \
                                        "          Configuration file malformed - cannot continue\n" \
                                        "-------------------------------------------------------\n" \
                                        f"File: {config_path}\n" \
                                        f"Error details: {str(e)}\n" \
                                        "-------------------------------------------------------"
                                raise ConfigurationError(error_msg) from e
                break  # Stop searching after finding first valid directory

        if not found_valid_dir:
            raise FileNotFoundError(f"No valid flow4ai directory found in search paths: {dir_paths}")

        # Store file paths in configs
        configs['__files__'] = config_files
        return configs

    @classmethod
    def _extract_config_section(cls, configs: Dict[str, dict], section_name: str) -> dict:
        """
        Extract a configuration section from either a dedicated file or flow4ai_all.
        
        Args:
            configs: Dictionary containing all configurations
            section_name: Name of the section to extract (e.g., 'graphs', 'jobs', 'parameters')
            
        Returns:
            Dictionary containing the configuration section, or empty dict if not found
        """
        # Try to get from dedicated file first
        if section_name in configs:
            return configs[section_name]

        # If not found, try to get from flow4ai_all
        if 'flow4ai_all' in configs and isinstance(configs['flow4ai_all'], dict):
            return configs['flow4ai_all'].get(section_name, {})

        # If nothing found, return empty dict
        return {}

    @classmethod
    def _find_parameterized_fields(cls, job_config: dict) -> set:
        """
        Find all parameterized fields in a job configuration.
        A field is parameterized if its value starts with '$'.
        
        Args:
            job_config: Job configuration dictionary
            
        Returns:
            Set of parameterized field names
        """
        params = set()

        def search_dict(d):
            for k, v in d.items():
                if isinstance(v, dict):
                    search_dict(v)
                elif isinstance(v, str) and v.startswith('$'):
                    params.add(v[1:])  # Remove the '$' prefix

        search_dict(job_config.get('properties', {}))
        return params

    @classmethod
    def _validate_graph_structure(cls, graph_def: dict, defined_jobs: set, graph_name: str) -> None:
        """
        Validate the structure of a job graph.
        - Checks for cycles
        - Ensures all referenced jobs exist
        - Validates head/tail nodes
        
        Args:
            graph_def: Graph definition from configuration
            defined_jobs: Set of all defined job names
            graph_name: Name of the graph being validated
            
        Raises:
            ValueError: If validation fails
        """
        # First validate all jobs exist and are properly connected
        for job, job_def in graph_def.items():
            if job not in defined_jobs:
                raise ValueError(f"Job '{job}' in graph '{graph_name}' is not defined")
            for next_job in job_def.get('next', []):
                if next_job not in defined_jobs:
                    raise ValueError(f"Job '{next_job}' referenced in 'next' field of job '{job}' in graph '{graph_name}' is not defined in jobs configuration")
                    
        # Build adjacency list after validating jobs
        adjacency = {job: job_def.get('next', []) for job, job_def in graph_def.items()}
        
        # Check for cycles using DFS
        visited = set()
        rec_stack = set()
        
        def has_cycle(node):
            visited.add(node)
            rec_stack.add(node)
            
            for neighbor in adjacency[node]:
                if neighbor not in visited:
                    if has_cycle(neighbor):
                        return True
                elif neighbor in rec_stack:
                    return True
                    
            rec_stack.remove(node)
            return False
            
        # Run cycle detection from each unvisited node
        for job in adjacency:
            if job not in visited:
                if has_cycle(job):
                    raise ValueError(f"Cycle detected in graph '{graph_name}'")
                    
    @classmethod
    def validate_configs(cls, configs: Dict[str, dict]) -> None:
        """
        Validate that:
        1. All jobs referenced in graphs exist in jobs configuration
        2. All parameterized jobs have corresponding parameter values
        3. Each graph structure is valid (no cycles, proper head/tail nodes, valid references)
        
        Args:
            configs: Dictionary containing all configurations
            
        Raises:
            ValueError: If validation fails
            ConfigurationError: If configuration is malformed (e.g. wrong types, missing required fields)
        """
        try:
            graphs_config = cls._extract_config_section(configs, 'graphs')
            jobs_config = cls._extract_config_section(configs, 'jobs')
            parameters_config = cls._extract_config_section(configs, 'parameters')
            config_files = configs.get('__files__', {})

            if not graphs_config or not jobs_config:
                return

            # First validate that all jobs in graphs exist
            defined_jobs = set(jobs_config.keys())

            # Track which config we're currently validating
            current_config = 'jobs'
            
            # Validate jobs config structure
            for job_name, job_config in jobs_config.items():
                if not isinstance(job_config, dict):
                    raise TypeError(f"Job '{job_name}' configuration must be a dictionary")
                
            current_config = 'graphs'
            for graph_name, graph_definition in graphs_config.items():
                # Validate graph structure (no cycles, etc)
                print(f"\nChecking {graph_name} for cycles...")
                cls._validate_graph_structure(graph_definition, defined_jobs, graph_name)
                print("No cycles detected")
                validate_graph(graph_definition, graph_name)

                # Find all parameterized jobs in this graph
                graph_parameterized_jobs = {}
                for job_name in graph_definition.keys():
                    job_config = jobs_config[job_name]
                    params = cls._find_parameterized_fields(job_config)
                    if params:
                        graph_parameterized_jobs[job_name] = params

                # If graph has parameterized jobs, it must have parameters
                if graph_parameterized_jobs:
                    current_config = 'parameters'
                    if graph_name not in parameters_config:
                        raise ValueError(
                            f"Graph '{graph_name}' contains parameterized jobs {list(graph_parameterized_jobs.keys())} but has no entry in parameters configuration")

                    parameters_for_graph = parameters_config[graph_name]

                    # Validate parameter groups
                    for param_name in parameters_for_graph.keys():
                        if not param_name.startswith('params'):
                            raise ValueError(
                                f"Invalid parameter group name '{param_name}' in graph '{graph_name}'. Parameter groups must start with 'params'")

                    # Validate that all parameters are filled for each group
                    for param_name, parameterized_jobs in parameters_for_graph.items():
                        for job_name, required_params in graph_parameterized_jobs.items():
                            if job_name not in parameterized_jobs:
                                raise ValueError(
                                    f"Job '{job_name}' in graph '{graph_name}' requires parameters {required_params} but has no entry in parameter group '{param_name}'")

                            # Each job should have a list of parameter sets
                            job_param_sets = parameterized_jobs[job_name]
                            if not isinstance(job_param_sets, list):
                                raise ValueError(
                                    f"Parameters for job '{job_name}' in graph '{graph_name}', group '{param_name}' should be a list of parameter sets")

                            # Validate each parameter set
                            for param_set in job_param_sets:
                                missing_params = required_params - set(param_set.keys())
                                if missing_params:
                                    raise ValueError(
                                        f"Parameter set for job '{job_name}' in graph '{graph_name}', group '{param_name}' is missing required parameters: {missing_params}")

                print(f"Graph {graph_name} passed all validations")

        except (AttributeError, TypeError, KeyError) as e:
            # Get the relevant file path based on which config we were validating
            error_file = config_files.get(current_config, 'unknown file')
            error_msg = "Configuration is malformed. Unable to proceed with job execution.\n\n" \
                        "-------------------------------------------------------\n" \
                        "          Configuration file malformed - cannot continue\n" \
                        "-------------------------------------------------------\n" \
                        f"File: {error_file}\n" \
                        f"Error details: {str(e)}\n" \
                        "-------------------------------------------------------"
            raise ConfigurationError(error_msg) from e

    @classmethod
    def load_all_configs(cls) -> Dict[str, dict]:
        """Load all configurations and validate them"""
        if cls._cached_configs is None:
            cls._cached_configs = cls.load_configs_from_dirs(cls.directories)
            cls.validate_configs(cls._cached_configs)
        return cls._cached_configs

    @classmethod
    def reload_configs(cls) -> Dict[str, dict]:
        """Force a reload of all configurations."""
        logger.info("Reloading configs...")
        cls._cached_configs = None
        return cls.load_all_configs()

    @classmethod
    def get_graphs_config(cls) -> dict:
        """
        Get graphs configuration from either dedicated graphs file or flow4ai_all.
        Returns empty dict if no configuration is found.
        """
        configs = cls.load_all_configs()
        return cls._extract_config_section(configs, 'graphs')

    @classmethod
    def get_jobs_config(cls) -> dict:
        """
        Get jobs configuration from either dedicated jobs file or flow4ai_all.
        Returns empty dict if no configuration is found.
        """
        configs = cls.load_all_configs()
        return cls._extract_config_section(configs, 'jobs')

    @classmethod
    def get_parameters_config(cls) -> dict:
        """
        Get parameters configuration from either dedicated parameters file or flow4ai_all.
        Returns empty dict if no configuration is found.
        """
        configs = cls.load_all_configs()
        return cls._extract_config_section(configs, 'parameters')

    @classmethod
    def is_parameterized_job(cls, raw_job_def):
        """
        Check if a job definition contains parameterized fields.
        
        Args:
            raw_job_def: Raw job definition from jobs.yaml
            
        Returns:
            bool: True if job has parameterized fields, False otherwise
        """
        if not isinstance(raw_job_def, dict):
            return False
            
        # Use existing method to find parameterized fields
        params = cls._find_parameterized_fields(raw_job_def)
        return len(params) > 0

    @classmethod
    def fill_job_with_parameters(cls, job_config: dict, graph_name: str, parameter_name: str) -> dict:
        """
        Fill a job configuration with parameters from parameters.yaml.
        
        Args:
            job_config: Raw job configuration from jobs.yaml
            graph_name: Name of the graph containing the job
            parameter_name: Name of the parameter group to use
            
        Returns:
            dict: Job configuration with parameters filled in
        """
        # Deep copy the job config to avoid modifying the original
        import copy
        filled_config = copy.deepcopy(job_config)
        
        # Get parameters for this job from parameters.yaml
        params_config = cls.get_parameters_config()
        if graph_name not in params_config or parameter_name not in params_config[graph_name]:
            raise ValueError(f"No parameters found for graph '{graph_name}' and parameter group '{parameter_name}'")
            
        # Get the job name by finding which job in the parameters matches this config
        job_name = None
        for job in params_config[graph_name][parameter_name].keys():
            if job_config == cls.get_jobs_config()[job]:
                job_name = job
                break
                
        if job_name is None:
            raise ValueError(f"Could not find job in parameters for graph '{graph_name}' and group '{parameter_name}'")
            
        # Get parameter values for this job
        param_sets = params_config[graph_name][parameter_name][job_name]
        if not param_sets or not isinstance(param_sets, list):
            raise ValueError(f"Invalid parameter sets for job '{job_name}' in graph '{graph_name}', group '{parameter_name}'")
            
        # Use the first parameter set (as defined in the spec)
        param_values = param_sets[0]
        
        def replace_params(obj, params):
            if isinstance(obj, dict):
                return {k: replace_params(v, params) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [replace_params(item, params) for item in obj]
            elif isinstance(obj, str) and obj.startswith('$'):
                param_name = obj[1:]  # Remove '$' prefix
                if param_name not in params:
                    raise ValueError(f"Parameter '{param_name}' not found in parameter set")
                return params[param_name]
            return obj
            
        # Replace all parameterized values in the config
        filled_config = replace_params(filled_config, param_values)
        return filled_config



================================================
FILE: src/flow4ai/jobs/__init__.py
================================================
from flow4ai.jobs.default_jobs import DefaultHeadJob, DefaultTailJob
from flow4ai.jobs.openai_jobs import OpenAIJob

__all__ = ['OpenAIJob', 'DefaultHeadJob', 'DefaultTailJob']



================================================
FILE: src/flow4ai/jobs/default_jobs.py
================================================
from typing import Any, Dict, Union

from flow4ai import f4a_logging as logging
from flow4ai.job import JobABC, Task

logger = logging.getLogger(__name__)


class DefaultHeadJob(JobABC):
    """A Job implementation that provides a simple default behavior."""
    
    async def run(self, task: Union[Dict[str, Any], Task]) -> Dict[str, Any]:
        """Run a simple job that logs and returns the task."""
        logger.info(f"Default head JOB for {task}")
        return {}

class DefaultTailJob(JobABC):
    """A Job implementation that provides a simple default behavior."""
    
    async def run(self, task: Union[Dict[str, Any], Task]) -> Dict[str, Any]:
        """Run a simple job that logs and returns the task."""
        logger.info(f"Default tail JOB for {task}")
        inputs_with_short_job_name = self.get_inputs()
        return inputs_with_short_job_name


================================================
FILE: src/flow4ai/jobs/openai_jobs.py
================================================
import os
from typing import Any, Dict, Optional, Union

from aiolimiter import AsyncLimiter
from openai import AsyncOpenAI

from flow4ai.f4a_logging import logging
from flow4ai.job import JobABC
from flow4ai.job_loader import JobFactory
from flow4ai.utils.api_utils import get_api_key
from flow4ai.utils.llm_utils import clean_prompt

logger = logging.getLogger("OpenAIJob")

class OpenAIClient:
    """
    Singleton class for AsyncOpenAI client.
    """
    _client = None

    @classmethod
    def get_client(cls, params: Dict[str, Any] = None):
        if cls._client is None:
            # Initialize params if None
            params = params or {}
            
            # Get API key using our utility function
            api_key = get_api_key(params, key_name='OPENAI_API_KEY')
            
            # Create client with remaining params
            cls._client = AsyncOpenAI(api_key=api_key, **params)
            logger.info(f"Created client with base_url: {params.get('base_url', 'default')}")
        return cls._client

class OpenAIJob(JobABC):

    # Shared AsyncLimiter for all jobs, default to 5,000 requests per minute
    default_rate_limit = {"max_rate": 5000, "time_period": 60}

    def __init__(self, name: Optional[str] = None, properties: Dict[str, Any] = {}):
        """
        Initialize an OpenAIJob instance with a properties dict containing three top-level keys, client, api, and rate_limit.
        All properties are optional.

        Args:
            name (Optional[str], optional): 
                A unique identifier for this job.
                The name must be unique among all jobs to ensure proper job identification 
                and dependency resolution. If not provided, a unique name will be auto-generated.

            properties (Dict[str, Any], optional): Optional properties for the job. A dictionary containing the following keys:

            {
                rate_limit: {
                    max_rate: Allow up to max_rate / time_period acquisitions before blocking.
                    time_period: duration of the time period in which to limit the rate. Note that up to max_rate acquisitions are allowed within this time period in a burst
                },
                client: {
                    api_key: str | None = None,
                    organization: str | None = None,
                    project: str | None = None,
                    base_url: str | URL | None = None,
                    websocket_base_url: str | URL | None = None,
                    timeout: float | Timeout | NotGiven | None = NOT_GIVEN,
                    max_retries: int = DEFAULT_MAX_RETRIES,
                    default_headers: Mapping[str, str] | None = None,
                    default_query: Mapping[str, object] | None = None,
                    http_client: AsyncClient | None = None,
                    _strict_response_validation: bool = False
                },
                api: {
                    messages: Iterable[ChatCompletionMessageParam],
                    model: ChatModel | str,
                    audio: ChatCompletionAudioParam | NotGiven | None = NOT_GIVEN,
                    frequency_penalty: float | NotGiven | None = NOT_GIVEN,
                    function_call: FunctionCall | NotGiven = NOT_GIVEN,
                    functions: Iterable[Function] | NotGiven = NOT_GIVEN,
                    logit_bias: Dict[str, int] | NotGiven | None = NOT_GIVEN,
                    logprobs: bool | NotGiven | None = NOT_GIVEN,
                    max_completion_tokens: int | NotGiven | None = NOT_GIVEN,
                    max_tokens: int | NotGiven | None = NOT_GIVEN,
                    metadata: Dict[str, str] | NotGiven | None = NOT_GIVEN,
                    modalities: List[ChatCompletionModality] | NotGiven | None = NOT_GIVEN,
                    n: int | NotGiven | None = NOT_GIVEN,
                    parallel_tool_calls: bool | NotGiven = NOT_GIVEN,
                    prediction: ChatCompletionPredictionContentParam | NotGiven | None = NOT_GIVEN,
                    presence_penalty: float | NotGiven | None = NOT_GIVEN,
                    reasoning_effort: ChatCompletionReasoningEffort | NotGiven = NOT_GIVEN,
                    response_format: ResponseFormat | NotGiven | None = NOT_GIVEN,
                    seed: int | NotGiven | None = NOT_GIVEN,
                    service_tier: NotGiven | Literal['auto', 'default'] | None = NOT_GIVEN,
                    stop: str | List[str] | NotGiven | None = NOT_GIVEN,
                    store: bool | NotGiven | None = NOT_GIVEN,
                    stream: NotGiven | Literal[False] | None = NOT_GIVEN,
                    stream_options: ChatCompletionStreamOptionsParam | NotGiven | None = NOT_GIVEN,
                    temperature: float | NotGiven | None = NOT_GIVEN,
                    tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven = NOT_GIVEN,
                    tools: Iterable[ChatCompletionToolParam] | NotGiven = NOT_GIVEN,
                    top_logprobs: int | NotGiven | None = NOT_GIVEN,
                    top_p: float | NotGiven | None = NOT_GIVEN,
                    user: str | NotGiven = NOT_GIVEN,
                    extra_headers: Headers | None = None,
                    extra_query: Query | None = None,
                    extra_body: Body | None = None,
                    timeout: float | Timeout | NotGiven | None = NOT_GIVEN
                }
            }
        """
        super().__init__(name, properties)
        
        # Initialize OpenAI client with properties
        self.client = OpenAIClient.get_client(self.properties.get("client", {}))
        
        # Rate limiter configuration
        rate_limit_config = self.properties.get("rate_limit", self.default_rate_limit)
        self.limiter = AsyncLimiter(**rate_limit_config)

        # Extract other relevant properties for OpenAI client
        self.api_properties = self.properties.get("api", {})

    async def run(self, task: Union[Dict[str, Any], Any]) -> Dict[str, Any]:
        """
        Perform an OpenAI API call while adhering to rate limits.
        
        Args:
            task: A dictionary containing either:
                - prompt: str - The prompt to send to the model
                - messages: list - Direct message format for the API
                Or any other valid parameters for the chat.completions.create API
        """
        # Start with default properties
        request_properties = {
            "model": "gpt-4o",
            "temperature": 0.7,
            "messages": [
                {"role": "system", "content": "You are a helpful assistant"},
                {"role": "user", "content": "You are a helpful assistant."}
            ]
        }
        
        # Add API properties from initialization
        request_properties.update(self.api_properties)

        # Check if response_format is a string and replace with the Pydantic class
        if "response_format" in request_properties and isinstance(request_properties["response_format"], str):
            try:
                response_format_name = request_properties["response_format"]
                request_properties["response_format"] = JobFactory.get_pydantic_class(response_format_name)
                logger.info(f"Successfully replaced response_format string with Pydantic class: {response_format_name}")
            except ValueError as e:
                logger.error(f"Could not find Pydantic class for response_format: {request_properties['response_format']}. Error: {e}")
            except Exception as e:
                logger.error(f"An unexpected error occurred while trying to get the Pydantic class: {e}")

        self.create_prompt(request_properties, task)

        # Acquire the rate limiter before making the request
        async with self.limiter:
            try:
                logger.info(f"{self.name} is making an OpenAI API call.")
                if "response_format" in request_properties:
                    response = await self.client.beta.chat.completions.parse(**request_properties)
                else:
                    response = await self.client.chat.completions.create(**request_properties)
                logger.info(f"{self.name} received a response.")
                
                # Handle the response
                if hasattr(response, 'choices') and response.choices:
                    if "response_format" in request_properties:
                        return response.choices[0].message.parsed
                    else:
                        return {"response": response.choices[0].message.content}
                else:
                    return {"error": "No valid response content found"}
            except Exception as e:
                logger.error(f"Error in {self.name}: {e}")
                return {"error": str(e)}

    def create_prompt(self, request_properties, task):
        # Handle the task input
        if isinstance(task, dict):
            # If task has a prompt, convert it to messages format
            if "prompt" in task:
                prompt = clean_prompt(task["prompt"])
                request_properties["messages"] = [
                    {"role": "system", "content": "You are a helpful assistant"},
                    {"role": "user", "content": prompt}
                ]
            # If task already has messages, use those
            elif "messages" in task:
                request_properties["messages"] = task["messages"]

            # Add any other valid API parameters from task
            # request_properties.update({k: v for k, v in task.items() if k not in ["prompt", "messages"]})
        elif task:  # If task is not empty and not a dict
            # If task is not a dict, treat it as the prompt
            prompt = clean_prompt(str(task))
            request_properties["messages"] = [
                {"role": "system", "content": "You are a helpful assistant"},
                {"role": "user", "content": prompt}
            ]


================================================
FILE: src/flow4ai/jobs/wrapping_job.py
================================================
import inspect
from typing import Any, Callable, Dict, List, Union

from flow4ai import JobABC
from flow4ai.job import Task


class WrappingJob(JobABC):
    FN_CONTEXT='j_ctx'

    def __init__(
        self,
        callable_obj: Callable,
        name: str = None
    ):
        """
        Initialize a wrapper for a callable object.

        Args:
            callable_obj: The function or method to wrap
            name: Identifier for this callable in parameter dictionaries
        Raises:
            TypeError: If callable_obj is not actually callable
        """

        is_callable = callable(callable_obj)
        self.is_callable = is_callable
        if not is_callable: #and not isinstance(callable_obj, (JobABC, Parallel, Serial))
            raise TypeError(f"WrappingJob will only wrap a callable, error due to {type(callable_obj).__name__}")
        self.callable = callable_obj
        super().__init__(name)
        self.default_args = []
        self.default_kwargs = {}

    async def run(self, task: Union[Dict[str, Any], Task]) -> Dict[str, Any]:
        """
        Execute the wrapped callable with parameters from the params dictionary.

        Args:
            task: legacy parameter dictionary

        Returns:
            The result of the callable execution
        """
        if not self.is_callable:
            raise ValueError(f"Callable '{self.callable}' is not callable")

        params = task if task else self.get_task()  # if calling run() directly in tests use get_task(), "if task" is falsey so fails on {}

        # Process shorthand dot notation params (e.g., "job.param": value)
        params = self._process_shorthand_params(params)

        # Check if the callable requires parameters
        sig = inspect.signature(self.callable)
        requires_params = bool(sig.parameters)
        
        # Check if the only parameter required is 'context' which is auto-provided
        requires_non_context_params = False
        if requires_params:
            non_context_params = [param for param in sig.parameters if param != self.FN_CONTEXT]
            requires_non_context_params = bool(non_context_params)

        parsed_name = JobABC.parse_job_name(self.name)
        short_name = self.name if parsed_name == "UNSUPPORTED NAME FORMAT" else parsed_name
        # Only check for parameters if the callable requires non-context parameters
        if requires_non_context_params and short_name not in params:
            raise ValueError(f"No parameters found for callable '{short_name}'")  

        # If no parameters are required, use empty args and kwargs
        if not requires_params or short_name not in params:
            callable_params = {"args": [], "kwargs": {}}
        else:
            callable_params = self._create_callable_params(params[short_name])

        # Add context to the kwargs if the callable accepts it
        if self.FN_CONTEXT in sig.parameters:
            callable_params["kwargs"][self.FN_CONTEXT] = {}
            callable_params["kwargs"][self.FN_CONTEXT]["global"] = self.global_ctx
            callable_params["kwargs"][self.FN_CONTEXT]["task"] = params
            callable_params["kwargs"][self.FN_CONTEXT]["inputs"] = self.get_inputs()

        # Validate parameters against the callable's signature
        self._validate_params(callable_params["args"], callable_params["kwargs"])

        # Apply type conversions based on callable's signature
        args, kwargs = self._convert_param_types(
            callable_params["args"],
            callable_params["kwargs"]
        )

        return await self._execute_callable(args, kwargs)

    def _process_shorthand_params(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process shorthand dot notation params (e.g., "job.param": value) and
        convert them to the standard nested format.
        
        Args:
            params: Dictionary of parameters that may contain shorthand notation
            
        Returns:
            Updated parameters dictionary with shorthand notation expanded
        """
        result = params.copy()
        dot_params = {}
        
        # Find and collect all shorthand dot notation parameters
        for key, value in params.items():
            if '.' in key and not key.startswith('fn.'):
                job_name, param_name = key.split('.', 1)
                if job_name not in dot_params:
                    dot_params[job_name] = {}
                dot_params[job_name][param_name] = value
                del result[key]  # Remove the dot notation key from the result
                
        # Merge the dot params with the existing job params
        for job_name, job_params in dot_params.items():
            if job_name in result:
                # If the job already exists in the params, merge the parameters
                result[job_name].update(job_params)
            else:
                # Otherwise, create a new entry
                result[job_name] = job_params
                
        return result
        
    def _create_callable_params(self, params: Dict[str, Any]) -> Dict[str, List[Any]]:
        """
        Extract and normalize parameters for the callable, supporting multiple styles.

        Args:
            params: Dictionary of parameters specific to this callable

        Returns:
            Dictionary containing 'args' and 'kwargs' keys
        """
        # Start with default values
        args = self.default_args.copy()
        kwargs = self.default_kwargs.copy()

        # Case 1: Explicit args list via 'fn.args' (legacy) or 'args' (new)
        if "fn.args" in params:
            args = params["fn.args"]
        elif "args" in params:
            args = params["args"]

        # Case 2: Explicit kwargs dictionary via 'fn.kwargs' (legacy) or 'kwargs' (new)
        if "fn.kwargs" in params:
            kwargs.update(params["fn.kwargs"])
        elif "kwargs" in params:
            kwargs.update(params["kwargs"])

        # Case 3: Named parameters with fn. prefix (legacy)
        for key, value in params.items():
            if key.startswith("fn.") and key not in ["fn.args", "fn.kwargs"]:
                param_name = key[3:]  # Remove "fn." prefix
                kwargs[param_name] = value
            # Case 4: Direct parameter passing (new)
            elif key not in ["args", "kwargs"]:
                kwargs[key] = value

        return {"args": args, "kwargs": kwargs}

    def _validate_params(self, args: List[Any], kwargs: Dict[str, Any]) -> None:
        """
        Validate that the provided parameters match the callable's signature.

        Args:
            args: Positional arguments to validate
            kwargs: Keyword arguments to validate

        Raises:
            ValueError: If parameters don't match the callable's signature
        """
        sig = inspect.signature(self.callable)
        try:
            sig.bind(*args, **kwargs)
        except TypeError as e:
            raise ValueError(f"Invalid parameters for {self.name}: {e}")

    def _convert_param_types(self, args: List[Any], kwargs: Dict[str, Any]) -> tuple:
        """
        Convert parameter types based on the callable's type annotations.

        Args:
            args: Positional arguments to convert
            kwargs: Keyword arguments to convert

        Returns:
            Tuple containing converted args and kwargs
        """
        sig = inspect.signature(self.callable)

        # Convert positional args
        converted_args = []
        for i, arg in enumerate(args):
            # Skip conversion if we have more args than parameters
            if i >= len(sig.parameters):
                converted_args.append(arg)
                continue

            param_name = list(sig.parameters.keys())[i]
            param = sig.parameters[param_name]

            # Skip if no annotation or if annotation is not a type
            if param.annotation == inspect.Parameter.empty or not isinstance(param.annotation, type):
                converted_args.append(arg)
                continue

            try:
                # Only convert if needed and possible
                if not isinstance(arg, param.annotation) and arg is not None:
                    converted_args.append(param.annotation(arg))
                else:
                    converted_args.append(arg)
            except (ValueError, TypeError):
                # If conversion fails, use original value
                converted_args.append(arg)

        # Convert kwargs
        converted_kwargs = {}
        for name, value in kwargs.items():
            if (name in sig.parameters and
                sig.parameters[name].annotation != inspect.Parameter.empty and
                isinstance(sig.parameters[name].annotation, type)):

                try:
                    # Only convert if needed and possible
                    if not isinstance(value, sig.parameters[name].annotation) and value is not None:
                        converted_kwargs[name] = sig.parameters[name].annotation(value)
                    else:
                        converted_kwargs[name] = value
                except (ValueError, TypeError):
                    # If conversion fails, use original value
                    converted_kwargs[name] = value
            else:
                converted_kwargs[name] = value

        return converted_args, converted_kwargs

    async def _execute_callable(self, args: List[Any], kwargs: Dict[str, Any]) -> Any:
        """
        Execute the callable with the given parameters.

        Args:
            args: Positional arguments to pass
            kwargs: Keyword arguments to pass

        Returns:
            Result of the callable execution
        """
        result = self.callable(*args, **kwargs)

        # Check if the result is a coroutine (from an async function)
        if inspect.iscoroutine(result):
            # Await the coroutine to get the actual result
            return await result

        return result



================================================
FILE: src/flow4ai/resources/__init__.py
================================================



================================================
FILE: src/flow4ai/resources/otel_config.yaml
================================================
exporter: file  # Default exporter is file; can be overridden by OTEL_TRACES_EXPORTER env variable.
service_name: MyService  # Can be overridden by OTEL_SERVICE_NAME env variable.
batch_processor:
  max_queue_size: 1000  # Batch processor will handle up to 1000 spans in queue.
  schedule_delay_millis: 1000  # 1-second timeout for exporting spans.
file_exporter:
  path: "~/.Flow4AI/otel_trace.json"  # Default path for trace export
  max_size_bytes: 5242880  # 5MB (5 * 1024 * 1024 bytes)
  rotation_time_days: 1  # Rotate daily



================================================
FILE: src/flow4ai/utils/__init__.py
================================================
# Make utils a Python package
from .otel_wrapper import TracerFactory, trace_function
from .timing import timing_decorator

__all__ = ['TracerFactory', 'trace_function', 'timing_decorator']



================================================
FILE: src/flow4ai/utils/api_utils.py
================================================
"""
Utility functions for handling API keys and authentication.
"""
import os
from typing import Any, Dict, Optional

from dotenv import load_dotenv

from flow4ai import f4a_logging as logging

logger = logging.getLogger(__name__)


def get_api_key(params: Optional[Dict[str, Any]] = None, 
               env_file: str = "api.env",
               key_name: str = None,
               required: bool = True) -> Optional[str]:
    """
    Resolves and returns an API key from parameters or environment variables.
    
    Args:
        params: Dictionary of parameters that may contain an 'api_key' entry
        env_file: Path to the .env file to load (defaults to "api.env")
        key_name: Default environment variable name to use if not specified in params
        required: Whether to raise an error if the API key is not found
        
    Returns:
        The API key string or None if not required and not found
        
    Raises:
        ValueError: If required is True and the API key is not found
    """
    # Initialize params if None
    params = params or {}
    
    # Load environment variables from env file
    load_dotenv(env_file)
    
    # Extract key name from params or use default
    env_var = params.pop("api_key", None) if params else key_name
    
    # Resolve API key: either from the env var specified in params or from default env var
    api_key = os.getenv(env_var)
    logger.info(f"Resolved API Key exists: {bool(api_key)}")
    
    # Check if the API key is not set and raise an error if required
    if not api_key and required:
        raise ValueError("API key is not set. Please provide an API key.")
        
    return api_key





================================================
FILE: src/flow4ai/utils/llm_utils.py
================================================
import re
import string

from flow4ai import f4a_logging as logging

logger = logging.getLogger(__name__)

def clean_prompt(text):
    # Keep only printable characters
    return ''.join(char for char in text if char in string.printable)


def clean_prompt(text):
    if not isinstance(text, str):
        logger.error("Input must be a string")
        raise ValueError("Input must be a string")
    
    # Remove control characters but keep normal whitespace
    cleaned_1 = ''.join(char for char in text if ord(char) >= 32 or char in '\n\r\t')
    cleaned = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F]', '', cleaned_1)
    # Optional: Check if the text was modified
    if cleaned != text:
        logger.info("Characters were cleaned from the prompt")
    
    # Optional: Ensure the text isn't empty after cleaning
    if not cleaned.strip():
        logger.error("Prompt is empty after cleaning")
        raise ValueError("Prompt is empty after cleaning")
    
    return cleaned

def check_response_errors(response:dict):
    if response.get("error"):
        logger.error(f"Response has an error: {response}")
        raise ValueError("Response has an error")
    elif response.get("status"):
        status = response.get("status")
        if status == "error":
            logger.error(f"Response has an error: {response}")
            raise ValueError("Response has an error")



================================================
FILE: src/flow4ai/utils/monitor_utils.py
================================================
"""Utilities for monitoring and logging task progress."""
import asyncio

NO_CHANGE_LOG_INTERVAL = 1.0

def should_log_task_stats(monitor_fn, tasks_created: int, tasks_completed: int) -> bool:
    """Check if task stats should be logged based on changes or time elapsed.
    
    Args:
        monitor_fn: The monitoring function to store state on
        tasks_created: Current count of created tasks
        tasks_completed: Current count of completed tasks
        
    Returns:
        bool: True if stats should be logged
    """
    if not hasattr(monitor_fn, '_last_log_time'):
        monitor_fn._last_log_time = 0
        monitor_fn._last_tasks_created = -1
        monitor_fn._last_tasks_completed = -1
    
    current_time = asyncio.get_event_loop().time()
    counts_changed = (tasks_created != monitor_fn._last_tasks_created or 
                     tasks_completed != monitor_fn._last_tasks_completed)
    
    should_log = counts_changed or (current_time - monitor_fn._last_log_time) >= NO_CHANGE_LOG_INTERVAL
    
    if should_log:
        monitor_fn._last_log_time = current_time
        monitor_fn._last_tasks_created = tasks_created
        monitor_fn._last_tasks_completed = tasks_completed
        
    return should_log



================================================
FILE: src/flow4ai/utils/otel_wrapper.py
================================================
import inspect
import json
import os
from functools import wraps
from importlib import resources
from threading import Lock
from typing import Any, Dict, Optional, Sequence

import yaml
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import \
    OTLPSpanExporter
from opentelemetry.sdk.trace import ReadableSpan, TracerProvider
from opentelemetry.sdk.trace.export import (BatchSpanProcessor,
                                            ConsoleSpanExporter, SpanExporter,
                                            SpanExportResult)

# Explicitly define exports
__all__ = ['TracerFactory', 'trace_function', 'AsyncFileExporter']
DEFAULT_OTEL_CONFIG = "otel_config.yaml"

class AsyncFileExporter(SpanExporter):
    """Asynchronous file exporter for OpenTelemetry spans with log rotation support."""
    
    def __init__(self, filepath: str, max_size_bytes: int = None, rotation_time_days: int = None):
        """Initialize the exporter with the target file path and rotation settings.
        
        Args:
            filepath: Path to the file where spans will be exported
            max_size_bytes: Maximum file size in bytes before rotation
            rotation_time_days: Number of days before rotating file
        """
        self.filepath = os.path.expanduser(filepath)
        self.max_size_bytes = max_size_bytes
        self.rotation_time_days = rotation_time_days
        self.last_rotation_time = None
        
        # Ensure directory exists
        os.makedirs(os.path.dirname(self.filepath), exist_ok=True)
        self._export_lock = Lock()
        
        # Initialize file with empty array if it doesn't exist
        if not os.path.exists(self.filepath):
            with open(self.filepath, 'w') as f:
                json.dump([], f)
                
        # Record initial rotation time
        if self.rotation_time_days:
            self.last_rotation_time = os.path.getmtime(self.filepath)
    
    def _should_rotate(self, additional_size: int = 0) -> bool:
        """Check if file should be rotated based on size or time.
        
        Args:
            additional_size: Additional size in bytes that will be added
        """
        if not os.path.exists(self.filepath):
            return False
            
        should_rotate = False
        
        # Check size-based rotation
        if self.max_size_bytes:
            current_size = os.path.getsize(self.filepath)
            if (current_size + additional_size) >= self.max_size_bytes:
                should_rotate = True
                
        # Check time-based rotation
        if self.rotation_time_days and self.last_rotation_time:
            current_time = os.path.getmtime(self.filepath)
            days_elapsed = (current_time - self.last_rotation_time) / (24 * 3600)
            if days_elapsed >= self.rotation_time_days:
                should_rotate = True
                
        return should_rotate
    
    def _rotate_file(self):
        """Rotate the current file if it exists."""
        if not os.path.exists(self.filepath):
            return
            
        # Generate rotation suffix based on timestamp
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        rotated_path = f"{self.filepath}.{timestamp}"
        
        # Rotate the file
        os.rename(self.filepath, rotated_path)
        
        # Create new empty file
        with open(self.filepath, 'w') as f:
            json.dump([], f)
            
        # Update rotation time
        self.last_rotation_time = os.path.getmtime(self.filepath)
    
    def _serialize_span(self, span: ReadableSpan) -> dict:
        """Convert a span to a JSON-serializable dictionary.
        
        Args:
            span: The span to serialize
        Returns:
            dict: JSON-serializable representation of the span
        """
        return {
            'name': span.name,
            'context': {
                'trace_id': format(span.context.trace_id, '032x'),
                'span_id': format(span.context.span_id, '016x'),
            },
            'parent_id': format(span.parent.span_id, '016x') if span.parent else None,
            'start_time': span.start_time,
            'end_time': span.end_time,
            'attributes': dict(span.attributes),
            'events': [
                {
                    'name': event.name,
                    'timestamp': event.timestamp,
                    'attributes': dict(event.attributes)
                }
                for event in span.events
            ],
            'status': {
                'status_code': str(span.status.status_code),
                'description': span.status.description
            }
        }

    def export(self, spans: Sequence[ReadableSpan]) -> SpanExportResult:
        """Export spans to file with rotation support.
        
        Args:
            spans: Sequence of spans to export
        Returns:
            SpanExportResult indicating success or failure
        """
        try:
            with self._export_lock:
                # Create serializable span data
                span_data = [self._serialize_span(span) for span in spans]
                
                # Read existing spans
                try:
                    with open(self.filepath, 'r') as f:
                        try:
                            existing_spans = json.load(f)
                        except json.JSONDecodeError:
                            existing_spans = []
                except FileNotFoundError:
                    existing_spans = []
                
                # Calculate size of new data
                new_data = existing_spans + span_data
                new_data_str = json.dumps(new_data, indent=2)
                additional_size = len(new_data_str.encode('utf-8'))
                
                # Check rotation after calculating new size
                if self._should_rotate(additional_size - os.path.getsize(self.filepath) if os.path.exists(self.filepath) else 0):
                    self._rotate_file()
                    existing_spans = []
                
                # Append new spans
                existing_spans.extend(span_data)
                
                # Write all spans back to file
                temp_file = f"{self.filepath}.tmp"
                try:
                    with open(temp_file, 'w') as f:
                        json.dump(existing_spans, f, indent=2)
                    # Atomic replace
                    os.replace(temp_file, self.filepath)
                finally:
                    if os.path.exists(temp_file):
                        os.unlink(temp_file)
                
            return SpanExportResult.SUCCESS
        except Exception as e:
            print(f"Error exporting spans to file: {e}")
            return SpanExportResult.FAILURE

    def shutdown(self) -> None:
        """Shutdown the exporter."""
        pass

class TestTracerProvider(TracerProvider):
    _instance = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(TestTracerProvider, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if not hasattr(self, '_initialized'):
            super().__init__()
            self._initialized = True
            
    def get_tracer(
        self,
        instrumenting_module_name: str,
        instrumenting_library_version: str = None,
        schema_url: str = None,
        attributes: dict = None,
    ) -> trace.Tracer:
        """Get a tracer for use in tests.
        
        Args:
            instrumenting_module_name: The name of the instrumenting module
            instrumenting_library_version: Optional version of the instrumenting module
            schema_url: Optional URL of the OpenTelemetry schema
            attributes: Optional attributes to add to the tracer
            
        Returns:
            A tracer instance for use in tests
        """
        return super().get_tracer(
            instrumenting_module_name,
            instrumenting_library_version,
            schema_url,
            attributes,
        )

# Singleton TracerFactory
class TracerFactory:
    _instance = None
    _config = None
    _lock = Lock()
    _is_test_mode = False
    
    @classmethod
    def set_test_mode(cls, enabled: bool = True):
        """Enable or disable test mode.
        
        Args:
            enabled: Whether to enable test mode
        """
        cls._is_test_mode = enabled
        cls._instance = None  # Reset instance to force recreation with new provider
    
    @classmethod
    def _load_config(cls, yaml_file=None):
        """Load configuration from YAML file.
        
        Args:
            yaml_file: Optional path override for the YAML configuration file
        Returns:
            dict: Configuration dictionary
        """
       # First try yaml_file parameter
        config_path = yaml_file
        if not config_path:
            # Then try environment variable
            config_path = os.environ.get('FLOW4AI_OT_CONFIG', "")
        
        if not config_path:
            # Finally use default path from package resources
            try:
                # Using files() instead of deprecated path() method
                config_path = str(resources.files('flow4ai.resources').joinpath(DEFAULT_OTEL_CONFIG))
            except Exception as e:
                raise RuntimeError(f"Could not find {DEFAULT_OTEL_CONFIG} in package resources: {e}")
        
        with open(config_path, 'r') as file:
                cls._config = yaml.safe_load(file)
        return cls._config
    
    @classmethod
    def get_tracer(cls, config=None):
        """Get or create the tracer instance.
        
        Args:
            config: Optional configuration override. If not provided, loads from file.
        Returns:
            Tracer instance
        """
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    # Use provided config or load from file
                    cfg = config if config is not None else cls._load_config()
                    
                    # Use TestTracerProvider in test mode
                    provider = TestTracerProvider() if cls._is_test_mode else TracerProvider()
                    
                    # Configure main exporter
                    main_exporter = cls._configure_exporter(cfg['exporter'])
                    batch_processor = BatchSpanProcessor(
                        main_exporter,
                        max_queue_size=cfg['batch_processor']['max_queue_size'],
                        schedule_delay_millis=cfg['batch_processor']['schedule_delay_millis']
                    )
                    provider.add_span_processor(batch_processor)
                    
                    trace.set_tracer_provider(provider)
                    cls._instance = trace.get_tracer(cfg["service_name"])
        return cls._instance

    @staticmethod
    def _configure_exporter(exporter_type):
        """Configure the appropriate exporter based on type.
        
        Args:
            exporter_type: Type of exporter to configure
        Returns:
            Configured exporter instance
        """
        if exporter_type == "otlp":
            return OTLPSpanExporter()  # OTEL_EXPORTER_OTLP_... environment variables apply here
        elif exporter_type == "console":
            return ConsoleSpanExporter()  # OTEL_EXPORTER_CONSOLE_... environment variables apply here
        elif exporter_type == "file":
            # Load config to get file path
            config = TracerFactory._load_config()
            file_path = config.get('file_exporter', {}).get('path', "~/.Flow4AI/otel_trace.json")
            max_size_bytes = config.get('file_exporter', {}).get('max_size_bytes')
            rotation_time_days = config.get('file_exporter', {}).get('rotation_time_days')
            return AsyncFileExporter(file_path, max_size_bytes, rotation_time_days)
        else:
            raise ValueError("Unsupported exporter type")

    @classmethod
    def trace(cls, message: str, detailed_trace: bool = False, attributes: Optional[Dict[str, Any]] = None):
        """Trace a message with OpenTelemetry tracing.
        
        Args:
            message: The message to trace
            detailed_trace: Whether to include detailed tracing information (args, kwargs, object fields)
            attributes: Optional dictionary of additional attributes to add to the span
        """
        tracer = cls.get_tracer()
        
        # Get the calling frame
        frame = inspect.currentframe()
        if frame:
            caller_frame = frame.f_back
            if caller_frame:
                # Get function info
                func_name = caller_frame.f_code.co_name
                module_name = inspect.getmodule(caller_frame).__name__ if inspect.getmodule(caller_frame) else "__main__"
                
                # Get local variables including 'self' if it exists
                local_vars = caller_frame.f_locals
                args = []
                kwargs = {}
                
                # If this is a method call (has 'self')
                if 'self' in local_vars:
                    args.append(local_vars['self'])
                    # Add other arguments if they exist
                    if len(local_vars) > 1:
                        # Filter out 'self' and get remaining arguments
                        args.extend([v for k, v in local_vars.items() if k != 'self'])
                
                span_name = f"{module_name}.{func_name}"
                with tracer.start_as_current_span(span_name) as span:
                    span.set_attribute("trace.message", message)
                    if detailed_trace:
                        span.set_attribute("function.args", str(tuple(args)))
                        span.set_attribute("function.kwargs", str(kwargs))
                        if args and hasattr(args[0], "__dict__"):
                            span.set_attribute("object.fields", str(vars(args[0])))
                    if attributes:
                        for key, value in attributes.items():
                            span.set_attribute(key, str(value))
                    print(message)
                
                # Clean up
                del frame
                del caller_frame
                return
        
        # Fallback if not in a function context
        with tracer.start_as_current_span("trace_message") as span:
            span.set_attribute("trace.message", message)
            if attributes:
                for key, value in attributes.items():
                    span.set_attribute(key, str(value))
            print(message)

# Decorator for OpenTelemetry tracing
def trace_function(func=None, *, detailed_trace: bool = False, attributes: Optional[Dict[str, Any]] = None):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            tracer = TracerFactory.get_tracer()
            span_name = f"{func.__module__}.{func.__name__}"
            with tracer.start_as_current_span(span_name) as span:
                # Record function arguments only if detailed_trace is True
                if detailed_trace:
                    span.set_attribute("function.args", str(args))
                    span.set_attribute("function.kwargs", str(kwargs))
                    if args and hasattr(args[0], "__dict__"):
                        span.set_attribute("object.fields", str(vars(args[0])))
                if attributes:
                    for key, value in attributes.items():
                        span.set_attribute(key, str(value))
                try:
                    result = func(*args, **kwargs)
                    return result
                except Exception as e:
                    span.record_exception(e)
                    raise
        return wrapper
    
    if func is None:
        return decorator
    return decorator(func)



================================================
FILE: src/flow4ai/utils/print_utils.py
================================================
from .. import f4a_logging as logging


def printh(text):
    """
    Log the given text surrounded by asterisks.
    """
    logger = logging.getLogger('PrintUtils')
    logger.info("*** " + text + " ***")



================================================
FILE: src/flow4ai/utils/timing.py
================================================
import time
from functools import wraps


def timing_decorator(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.perf_counter()
        result = func(*args, **kwargs)
        end_time = time.perf_counter()
        elapsed_time = end_time - start_time
        print(f"Elapsed time: {elapsed_time:.6f} seconds")
        return result
    return wrapper


