Directory structure:
└── dav-rob-flow4ai/
    ├── src/
    │   ├── __init__.py
    │   └── flow4ai/
    │       ├── __init__.py
    │       ├── dsl.py
    │       ├── dsl_graph.py
    │       ├── f4a_graph.py
    │       ├── f4a_logging.py
    │       ├── flowmanager.py
    │       ├── flowmanager_base.py
    │       ├── flowmanagerMP.py
    │       ├── graph_pic.py
    │       ├── job.py
    │       ├── job_loader.py
    │       ├── jobs/
    │       │   ├── __init__.py
    │       │   ├── default_jobs.py
    │       │   ├── openai_jobs.py
    │       │   └── wrapping_job.py
    │       ├── resources/
    │       │   ├── __init__.py
    │       │   └── otel_config.yaml
    │       └── utils/
    │           ├── __init__.py
    │           ├── api_utils.py
    │           ├── llm_utils.py
    │           ├── monitor_utils.py
    │           ├── otel_wrapper.py
    │           ├── print_utils.py
    │           └── timing.py
    └── tests/
        ├── README.md
        ├── conftest.py
        ├── test_aa.py
        ├── test_concurrency.py
        ├── test_dsl.py
        ├── test_dsl_graph.py
        ├── test_error_conditions.py
        ├── test_flowmanager.py
        ├── test_flowmanager_details.py
        ├── test_flowmanager_init_params.py
        ├── test_fmmp_async_functionality.py
        ├── test_fmmp_factory.py
        ├── test_fmmp_parallel_execution.py
        ├── test_fmmp_parallel_load.py
        ├── test_fmmp_queue_stress.py
        ├── test_fmmp_result_processing.py
        ├── test_fqname_collision.py
        ├── test_graph_config_parsing.py
        ├── test_job_graph.py
        ├── test_job_loading.py
        ├── test_job_name_parsing.py
        ├── test_job_tracing.py
        ├── test_jobs.py
        ├── test_logging_config.py
        ├── test_opentelemetry.py
        ├── test_task_passthrough.py
        ├── test_zzz_final_cleanup.py
        ├── test_configs/
        │   ├── test_concurrency_by_returns/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   └── jobs/
        │   │       └── concurrent_jobs.py
        │   ├── test_jc_config/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   ├── parameters.yaml
        │   │   └── jobs/
        │   │       ├── mock_jobs.py
        │   │       └── mock_jobs2.py
        │   ├── test_jc_config_all/
        │   │   ├── flow4ai_all.yaml
        │   │   └── jobs/
        │   │       ├── mock_jobs.py
        │   │       └── mock_jobs2.py
        │   ├── test_jc_config_invalid/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   └── parameters.yaml
        │   ├── test_jc_config_invalid_parameters/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   └── parameters.yaml
        │   ├── test_malformed_config/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   └── parameters.yaml
        │   ├── test_malformed_config_jobs/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   └── parameters.yaml
        │   ├── test_malformed_config_params/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   └── parameters.yaml
        │   ├── test_multiple_heads/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   ├── parameters.yaml
        │   │   └── jobs/
        │   │       └── mock_jobs.py
        │   ├── test_multiple_tails/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   └── jobs/
        │   │       └── mock_jobs.py
        │   ├── test_multiple_tails2/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   ├── parameters.yaml
        │   │   └── jobs/
        │   │       └── mock_jobs.py
        │   ├── test_pydantic_config/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   ├── parameters.yaml
        │   │   └── jobs/
        │   │       ├── mock_jobs.py
        │   │       ├── mock_jobs2.py
        │   │       └── pydantic.py
        │   ├── test_save_result/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   ├── parameters.yaml
        │   │   └── jobs/
        │   │       ├── mock_jobs.py
        │   │       └── mock_jobs2.py
        │   ├── test_simple_parallel/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   └── jobs/
        │   │       └── mock_jobs.py
        │   ├── test_single_job/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   └── parameters.yaml
        │   └── test_task_passthrough/
        │       ├── graphs.yaml
        │       ├── jobs.yaml
        │       └── jobs/
        │           └── text_processing_jobs.py
        └── test_utils/
            ├── __init__.py
            ├── graph_evaluation.py
            └── simple_job.py

================================================
FILE: src/__init__.py
================================================



================================================
FILE: src/flow4ai/__init__.py
================================================
"""
Flow4AI - A scalable AI job scheduling and execution platform
"""

# No convenience imports - modules should be imported directly



================================================
FILE: src/flow4ai/dsl.py
================================================
from functools import reduce
from typing import Any, Dict, List, Union

from . import f4a_logging as logging
from .job import JobABC
from .jobs.wrapping_job import WrappingJob

logger = logging.getLogger(__name__)

# Type definitions for DSL components
DSLComponent = Union[JobABC, 'Parallel', 'Serial']
JobsDict = Dict[str, JobABC]

class Parallel:
    def __init__(self, *components):
        self.components = components
        self.obj = None  # No direct object for this composite

    def __or__(self, other):
        """Support chaining with | operator"""
        if not isinstance(other, (JobABC, Parallel, Serial)):
            other = WrappingJob(other)
        
        return Parallel(*list(self.components) + [other])
        
    def __rshift__(self, other):
        """Support chaining with >> operator"""
        if not isinstance(other, (JobABC, Parallel, Serial)):
            other = WrappingJob(other)
            
        return Serial(self, other)

    def __repr__(self):
        return f"parallel({', '.join(repr(c) for c in self.components)})"

class Serial:
    def __init__(self, *components):
        self.components = components
        self.obj = None  # No direct object for this composite
        
    def __or__(self, other):
        """Support chaining with | operator"""
        if not isinstance(other, (JobABC, Parallel, Serial)):
            other = WrappingJob(other)
            
        return Parallel(self, other)
        
    def __rshift__(self, other):
        """Support chaining with >> operator"""
        if not isinstance(other, (JobABC, Parallel, Serial)):
            other = WrappingJob(other)
            
        return Serial(*list(self.components) + [other])
        
    def __repr__(self):
        return f"serial({', '.join(repr(c) for c in self.components)})"


def wrap(obj=None, **kwargs):
    """
    Wrap any object to enable direct graph operations with | and >> operators.
    
    This function is the key to enabling the clean syntax:
    wrap(obj1) | wrap(obj2)  # For parallel composition
    wrap(obj1) >> wrap(obj2)  # For serial composition
    
    Enhanced functionality:
    1. Single object wrapping:
       - For JobABC instances: sets the name property and returns the instance
       - For Serial/Parallel: returns the object unchanged
       - For other objects: creates a WrappingJob with the given name
    
    2. dict (kwargs) object wrapping:
       wrap(obj_a_name=obj_a, obj_b_name=obj_b) or wrap({"obj_a_name": obj_a, "obj_b_name": obj_b})
       - Returns a collection of wrapped objects following the rules in case 1
       - If only one item, returns a dict with the name as key and the wrapped object as value
    """
    # Case 1: Only keyword arguments provided (no positional argument)
    if obj is None and kwargs:
        # Process keyword arguments
        result = {}
        for name, value in kwargs.items():
            if isinstance(value, JobABC):
                value.name = name
                result[name] = value
            elif isinstance(value, (Parallel, Serial)):
                result[name] = value
            else:
                result[name] = WrappingJob(value, name)
        
        # If only one item, return just that item
        if len(result) == 1:
            return next(iter(result.values()))
        return result
    
    # Case 2: Dictionary passed as the first argument
    if isinstance(obj, dict):
        result = {}
        for name, value in obj.items():
            if isinstance(value, JobABC):
                value.name = name
                result[name] = value
            elif isinstance(value, (Parallel, Serial)):
                result[name] = value
            else:
                result[name] = WrappingJob(value, name)
        
        # If only one item, return just that item
        if len(result) == 1:
            return next(iter(result.values()))
        return result
    
    # Case 3: Original behavior - single object
    # Handle the case where obj is None (could happen if called with wrap())
    if obj is None:
        raise ValueError("wrap() requires at least one argument")
        
    if isinstance(obj, (JobABC, Parallel, Serial)):
        return obj  # Already has the operations we need
    return WrappingJob(obj)

# Synonym for wrap
w = wrap

def parallel(*objects, **kwargs):
    """
    Create a parallel composition from multiple objects.
    
    This utility function takes objects (which can be a mix of JobABC
    instances and regular objects) and creates a parallel composition of all of them.
    
    Example:
        graph = parallel(obj1, obj2, obj3)  # Equivalent to wrap(obj1) | wrap(obj2) | wrap(obj3)
        
        # Also supports list argument for backward compatibility
        objects = [obj1, obj2, obj3]
        graph = parallel(objects)  # Still works if a single list is passed
        
        # Named objects using kwargs
        graph = parallel(object_a_name=object_a, object_b_name=object_b)
        
        # Named objects using a dictionary
        graph = parallel({"object_a_name": object_a, "object_b_name": object_b})
    """
    # Case 1: Only keyword arguments provided (no positional arguments)
    if not objects and kwargs:
        # Wrap each item with its name and then combine them with the | operator
        wrapped_items = wrap(**kwargs)
        if not isinstance(wrapped_items, dict):
            # If wrap returned a single item (not a dict), return it
            return wrapped_items
            
        if not wrapped_items:
            raise ValueError("Cannot create a parallel composition from empty arguments")
        
        # Convert dictionary to a list of items
        items = list(wrapped_items.values())
        if len(items) == 1:
            return items[0]
        
        # Combine all items with the | operator
        return reduce(lambda acc, obj: acc | obj, items[1:], items[0])
    
    # Case 2: Dictionary passed as the first argument
    if len(objects) == 1 and isinstance(objects[0], dict) and not kwargs:
        # Wrap each item with its name and then combine them with the | operator
        wrapped_items = wrap(objects[0])
        if not isinstance(wrapped_items, dict):
            # If wrap returned a single item (not a dict), return it
            return wrapped_items
            
        if not wrapped_items:
            raise ValueError("Cannot create a parallel composition from empty arguments")
        
        # Convert dictionary to a list of items
        items = list(wrapped_items.values())
        if len(items) == 1:
            return items[0]
        
        # Combine all items with the | operator
        return reduce(lambda acc, obj: acc | obj, items[1:], items[0])
    
    # Case 3: Original behavior - using positional arguments
    # Handle case where a single list is passed (for backward compatibility)
    if len(objects) == 1 and isinstance(objects[0], list):
        objects = objects[0]
        
    if not objects:
        raise ValueError("Cannot create a parallel composition from empty arguments")
    if len(objects) == 1:
        return wrap(objects[0])
    return reduce(lambda acc, obj: acc | wrap(obj), objects[1:], wrap(objects[0]))

# Synonym for parallel
p = parallel

def serial(*objects, **kwargs):
    """
    Create a serial composition from multiple objects.
    
    This utility function takes objects (which can be a mix of JobABC
    instances and regular objects) and creates a serial composition of all of them.
    
    Example:
        graph = serial(obj1, obj2, obj3)  # Equivalent to wrap(obj1) >> wrap(obj2) >> wrap(obj3)
        
        # Also supports list argument for backward compatibility
        objects = [obj1, obj2, obj3]
        graph = serial(objects)  # Still works if a single list is passed
        
        # Named objects using kwargs
        graph = serial(object_a_name=object_a, object_b_name=object_b)
        
        # Named objects using a dictionary
        graph = serial({"object_a_name": object_a, "object_b_name": object_b})
    """
    # Case 1: Dictionary argument
    if len(objects) == 1 and isinstance(objects[0], dict) and not kwargs:
        wrapped_items = {name: wrap({name: obj}) for name, obj in objects[0].items()}
        if not wrapped_items:
            raise ValueError("Cannot create a serial composition from empty arguments")
        if len(wrapped_items) == 1:
            return list(wrapped_items.values())[0]
        items = list(wrapped_items.values())
        return reduce(lambda acc, obj: acc >> obj, items[1:], items[0])
        
    # Case 2: Kwargs provided (object_name=object syntax)
    if kwargs:
        wrapped_items = {name: wrap({name: obj}) for name, obj in kwargs.items()}
        if not wrapped_items:
            raise ValueError("Cannot create a serial composition from empty arguments")
        if len(wrapped_items) == 1:
            return list(wrapped_items.values())[0]
        items = list(wrapped_items.values())
        return reduce(lambda acc, obj: acc >> obj, items[1:], items[0])
    
    # Case 3: Original behavior - using positional arguments
    # Handle case where a single list is passed (for backward compatibility)
    if len(objects) == 1 and isinstance(objects[0], list):
        objects = objects[0]
        
    if not objects:
        raise ValueError("Cannot create a serial composition from empty arguments")
    if len(objects) == 1:
        return wrap(objects[0])
    return reduce(lambda acc, obj: acc >> wrap(obj), objects[1:], wrap(objects[0]))

# Synonym for serial
s = serial

# Graph evaluation utilities have been moved to tests/test_utils/graph_evaluation.py


================================================
FILE: src/flow4ai/dsl_graph.py
================================================
from typing import Dict, List, Tuple

from . import f4a_logging as logging
from .dsl import JobsDict, Parallel, Serial
from .job import JobABC
from .jobs.wrapping_job import WrappingJob

logger = logging.getLogger(__name__)


PrecedenceGraph = Dict[str, Dict[str, List[str]]]

def dsl_to_precedence_graph(dsl_obj) -> Tuple[PrecedenceGraph, JobsDict]:
    """
    Convert a DSL object into a precedence graph with nested dictionary.
    
    Args:
        dsl_obj: A DSL object created with operators >> and |, or functions p() and s()
    
    Returns:
        Dict[str, Dict[str, List[str]]]: A graph definition in the format:
        {
            'Job A': {'next': ['Job B', 'Job C']},
            'Job B': {'next': []},
            ...
        }
        Where keys are node string representations and values are dictionaries with 'next' key
        pointing to lists of successor node representations.
    """
    # Print the DSL object structure details to help with debugging and understanding
    if logger.getEffectiveLevel() == logging.DEBUG:
        debug_dsl_structure(dsl_obj)
    
    # Extract all unique job objects from the DSL structure
    jobs = extract_jobs(dsl_obj)
    logger.info(f"Extracted {len(jobs)} jobs from DSL")
    
    # Initialize the graph with empty adjacency nested dictionaries using string representation of jobs
    graph = {job.name: {'next': []} for job in jobs}
    jobs: JobsDict = {job.name: job for job in jobs}
    
    # Build connections based on DSL structure using the new format
    build_connections(dsl_obj, graph, nested=True)
    
    return graph, jobs



def extract_jobs(dsl_obj):
    """
    Extract all individual job objects from a DSL structure.
    
    Args:
        dsl_obj: The DSL object to extract jobs from
        
    Returns:
        list: List of unique job objects
    """
    jobs = []
    
    def _extract(obj):
        if isinstance(obj, (Serial, Parallel)):
            # For compositional structures, extract jobs from each component
            for comp in obj.components:
                _extract(comp)
        elif isinstance(obj, JobABC):
            # Recursively extract jobs from wrapped compositional structures
            if isinstance(obj, WrappingJob) and isinstance(obj.callable, (Serial, Parallel)):
                _extract(obj.callable)
            # Terminal job object
            elif obj not in jobs:
                jobs.append(obj)
        else:
            # This is a primitive value that will be auto-wrapped
            wrapped = WrappingJob(obj)
            if wrapped not in jobs:
                jobs.append(wrapped)
    
    _extract(dsl_obj)
    return jobs



def build_connections(dsl_obj, graph, nested=False):
    """
    Build the connections in the graph based on the DSL structure.
    
    Args:
        dsl_obj: The DSL object to analyze
        graph: The graph to build connections in where keys and values are string representations of jobs
        nested: If True, the graph uses the nested format {'next': [...]} for edges instead of direct lists
    """
    def _process_serial(serial_obj, prev_terminals=None):
        if not serial_obj.components:
            return []
        
        # Start with the first component
        curr_terminals = []
        for i, comp in enumerate(serial_obj.components):
            # Process the current component
            if i == 0:
                # First component
                curr_terminals = _process_component(comp, prev_terminals)
            else:
                # Connect previous terminals to the current component
                curr_terminals = _process_component(comp, curr_terminals)
        
        return curr_terminals
    
    def _process_parallel(parallel_obj, prev_terminals=None):
        if not parallel_obj.components:
            return []
        
        all_terminals = []
        for comp in parallel_obj.components:
            # Process each component in parallel
            comp_terminals = _process_component(comp, prev_terminals)
            all_terminals.extend(comp_terminals)
        
        return all_terminals
    
    def _process_component(comp, prev_terminals=None):
        # Handle different types of components
        if isinstance(comp, Serial):
            return _process_serial(comp, prev_terminals)
        elif isinstance(comp, Parallel):
            return _process_parallel(comp, prev_terminals)
        elif isinstance(comp, JobABC):
            if isinstance(comp, WrappingJob):
                # Check if the wrapped object is a compositional structure
                wrapped = comp.callable
                if isinstance(wrapped, (Serial, Parallel)):
                    return _process_component(wrapped, prev_terminals)
            
            # This is a terminal job object (either WrappingJob with non-compositional object
            # or any other JobABC subclass)
            comp_str = comp.name
            
            # Connect previous terminals to this component
            if prev_terminals:
                for term in prev_terminals:
                    term_str = term.name
                    if nested:
                        if comp_str not in graph[term_str]['next']:
                            graph[term_str]['next'].append(comp_str)
                    else:
                        if comp_str not in graph[term_str]:
                            graph[term_str].append(comp_str)
            
            return [comp]
        else:
            # This is a primitive value that will be auto-wrapped
            wrapped = WrappingJob(comp)
            comp_str = wrapped.name
            
            # Connect previous terminals to this component
            if prev_terminals:
                for term in prev_terminals:
                    term_str = term.name
                    if nested:
                        if comp_str not in graph[term_str]['next']:
                            graph[term_str]['next'].append(comp_str)
                    else:
                        if comp_str not in graph[term_str]:
                            graph[term_str].append(comp_str)
            
            return [wrapped]
    
    # Start processing from the top-level DSL object
    _process_component(dsl_obj)

def debug_dsl_structure(dsl_obj, indent=0):
    """
    Print the structure of a DSL object for debugging purposes.
    
    Args:
        dsl_obj: The DSL object to analyze
        indent: Current indentation level for nested printing
    """
    indent_str = "  " * indent
    
    if isinstance(dsl_obj, Serial):
        print(f"{indent_str}Serial with {len(dsl_obj.components)} components")
        for i, comp in enumerate(dsl_obj.components):
            print(f"{indent_str}Component {i}:")
            debug_dsl_structure(comp, indent + 1)
    
    elif isinstance(dsl_obj, Parallel):
        print(f"{indent_str}Parallel with {len(dsl_obj.components)} components")
        for i, comp in enumerate(dsl_obj.components):
            print(f"{indent_str}Component {i}:")
            debug_dsl_structure(comp, indent + 1)
    
    elif isinstance(dsl_obj, WrappingJob):
        print(f"{indent_str}WrappingJob wrapping: {dsl_obj.callable}")
    
    else:
        print(f"{indent_str}Other Type: {type(dsl_obj).__name__} - Value: {dsl_obj}")



def visualize_graph(graph):
    """
    Visualize the graph structure for debugging purposes.
    Displays parent nodes before their children for better readability.
    Ensures a node is only displayed after all of its parents have been processed.
    
    Args:
        graph: A graph definition, either in:
            - Adjacency list format Dict[str, List[str]]
            - Or nested dictionary format Dict[str, Dict[str, List[str]]] with 'next' key
    """
    print("Graph Structure:")
    
    # Check if we're using the nested format
    is_nested = all(isinstance(node_data, dict) and 'next' in node_data for node_data in graph.values())
    
    # Build a reverse graph (child -> parents) to track parent relationships
    reverse_graph = {}
    for node in graph:
        reverse_graph[node] = []
    
    # Find all parent-child relationships
    for parent, node_data in graph.items():
        # Get children based on the graph format
        if is_nested:
            children = node_data['next']
        else:
            children = node_data
            
        for child in children:
            if child not in reverse_graph:
                reverse_graph[child] = []
            reverse_graph[child].append(parent)
    
    # Use Kahn's algorithm for topological sorting
    # Identify nodes with no parents (root nodes)
    root_nodes = []
    for node, parents in reverse_graph.items():
        if not parents and node in graph:  # Only include nodes that are in the original graph
            root_nodes.append(node)
    
    # Process nodes in order, ensuring a node is only processed after all its parents
    node_order = []
    while root_nodes:
        node = root_nodes.pop(0)  # Get a node with no unprocessed parents
        node_order.append(node)
        
        # Process this node's children
        if is_nested:
            children = graph.get(node, {}).get('next', [])
        else:
            children = graph.get(node, [])
            
        for child in children:
            # Remove this parent from the child's parent list
            reverse_graph[child].remove(node)
            # If the child has no more unprocessed parents, add it to the root nodes
            if not reverse_graph[child]:
                root_nodes.append(child)
    
    # Check for cycles or disconnected components
    remaining_nodes = [node for node in graph if node not in node_order]
    if remaining_nodes:
        # For disconnected components, process them separately
        for node in sorted(remaining_nodes):
            if node not in node_order:  # Skip if already added through processing
                node_order.append(node)
    
    # Print the graph in the calculated order
    for node in node_order:
        if is_nested:
            next_nodes = graph.get(node, {}).get('next', [])
            print(f"{node}: {{'next': {next_nodes}}}")
        else:
            next_nodes = graph.get(node, [])
            if next_nodes:
                print(f"{node}: {next_nodes}")
            else:
                print(f"{node}: []")



if __name__ == "__main__":
   pass


================================================
FILE: src/flow4ai/f4a_graph.py
================================================
"""
Flow4AI Graph module for handling directed acyclic graphs with subgraphs.
Provides functionality for graph traversal, cycle detection, and validation.
"""

from typing import Any, Dict, List, Optional, Set, Tuple

from . import f4a_logging

logging = f4a_logging.getLogger(__name__)


def has_cycle(graph: Dict[str, Dict[str, Any]], node: str, 
             visited: Optional[Set[str]] = None, path: Optional[Set[str]] = None) -> Tuple[bool, List[str]]:
    """
    Check if the graph has a cycle starting from the given node.
    
    Args:
        graph: The graph structure to check
        node: Starting node for cycle detection
        visited: Set of all visited nodes (for recursive calls)
        path: Set of nodes in current path (for cycle detection)
    
    Returns:
        Tuple[bool, List[str]]: (has_cycle, cycle_path)
    """
    if visited is None:
        visited = set()
    if path is None:
        path = set()
    
    visited.add(node)
    path.add(node)
    
    node_obj = graph[node]
    # Check next nodes
    for next_node in node_obj.get('next', []):
        if next_node in path:  # Cycle detected
            return True, [*path, next_node]
        if next_node not in visited:
            has_cycle_result, cycle_path = has_cycle(graph, next_node, visited, path)
            if has_cycle_result:
                return True, cycle_path
    
    # Check subgraph if it exists
    if 'subgraph' in node_obj:
        subgraph = node_obj['subgraph']
        for subnode in subgraph:
            if subnode not in visited:
                has_cycle_result, cycle_path = has_cycle(subgraph, subnode, visited, path)
                if has_cycle_result:
                    return True, cycle_path
    
    path.remove(node)
    return False, []

def check_graph_for_cycles(graph: Dict[str, Dict[str, Any]], name: str = "") -> bool:
    """
    Check entire graph for cycles.
    
    Args:
        graph: The graph structure to check
        name: Optional name for the graph (for logging)
    
    Returns:
        bool: True if cycles were found, False otherwise
    """
    print(f"\nChecking {name} for cycles...")
    for node in graph:
        has_cycle_result, cycle_path = has_cycle(graph, node)
        if has_cycle_result:
            print(f"Cycle detected! Path: {' -> '.join(cycle_path)}")
            return True
    print("No cycles detected")
    return False

def find_node_and_graph(main_graph: Dict[str, Dict[str, Any]], target_node: str, 
                       current_graph: Optional[Dict[str, Dict[str, Any]]] = None) -> Tuple[Optional[Dict[str, Dict[str, Any]]], List[str]]:
    """
    Recursively finds a node and its containing graph in the graph structure.
    
    Args:
        main_graph: The root graph structure
        target_node: The node to find
        current_graph: Current graph being searched (for recursive calls)
    
    Returns:
        Tuple[Optional[Dict], List[str]]: (containing_graph, path_to_node)
    """
    if current_graph is None:
        current_graph = main_graph
        
    # Check if node is in current level
    if target_node in current_graph:
        return current_graph, []
        
    # Search in subgraphs
    for node in current_graph:
        if 'subgraph' in current_graph[node]:
            subgraph = current_graph[node]['subgraph']
            result_graph, path = find_node_and_graph(main_graph, target_node, subgraph)
            if result_graph is not None:
                return result_graph, [node] + path
                
    return None, []

def add_edge(graph: Dict[str, Dict[str, Any]], from_node: str, to_node: str) -> bool:
    """
    Attempts to add an edge between two nodes in the same graph level.
    
    Args:
        graph: The graph containing both nodes
        from_node: Source node
        to_node: Target node
    
    Returns:
        bool: True if edge was added successfully
    """
    if from_node not in graph:
        print(f"Error: Source node {from_node} not found in graph")
        return False
    
    # Initialize 'next' list if it doesn't exist
    if 'next' not in graph[from_node]:
        graph[from_node]['next'] = []
    
    # Check if edge already exists
    if to_node in graph[from_node]['next']:
        print(f"Edge {from_node} -> {to_node} already exists")
        return True
    
    # Temporarily add the edge
    graph[from_node]['next'].append(to_node)
    
    # Check for cycles
    has_cycle_result, cycle_path = has_cycle(graph, from_node)
    
    if has_cycle_result:
        # Remove the edge if it would create a cycle
        graph[from_node]['next'].remove(to_node)
        print(f"Cannot add edge {from_node} -> {to_node} as it would create a cycle")
        print(f"Cycle detected: {' -> '.join(cycle_path)}")
        return False
    
    print(f"Successfully added edge {from_node} -> {to_node}")
    return True

def add_edge_anywhere(main_graph: Dict[str, Dict[str, Any]], from_node: str, to_node: str) -> bool:
    """
    Attempts to add an edge between two nodes in the graph structure.
    
    Rules for edge addition:
    1. Both nodes must exist in the graph structure
    2. Nodes can only reference other nodes within the same graph level:
       - Main graph nodes can only reference other main graph nodes
       - Subgraph nodes can only reference nodes within the same subgraph
    3. No cycles are allowed within any graph level
    
    Args:
        main_graph: The root graph structure
        from_node: Source node
        to_node: Target node
    
    Returns:
        bool: True if edge was added successfully
    """
    # Find the containing graphs for both nodes
    from_graph, from_path = find_node_and_graph(main_graph, from_node)
    to_graph, to_path = find_node_and_graph(main_graph, to_node)
    
    if from_graph is None:
        print(f"Error: Source node {from_node} not found in graph")
        return False
        
    if to_graph is None:
        print(f"Error: Target node {to_node} not found in graph")
        return False
    
    # Check if nodes are in the same graph level
    if from_graph is not to_graph:
        print(f"Error: Cannot create edge between different graph levels")
        print(f"Source node {from_node} is in {' -> '.join(['main'] + from_path) if from_path else 'main graph'}")
        print(f"Target node {to_node} is in {' -> '.join(['main'] + to_path) if to_path else 'main graph'}")
        return False
    
    return add_edge(from_graph, from_node, to_node)

def print_graph(graph: Dict[str, Dict[str, Any]], spaces: int = 0) -> None:
    """
    Traverse and print the graph structure.
    
    Args:
        graph: The graph structure to traverse
        spaces: Number of spaces for indentation
    """
    for key in graph.keys():
        print("." * spaces + key)
        print_visit_node(graph, key, spaces)

def print_visit_node(graph: Dict[str, Dict[str, Any]], key: str, spaces: int = 0) -> None:
    """
    Visit and print a node's details.
    
    Args:
        graph: The graph containing the node
        key: The node key to visit
        spaces: Number of spaces for indentation
    """
    node_key_obj = graph[key]
    sub_graph_obj = node_key_obj.get('subgraph')
    if sub_graph_obj:
        print("-" * (spaces + 2) + "subgraph:")
        print_graph(sub_graph_obj, spaces+2)
        print("-" * (spaces + 2) + "end subgraph.")
    next_obj = node_key_obj.get('next')
    if next_obj:
        print_traverse_list(next_obj, graph, spaces+2)

def print_traverse_list(nodes: List[str], graph: Dict[str, Dict[str, Any]], spaces: int = 0) -> None:
    """
    Traverse and print a list of nodes.
    
    Args:
        nodes: List of node names
        graph: The graph containing the nodes
        spaces: Number of spaces for indentation
    """
    for node in nodes:
        print("." * spaces + " has dependent " + node)

def validate_graph_references(graph: Dict[str, Dict[str, Any]], path: Optional[List[str]] = None) -> Tuple[bool, List[str]]:
    """
    Validates that all node references in a graph structure are within their own graph level.
    This includes the main graph and all subgraphs.
    
    Args:
        graph: The graph structure to validate
        path: Current path in the graph (for error reporting)
        
    Returns:
        tuple: (is_valid, list_of_violations)
        where violations are strings describing each cross-graph reference found
    """
    if path is None:
        path = []
        
    violations = []
    graph_nodes = set(graph.keys())
    
    # Check each node's references
    for node, node_data in graph.items():
        current_path = path + [node] if path else [node]
        
        # Check 'next' references
        next_nodes = node_data.get('next', [])
        for next_node in next_nodes:
            if next_node not in graph_nodes:
                violations.append(
                    f"Node '{' -> '.join(current_path)}' references '{next_node}' "
                    f"which is not in the same graph level"
                )
        
        # Recursively check subgraphs
        if 'subgraph' in node_data:
            subgraph_valid, subgraph_violations = validate_graph_references(
                node_data['subgraph'], 
                current_path
            )
            violations.extend(subgraph_violations)
    
    return len(violations) == 0, violations

def find_head_nodes(graph: Dict[str, Dict[str, Any]]) -> Set[str]:
    """
    Find all nodes that have no incoming edges (head nodes) in a graph.
    
    Args:
        graph: The graph structure to check
        
    Returns:
        Set[str]: Set of nodes that have no incoming edges
    """
    # First collect all nodes that are destinations
    has_incoming_edges = set()
    for node in graph:
        # Check next nodes
        for next_node in graph[node].get('next', []):
            has_incoming_edges.add(next_node)
        # Check subgraph if it exists
        if 'subgraph' in graph[node]:
            subgraph = graph[node]['subgraph']
            # Recursively find head nodes in subgraph
            subgraph_heads = find_head_nodes(subgraph)
            # All nodes in subgraph are considered to have an incoming edge
            # from the parent node that contains the subgraph
            has_incoming_edges.update(subgraph.keys())
    
    # Head nodes are those that exist in the graph but have no incoming edges
    return set(graph.keys()) - has_incoming_edges

def find_tail_nodes(graph: Dict[str, Dict[str, Any]]) -> Set[str]:
    """
    Find all nodes that have no outgoing edges (tail nodes) in a graph.
    
    Args:
        graph: The graph structure to check
        
    Returns:
        Set[str]: Set of nodes that have no outgoing edges
    """
    tail_nodes = set()
    for node, node_data in graph.items():
        has_next = bool(node_data.get('next', []))
        has_subgraph = 'subgraph' in node_data
        
        if not has_next:
            if has_subgraph:
                # If node has no next but has subgraph, the tail nodes are in the subgraph
                subgraph_tails = find_tail_nodes(node_data['subgraph'])
                tail_nodes.update(subgraph_tails)
            else:
                # Node with no next and no subgraph is a tail
                tail_nodes.add(node)
    
    return tail_nodes

def validate_graph(graph: Dict[str, Dict[str, Any]], name: str = "") -> None:
    """
    Performs comprehensive validation of a graph structure.
    Checks for:
    1. Graph cycles
    2. Cross-graph reference violations
    3. Head node requirements (exactly one head node)
    4. Tail node requirements (exactly one tail node)
    
    Args:
        graph: The graph structure to validate
        name: Optional name for the graph (for logging)
        
    Raises:
        ValueError: If any validation fails, with detailed error message
    """
    errors = []
    
    # Check for cycles
    has_cycles = check_graph_for_cycles(graph, name)
    if has_cycles:
        msg = f"Graph {name} contains cycles"
        logging.error(msg)
        errors.append(msg)
    
    # Check for cross-graph reference violations
    is_valid_refs, violations = validate_graph_references(graph)
    if not is_valid_refs:
        msg = f"Graph {name} contains invalid cross-graph references:\n" + "\n".join(violations)
        logging.error(msg)
        errors.append(msg)
    
    # Check for head node requirements
    head_nodes = find_head_nodes(graph)
    if len(head_nodes) == 0:
        msg = f"Graph {name} has no head nodes (nodes with no incoming edges)"
        logging.error(msg)
        errors.append(msg)
    elif len(head_nodes) > 1:
        msg = f"Graph {name} has multiple head nodes: {head_nodes}. Exactly one head node is required."
        logging.warning(msg)
    
    # Check for tail node requirements
    tail_nodes = find_tail_nodes(graph)
    if len(tail_nodes) == 0:
        msg = f"Graph {name} has no tail nodes (nodes with no outgoing edges)"
        logging.error(msg)
        errors.append(msg)
    elif len(tail_nodes) > 1:
        msg = f"Graph {name} has multiple tail nodes: {tail_nodes}. Exactly one tail node is required."
        logging.warning(msg)
    
    # If any errors were found, raise exception with all error messages
    if errors:
        raise ValueError(f"Graph validation failed:\n" + "\n".join(errors))
    
    # Log success if no errors
    logging.info(f"Graph {name} passed all validations")



================================================
FILE: src/flow4ai/f4a_logging.py
================================================
"""
Logging configuration for Flow4AI.

Environment Variables:
    FLOW4AI_LOG_LEVEL: Set the logging level (e.g., 'DEBUG', 'INFO'). Defaults to 'INFO'.
    FLOW4AI_LOG_HANDLERS: Set logging handlers. Options:
        - Not set or 'console': Log to console only (default)
        - 'console,file': Log to both console and file
        
Example:
    To enable both console and file logging:
    $ export FLOW4AI_LOG_HANDLERS='console,file'
    
    To set debug level logging:
    $ export FLOW4AI_LOG_LEVEL='DEBUG'
"""


import os

# Initializing the flag here stops logging caching root levels to another value
# for reasons I'm not completely sure about.
WINDSURF_LOG_FLAG = None #None #"DEBUG"
os.environ['FLOW4AI_LOG_LEVEL'] = WINDSURF_LOG_FLAG or os.getenv('FLOW4AI_LOG_LEVEL', 'INFO')
import logging
from logging.config import dictConfig


def get_logging_config():
    """Get the logging configuration based on current environment variables."""
    
    
    return {
        'version': 1,
        'disable_existing_loggers': False,
        'formatters': {
            'detailed': {
                'format': '%(asctime)s [%(levelname)s] %(name)s:%(lineno)d - %(message)s'
            }
        },
        'handlers': {
            'console': {
                'class': 'logging.StreamHandler',
                'level': os.getenv('FLOW4AI_LOG_LEVEL', 'INFO'),
                'formatter': 'detailed',
                'stream': 'ext://sys.stdout'
            },
            'file': {
                'class': 'logging.FileHandler',
                'level': os.getenv('FLOW4AI_LOG_LEVEL', 'INFO'),
                'formatter': 'detailed',
                'filename': 'flow4ai.log',
                'mode': 'a'
            }
        },
        'loggers': {
            'ExampleCustom': {
                'level': 'DEBUG',
                'handlers': ['console', 'file'],
                'propagate': False
            }
        },
        'root': {
            'level':  os.getenv('FLOW4AI_LOG_LEVEL', 'INFO'),
            # Set FLOW4AI_LOG_HANDLERS='console,file' to enable both console and file logging
            'handlers': os.getenv('FLOW4AI_LOG_HANDLERS', 'console').split(',')
        }
    }

def setup_logging():
    """Setup logging with current configuration."""
    config = get_logging_config()
    
    # Always create log file with header, actual logging will only happen if handlers use it
    if not os.path.exists('flow4ai.log'):
        with open('flow4ai.log', 'w') as f:
            f.write('# Flow4AI log file - This file is created empty and will be written to only when file logging is enabled\n')
    
    print(f"Logging level: {config['root']['level']}")
    # Apply configuration
    dictConfig(config)

# Apply configuration when module is imported
setup_logging()

# Re-export everything from logging
# Constants
CRITICAL = logging.CRITICAL
ERROR = logging.ERROR
WARNING = logging.WARNING
INFO = logging.INFO
DEBUG = logging.DEBUG
NOTSET = logging.NOTSET

# Functions
getLogger = logging.getLogger
basicConfig = logging.basicConfig
shutdown = logging.shutdown
debug = logging.debug
info = logging.info
warning = logging.warning
error = logging.error
critical = logging.critical
exception = logging.exception
log = logging.log

# Classes
Logger = logging.Logger
Handler = logging.Handler
Formatter = logging.Formatter
Filter = logging.Filter
LogRecord = logging.LogRecord

# Handlers
StreamHandler = logging.StreamHandler
FileHandler = logging.FileHandler
NullHandler = logging.NullHandler

# Configuration
dictConfig = dictConfig  # Already imported from logging.config
fileConfig = logging.config.fileConfig

# Exceptions
exception = logging.exception
captureWarnings = logging.captureWarnings

# Additional utilities
getLevelName = logging.getLevelName
makeLogRecord = logging.makeLogRecord



================================================
FILE: src/flow4ai/flowmanager.py
================================================
import asyncio
import threading
from collections import defaultdict
from typing import Any, Callable, Dict, List, Optional, Union

from flow4ai import f4a_logging as logging
from flow4ai.flowmanager_base import FlowManagerABC
from flow4ai.job import SPLIT_STR, JobABC, Task, job_graph_context_manager
from flow4ai.job_loader import JobFactory


class FlowManager(FlowManagerABC):
    _instance = None
    _lock = threading.Lock()  # Class-level lock for singleton creation

    def __new__(cls, *args, **kwargs):  # Accept arbitrary arguments
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance.__initialized = False
        return cls._instance

    def __init__(self, dsl=None, jobs_dir_mode=False, on_complete: Optional[Callable[[Any], None]] = None):
        """Initialize the FlowManager.
        
        Args:
            dsl: A dictionary of job DSLs, a job DSL, a JobABC instance, or a collection of JobABC instances.
            jobs_dir_mode: If True, the FlowManager will load jobs from a directory.
            on_complete: A callback function to be called when a job is completed.
        """
        super().__init__()
        self.jobs_dir_mode = jobs_dir_mode
        self.on_complete = on_complete
        if not hasattr(self, '_TaskManager__initialized') or not self.__initialized:
            with self._lock:
                if not hasattr(self, '_TaskManager__initialized') or not self.__initialized:
                    self._initialize()
                    self.__initialized = True
        
        # Add DSL dictionary if provided
        if dsl:
            self.create_job_graph_map(dsl)

    def _initialize(self):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.loop = asyncio.new_event_loop()
        self.thread = threading.Thread(target=self._run_loop, daemon=True)
        self.thread.start()
        self.head_jobs: List[JobABC] = []
        if self.jobs_dir_mode:
            self.head_jobs = JobFactory.get_head_jobs_from_config()
            self.job_graph_map = {job.name: job for job in self.head_jobs}
        self.submitted_count = 0
        self.completed_count = 0
        self.error_count = 0
        self.completed_results = defaultdict(list)
        self.error_results = defaultdict(list)

        self._data_lock = threading.Lock()

    def _run_loop(self):
        asyncio.set_event_loop(self.loop)
        self.loop.run_forever()

    async def _execute_with_context(self, job: JobABC, task: Task):
        """Execute a job with the job graph context manager.
        
        This ensures that the local async context variables are reset for each
        coroutine that is executed.

        Args:
            job: The job to execute
            task: The task to process
            
        Returns:
            The result of the job execution
        """
        # Create a job set for this job
        job_set = JobABC.job_set(job)
        
        # Execute the job within the context manager
        async with job_graph_context_manager(job_set):
            return await job._execute(task)
    
    def _record_error(self, task: Union[Task, List[Task]], job_key: str, error_msg: str):
        """Helper method to record errors for both single tasks and lists of tasks.
        
        Args:
            task: A Task or list of Tasks that encountered an error
            job_key: The job key for tracking errors
            error_msg: The error message
            
        Returns:
            None
        """
        with self._data_lock:
            if isinstance(task, list):
                self.error_count += len(task)
                for single_task in task:
                    self.error_results[job_key].append({
                        "error": ValueError(error_msg),
                        "task": single_task
                    })
            else:
                self.error_count += 1
                self.error_results[job_key].append({
                    "error": ValueError(error_msg),
                    "task": task
                })


    def submit_task(self, task: Union[Dict[str, Any], List[Dict[str, Any]], str], fq_name: str = None):
        fq_name = self.check_fq_name_and_job_graph_map(fq_name)

        # Handle single task or list of tasks
        if isinstance(task, list):
            for single_task in task:
                self._submit_single_task(single_task, fq_name)
        else:
            self._submit_single_task(task, fq_name)

    def _submit_single_task(self, task: Dict[str, Any], fq_name: str):
        """Helper method to submit a single task to the job.
        
        Args:
            task: The task to submit
            fq_name: The fully qualified name of the job graph
        """
        with self._data_lock:
            self.submitted_count += 1
        if not isinstance(task, dict):
            task = {'task': str(task)}
        task_obj = Task(task, fq_name)
        job = self.job_graph_map.get(task_obj.get_fq_name())
        if job is None:
            raise ValueError(f"Job not found for fq_name: {task_obj.get_fq_name()}")
        coro = self._execute_with_context(job, task_obj)
        future = asyncio.run_coroutine_threadsafe(coro, self.loop)
        future.add_done_callback(
            lambda f: self._handle_completion(f, job, task_obj)
        )

    def _handle_completion(self, future, job: JobABC, task: Task):
        result = None
        exception = None
        try:
            result = future.result()
            with self._data_lock:
                self.completed_count += 1
                # job.name is fq_name
                self.completed_results[job.name].append(result)
                
            if self.on_complete:
                #try: don't catch the exception let it bubble up
                self.on_complete(result)
       
        except Exception as e:
            exception = e
            self.logger.error(f"Error processing result: {e}")
            self.logger.info("Detailed stack trace:", exc_info=True)
            with self._data_lock:
                self.error_count += 1
                self.error_results[job.name].append({
                    "error": e,
                    "task": task
                })

    def get_fq_names_by_graph(self, graph_name, variant=""):
        """
        Get the fully qualified names for a specific graph and variant.
        
        This handles cases where multiple DSLs with the same structure 
        might have been added with the same graph_name and variant, 
        but received different FQ names due to collision handling.
        
        Args:
            graph_name: The name of the graph
            variant: The variant name, defaults to empty string
            
        Returns:
            The list of matching fully qualified names, or empty list if none found
        """
        matching_names = []
        base_prefix = f"{graph_name}{SPLIT_STR}{variant}"
        
        # Import re for regex pattern matching
        import re

        # Find exact match first
        for job_name in self.job_graph_map.keys():
            if job_name.startswith(base_prefix + SPLIT_STR):
                matching_names.append(job_name)
                
        # Also look for variants with numeric suffixes (added by collision handling)
        pattern = re.compile(re.escape(graph_name + SPLIT_STR) + 
                           re.escape(variant) + r'_\d+' + 
                           re.escape(SPLIT_STR))
        
        for job_name in self.job_graph_map.keys():
            if pattern.match(job_name):
                # Only add if not already added (could happen if exact match already found)
                if job_name not in matching_names:
                    matching_names.append(job_name)
                    
        return matching_names
    
    def submit_by_graph(self, task, graph_name, variant=""):
        """
        Submit task(s) to a specific graph and variant.
        
        Args:
            task: The task or list of tasks to submit
            graph_name: The name of the graph
            variant: The variant name, defaults to empty string
            
        Returns:
            None
            
        Raises:
            ValueError: If no matching graph is found or if multiple matches found
        """
        matching_fq_names = self.get_fq_names_by_graph(graph_name, variant)
        
        if not matching_fq_names:
            error_msg = f"No graph found with name '{graph_name}' and variant '{variant}'"
            self.logger.error(error_msg)
            raise ValueError(error_msg)
        
        if len(matching_fq_names) > 1:
            # If multiple matches, log them all and raise an error
            self.logger.error(f"Multiple matching graphs found for '{graph_name}' variant '{variant}': {matching_fq_names}")
            error_msg = (f"Multiple matching graphs found. Please use submit() with the specific FQ name "
                        f"from these options: {matching_fq_names}")
            raise ValueError(error_msg)
        
        # If exactly one match, use it
        fq_name = matching_fq_names[0]
        return self.submit_task(task, fq_name)

    def get_counts(self):
        with self._data_lock:
            return {
                'submitted': self.submitted_count,
                'completed': self.completed_count,
                'errors': self.error_count
            }

    def pop_results(self):
        with self._data_lock:
            completed = dict(self.completed_results)
            errors = dict(self.error_results)
            self.completed_results.clear()
            self.error_results.clear()
            return {
                'completed': completed,
                'errors': errors
            }
            
    def wait_for_completion(self, timeout=10, check_interval=0.1):
        """
        Wait for all submitted tasks to complete or error out.
        
        Args:
            timeout: Maximum time to wait in seconds. Defaults to 10 seconds.
            check_interval: How often to check for completion in seconds. Defaults to 0.1 seconds.
            
        Returns:
            bool: True if all tasks completed or errored, False if timed out
        """
        import time
        start_time = time.time()
        
        while (time.time() - start_time) < timeout:
            counts = self.get_counts()
            self.logger.info(f"Submitted: {counts['submitted']}, Completed: {counts['completed']}, Errors: {counts['errors']}")
            if counts['submitted'] > 0 and counts['submitted'] == (counts['completed'] + counts['errors']):
                return True
            time.sleep(check_interval)
            
        # Check one last time before returning
        counts = self.get_counts()
        return counts['submitted'] == (counts['completed'] + counts['errors'])
        
    def execute(self, task, dsl=None, graph_name=None, fq_name=None, timeout=10):
        """
        Simplified execution method that handles the entire workflow.
        
        Args:
            task: The task to process
            dsl: Optional DSL component (if not using an existing graph)
            graph_name: Name for the graph if providing a DSL
            fq_name: Fully qualified name for an existing graph
            timeout: Maximum time to wait for completion
            
        Returns:
            A tuple of (errors, result) where errors is a list of error dictionaries and
            result is the result dictionary of the completed job(s)
        
        Raises:
            ValueError: If required parameters are missing
            TimeoutError: If tasks don't complete within the timeout period
            Exception: If any errors occurred during execution
        """
        if dsl and graph_name:
            fq_name = self.add_dsl(dsl, graph_name)
            
        if not fq_name:
            raise ValueError("Either provide both dsl and graph_name or an fq_name")
            
        self.submit_task(task, fq_name)
        success = self.wait_for_completion(timeout=timeout)
        
        if not success:
            raise TimeoutError(f"Timed out waiting for tasks to complete after {timeout} seconds")
            
        results = self.pop_results()
        
        # Check for errors and raise if present
        if results["errors"]:
            error_messages = []
            for job_name, job_errors in results["errors"].items():
                for error_data in job_errors:
                    error_msg = f"{job_name}: {error_data['error']}"
                    error_messages.append(error_msg)
            
            raise Exception("Errors occurred during job execution:\n" + "\n".join(error_messages))
            
        # Extract the result directly using the known fq_name
        result = None
        if results["completed"] and fq_name in results["completed"] and results["completed"][fq_name]:
            result = results["completed"][fq_name][0]
            
        return (results["errors"], result)
        
    def get_result(self, results=None, job_name_filter=None):
        """
        Extract a specific result from the completed results.
        
        Args:
            results: Results dictionary from pop_results() or None to use latest results
            job_name_filter: Optional string to filter job names (e.g., "add" to match jobs containing "add")
            
        Returns:
            The result data dictionary for the matched job, or None if not found
        """
        if results is None:
            results = self.pop_results()
            
        # Simple implementation - just find the job with the matching name
        for job_name, job_results in results["completed"].items():
            if job_name_filter is None or job_name_filter in job_name:
                if job_results:  # Make sure there's at least one result
                    return job_results[0]
        return None
        
    def get_result_value(self, results=None, job_name_filter=None):
        """
        Extract just the result value from a specific job result.
        
        Args:
            results: Results dictionary from pop_results() or None to use latest results
            job_name_filter: Optional string to filter job names
            
        Returns:
            The value of the "result" key for the matched job, or None if not found
        """
        result = self.get_result(results, job_name_filter)
        if result and "result" in result:
            return result["result"]
        return None
        
    def get_result_by_graph_name(self, graph_name, results=None):
        """
        Get the result dictionary for a specific graph by name.
        
        This simplifies result extraction by handling the lookup of the fully qualified name.
        Note: This method is primarily for backward compatibility - new code should use execute
        which directly returns the result.
        
        Args:
            graph_name: The graph name used when adding the DSL or executing the task
            results: Results dictionary from pop_results() or None to use latest results
            
        Returns:
            The result dictionary for the matched graph, or None if not found
        """
        if results is None:
            results = self.pop_results()
            
        # Find the fully qualified name based on the graph name
        fq_name = None
        for key in results["completed"].keys():
            if graph_name in key:
                fq_name = key
                break
                
        if fq_name is None:
            return None
            
        # Get the result dictionary
        if results["completed"][fq_name]:
            return results["completed"][fq_name][0]
            
        return None
        
    def get_fq_names(self):
        """
        Returns a list of all fully qualified names of graphs added to the FlowManager.
        
        Returns:
            List[str]: List of fully qualified names
        """
        return [job.name for job in self.head_jobs]

    @classmethod
    def run(cls, dsl, task, graph_name="default_graph", timeout=10):
        """
        Static helper method for one-line execution of a DSL graph with a task.
        
        Args:
            dsl: The DSL component defining the job graph
            task: Task dictionary to process
            graph_name: Name for the graph
            timeout: Maximum time to wait for completion
            
        Returns:
            A tuple of (errors, result) where errors is a list of error dictionaries and
            result is the result dictionary of the completed job(s)
        
        Raises:
            TimeoutError: If tasks don't complete within the timeout period
            Exception: If any errors occurred during execution
        """
        tm = cls()
        return tm.execute(task, dsl=dsl, graph_name=graph_name, timeout=timeout)

    def display_results(self, results=None):
        """
        Display results in a Jupyter/Colab-friendly format with rich formatting.
        
        Args:
            results: Results dictionary from pop_results() or None to use latest results
            
        Returns:
            The results dictionary for chaining
        """
        try:
            from IPython.display import HTML, Markdown, display
            jupyter_available = True
        except ImportError:
            jupyter_available = False
            
        if results is None:
            results = self.pop_results()
        
        if jupyter_available:
            # Display in rich Jupyter format
            if results["completed"]:
                display(Markdown("## Completed Tasks"))
                for job_name, job_results in results["completed"].items():
                    display(Markdown(f"### {job_name}"))
                    for result_data in job_results:
                        display(result_data["result"])
            
            if results["errors"]:
                display(Markdown("## Errors"))
                for job_name, job_errors in results["errors"].items():
                    display(Markdown(f"### {job_name}"))
                    for error_data in job_errors:
                        display(HTML(f"<div style='color:red'>{error_data['error']}</div>"))
        else:
            # Fallback to plain text output
            if results["completed"]:
                print("\nCompleted tasks:")
                for job_name, job_results in results["completed"].items():
                    for result_data in job_results:
                        print(f"- {job_name}: {result_data['result']}")
            
            if results["errors"]:
                print("\nErrors:")
                for job_name, job_errors in results["errors"].items():
                    for error_data in job_errors:
                        print(f"- {job_name}: {error_data['error']}")
        
        return results



================================================
FILE: src/flow4ai/flowmanager_base.py
================================================
"""
Base class for Flow Manager implementations.

This module provides the abstract base class that defines the common interface
for all Flow Manager implementations in the Flow4AI framework.
"""

from abc import ABC, abstractmethod
from typing import Any, Collection, Dict, List, Union

from .dsl import DSLComponent, JobsDict
from .dsl_graph import PrecedenceGraph, dsl_to_precedence_graph
from .f4a_graph import validate_graph
from .job import SPLIT_STR, JobABC
from .job_loader import JobFactory


class FlowManagerABC(ABC):
    """
    Abstract base class for Flow Manager implementations.
    
    FlowManagerABC defines the minimal common interface for all Flow Manager implementations.
    Since FlowManager and FlowManagerMP were developed independently with different interfaces,
    this base class focuses on the core conceptual similarities rather than enforcing identical
    method signatures.
    """
    
    @abstractmethod
    def __init__(self, *args, **kwargs):
        """
        Initialize the FlowManager instance.
        
        Each implementation should handle its own initialization with appropriate parameters.
        """
        self.job_graph_map: Dict[str, JobABC] = {}
        self.head_jobs: List[JobABC] = []
        
    def get_head_jobs(self) -> List[JobABC]:
        """
        Get all head jobs in the job graph.
        
        Head jobs are the entry points of the job graph with no predecessors.
        
        Returns:
            List[JobABC]: A list of all head jobs in the job graph
        """
        return self.head_jobs

    @abstractmethod
    def _submit_single_task(self, task: Union[Dict[str, Any], str], fq_name: str) -> None:
        """
        Process and submit a single task to be executed by the job graph. Must be implemented by subclasses.
        
        This is an internal method that handles the submission of a single task. It should:
        1. Convert the dict or string to a Task object if it isn't already one
        2. Validate that the specified job graph exists
        3. Submit the task for execution according to the implementation's concurrency model
        
        Args:
            task: The task to process, either as a dictionary or string. If a string is provided,
                 it will be wrapped in a dictionary with the key 'task'.
            fq_name: The fully qualified name of the job graph to execute the task against.
                   
        Returns:
            None
            
        Raises:
            ValueError: If the specified job graph cannot be found for the given fq_name.
        """
        pass
        
    @abstractmethod
    def submit_task(self, task: Union[Dict[str, Any], List[Dict[str, Any]]], fq_name: str = None) -> None:
        """
        Submit a task or list of tasks to be processed by the job graph. Must be implemented by subclasses.
        
        Args:
            task: A single task dictionary or a list of task dictionaries to be processed.
                  Each task should contain the necessary input data for the job graph.
                  Optionally, each task dict can contain a "fq_name" key to specify the job graph 
                  to execute the task against, if not provided, the fq_name param will be used.
            fq_name: The fully qualified name of the job graph to execute the task(s) against.
                   If not provided and there is only one job graph, it will be used automatically.
                   Required if multiple job graphs are registered.
                   
        Returns:
            None
            
        Raises:
            ValueError: If fq_name is required but not provided, or if the specified job graph cannot be found.
        """
        pass

    def create_graph_name(self, graph: Dict[str, Dict[str, List[str]]]) -> str:
        """
        Create a default graph name based on head jobs in the graph.
        Head jobs are jobs with no predecessors.

        Args:
            graph: The precedence graph structure

        Returns:
            str: A suitable graph name
        """
        # Find all jobs that are referenced as successors
        all_jobs = set(graph.keys())
        successor_jobs = set()
        for job, details in graph.items():
            successor_jobs.update(details['next'])

        # Head jobs are those not present in any 'next' list
        head_jobs = all_jobs - successor_jobs

        if head_jobs:
            # Use the first head job name
            return next(iter(sorted(head_jobs)))  # Sort for deterministic behavior
        else:
            # Fallback to the first job in the graph if no clear head job
            return next(iter(sorted(graph.keys())))

    def add_dsl(self, dsl: DSLComponent, graph_name: str = "", variant: str = "") -> str:
        """
        Adds a DSL component to the FlowManager. Each DSL component should only be added once
        as it is modified during the process by dsl_to_precedence_graph.

        Args:
            dsl: The DSL component defining the data flow between jobs.
                This DSL will be modified by being converted to a precedence graph.
            graph_name: The name of the graph. Used to generate fully qualified job names.
                If empty, a name will be automatically generated based on head jobs.
            variant: The variant of the graph e.g. "dev", "prod"

        Returns:
            str: The fully qualified name of the head job in the graph, to be used with submit().

        Warning:
            Do not add the same DSL object multiple times, as it will be modified each time.
            Instead, get the fully qualified name (FQ name) from the first call and reuse it.
        """
        if dsl is None:
            raise ValueError("graph cannot be None")

        # Check if this DSL has a tracking attribute to prevent multiple additions
        if hasattr(dsl, "_f4a_already_added") and dsl._f4a_already_added:
            # Try to find the existing graph for this DSL
            for job in self.head_jobs:
                # Check if this job's associated DSL is the same object
                if hasattr(job, "_f4a_source_dsl") and job._f4a_source_dsl is dsl:
                    self.logger.info(f"DSL already added, returning existing FQ name: {job.name}")
                    return job.name

            self.logger.warning(
                f"DSL appears to have been added already but existing graph not found. "
                f"Creating a new graph, which may lead to duplicate processing."
            )

        # Transform the DSL into a precedence graph (this modifies the DSL)
        graph, jobs = dsl_to_precedence_graph(dsl)

        # Mark this DSL as added to prevent multiple additions
        setattr(dsl, "_f4a_already_added", True)
        
        # If no graph_name was provided, generate one based on head jobs
        if not graph_name:
            graph_name = self.create_graph_name(graph)
            self.logger.info(f"Auto-generated graph name: {graph_name}")

        # Check for FQ name collisions from different DSL objects with same structure
        # Create a base name prefix to check for collisions
        base_name_prefix = f"{graph_name}{SPLIT_STR}{variant}"

        # If there's already a job in job_map that would lead to a collision,
        # we need to make this variant name unique by adding a suffix
        variant_suffix = self.find_unique_variant_suffix(base_name_prefix)

        # Add the suffix to the variant if needed
        if variant_suffix:
            self.logger.info(f"Detected potential FQ name collision, adding suffix '{variant_suffix}' to variant")
            enhanced_variant = f"{variant}{variant_suffix}"
        else:
            enhanced_variant = variant

        # Add the graph to our job graph map with potentially modified variant name
        fq_name = self.add_to_job_graph_map(graph, jobs, graph_name, enhanced_variant)

        # Store the reference to the source DSL in the head job
        head_job = self.job_graph_map[fq_name]
        setattr(head_job, "_f4a_source_dsl", dsl)

        return fq_name

    def add_dsl_dict(self, dsl_dict: Dict) -> List[str]:
        """
        Adds multiple graphs to the task manager from a dictionary structure.

        Args:
            dsl_dict: A dictionary containing graph definitions, with optional variants.
                Format can be either:
                {
                    "graph1": {
                        "dev": dsl1d,
                        "prod": dsl1p
                    }
                    "graph2": {
                        "dev": dsl2d,
                        "prod": dsl2p
                    }
                }
                Or without variants:
                {
                    "graph1": dsl1,
                    "graph2": dsl2
                }

        Returns:
            List[str]: The fully qualified names of all added graphs.

        Raises:
            ValueError: If the dictionary structure is invalid or missing required components.
        """
        if not dsl_dict:
            raise ValueError("dsl_dict cannot be None or empty")

        fq_names = []

        for graph_name, graph_data in dsl_dict.items():
            # Check if graph_data is a DSL component directly (no variants)
            if not isinstance(graph_data, dict):
                # No variants, graph_data is the DSL directly
                dsl = graph_data

                fq_name = self.add_dsl(dsl, graph_name)
                fq_names.append(fq_name)
            else:
                # Check if this is a variant structure or old-style direct dsl structure
                if "dsl" in graph_data:
                    # Old format - no variants, direct dsl
                    dsl = graph_data.get("dsl")

                    if dsl is None:
                        raise ValueError(f"Graph '{graph_name}' is missing required 'dsl'")

                    fq_name = self.add_dsl(dsl, graph_name)
                    fq_names.append(fq_name)
                else:
                    # With variants - each key is a variant name, value is the DSL
                    for variant, variant_data in graph_data.items():
                        # Check if variant_data is a dict with 'dsl' key (old format)
                        if isinstance(variant_data, dict) and "dsl" in variant_data:
                            # Old format with nested 'dsl' key
                            dsl = variant_data.get("dsl")

                            if dsl is None:
                                raise ValueError(f"Graph '{graph_name}' variant '{variant}' is missing required 'dsl'")
                        else:
                            # New format - variant_data is the DSL directly
                            dsl = variant_data

                        fq_name = self.add_dsl(dsl, graph_name, variant)
                        fq_names.append(fq_name)

        return fq_names

    def add_to_job_graph_map(self, precedence_graph: PrecedenceGraph, jobs: JobsDict, graph_name: str, variant: str = "") -> str:
        """
        Validates the precedence graph then calls JobFactory.create_job_graph which adds next_jobs and expected_inputs to 
        the job instances, also adds default head and tail jobs to the graph, if necessary.

        Args:
            precedence_graph: A precedence graph that defines the data flow between jobs.
            jobs: A dictionary of jobs.
            graph_name: The name of the graph.
            variant: The variant of the graph e.g. "dev", "pr

        Returns:
            str: The fully qualified name of the graph.
        """
        if not graph_name:
            raise ValueError("graph_name cannot be None or empty")
        if not jobs:
            raise ValueError("jobs cannot be None or empty")
        if precedence_graph is None:
            raise ValueError("precedence_graph cannot be None")
        validate_graph(precedence_graph)
        for (short_job_name, job) in jobs.items():
            job.name = JobABC.create_FQName(graph_name, variant, short_job_name)
        head_job: JobABC = JobFactory.create_job_graph(precedence_graph, jobs)
        self.head_jobs.append(head_job)
        self.job_graph_map.update({job.name: job for job in self.head_jobs})
        return head_job.name

    def find_unique_variant_suffix(self, base_name_prefix: str) -> str:
        """
        Find a unique numeric suffix to append to a variant name to avoid FQ name collisions.
        
        This function checks for existing keys in job_map that start with the given base_name_prefix
        and returns a suffix that will make the name unique when appended to the variant name.
        
        Args:
            job_map: The job map dictionary containing existing job FQ names as keys
            base_name_prefix: The prefix of the FQ name to check for collisions (graph_name$$variant)
            
        Returns:
            str: A numeric suffix (empty string if no collision found, or "_1", "_2", etc.)
        """
        # If no collision in job_map, no suffix needed
        collision_found = False
        for existing_key in self.job_graph_map.keys():
            if existing_key.startswith(base_name_prefix):
                collision_found = True
                break
                
        if not collision_found:
            return ""
            
        # Find existing suffixes by looking at keys with the same base name prefix
        # Extract suffix numbers from variants like "graph_name$$_1$$job_name$$"
        existing_suffixes = set()
        import re

        # Match variants with numeric suffixes in the format "prefix_N$$"
        suffix_pattern = re.compile(re.escape(base_name_prefix) + r'_([0-9]+)\$\$')
        
        for existing_key in self.job_graph_map.keys():
            match = suffix_pattern.match(existing_key)
            if match and match.group(1).isdigit():
                existing_suffixes.add(int(match.group(1)))
        
        # Find the next available suffix number
        suffix_num = 1
        while suffix_num in existing_suffixes:
            suffix_num += 1
        
        return f"_{suffix_num}"

    def create_job_graph_map(self, dsl):
        if isinstance(dsl, Dict):
            self.add_dsl_dict(dsl)
        elif isinstance(dsl, Collection) and not isinstance(dsl, (str, bytes, bytearray)):
            # Handle collections first, before checking for DSLComponent
            if not dsl:  # Check if collection is empty
                raise ValueError("Job collection cannot be empty")

            # Process each item in the collection individually
            for j in dsl:
                # Add the job to DSL if it's a DSLComponent
                if isinstance(j, DSLComponent):  # Check if it's a DSLComponent
                    self.add_dsl(j)
                else:
                    raise TypeError(f"Items in job collection must be DSLComponent instances, got {type(j)}")
        elif isinstance(dsl, DSLComponent):  # Check if it's a DSLComponent
            # Process as a single DSL component
            self.add_dsl(dsl)
        else:
            raise TypeError(f"dsl must be either Dict[str, Any], DSLComponent instance, or Collection of DSLComponent instances, got {type(dsl)}")

    def check_fq_name_and_job_graph_map(self, fq_name, job_map=None):
        """
        Check that the job graph map is not None or empty and sets the fq_name if it is None and there 
        is only one job graph in the map.
        
        Args:
            fq_name: The fully qualified name of the job graph to check.
            job_map: Uses lightweight job_graph_map for multiprocessing mode, or job_graph_map 
            for thread-based single process mode. If None, uses self.job_graph_map.
            
        Returns:
            The fq_name.
            
        Raises:
            ValueError: If job_map is None or empty, or if the specified fq_name does not exist in the map.
        """
        # Use provided job_map or default to self.job_graph_map
        job_map_to_use = job_map if job_map is not None else self.job_graph_map
        
        # Check that job_map is not None or empty
        if not job_map_to_use:
            error_msg = "job_map is None or empty"
            raise ValueError(error_msg)
            
        # If fq_name is None and there's only one job graph in job_map, use that one
        if fq_name is None and len(job_map_to_use) == 1:
            fq_name = next(iter(job_map_to_use))
            self.logger.debug(f"Using the only available job graph: {fq_name}")

        return fq_name



================================================
FILE: src/flow4ai/flowmanagerMP.py
================================================
import asyncio
# In theory it makes sense to use dill with the "multiprocess" package
# instead of pickle with "multiprocessing", but in practice it leads to 
# performance and stability issues.
import multiprocessing as mp
import pickle
import queue
from multiprocessing import freeze_support, set_start_method
from typing import Any, Callable, Dict, List, Optional, Union

from pydantic import BaseModel

from flow4ai.flowmanager_base import FlowManagerABC

from . import f4a_logging as logging
from .dsl import DSLComponent
from .job import JobABC, Task, job_graph_context_manager
from .job_loader import ConfigLoader, JobFactory
from .utils.monitor_utils import should_log_task_stats


class FlowManagerMP(FlowManagerABC):
    """
    FlowManagerMP executes up to thousands of tasks in parallel using one or more Jobs passed into constructor.
    FlowManagerMP is a multiprocessing implementation of FlowManager, so tasks and results are passed between entirely
    different processes, which means that results and tasks must be picklable.
    Optionally passes results to a pre-existing result processing function after task completion.

    Args:
        dsl (Union[Dict[str, Any], DSLComponent, List[DSLComponent]]): If missing, jobs will be loaded from config file.
            Otherwise either a dictionary containing job configuration,
            a single DSLComponent instance, or a list of DSLComponent instances.

        result_processing_function (Optional[Callable[[Any], None]]): Code to handle results after the Job executes its task.
            By default, this hand-off happens in parallel, immediately after a Job processes a task.
            Typically, this function is from an existing codebase that FlowManagerMP is supplementing.
            This function must be picklable, for parallel execution, see serial_processing parameter below.
            This code is not assumed to be asyncio compatible.

        serial_processing (bool, optional): Forces result_processing_function to execute only after all tasks are completed by the Job.
            Enables an unpicklable result_processing_function to be used by setting serial_processing=True.
            However, in most cases changing result_processing_function to be picklable is straightforward and should be the default.
            Defaults to False.
    """
    # Constants
    JOB_MAP_LOAD_TIME = 5  # Timeout in seconds for job map loading

    def __init__(self, dsl: Optional[Any] = None, result_processing_function: Optional[Callable[[Any], None]] = None, 
                 serial_processing: bool = False):
        super().__init__()
        # Get logger for FlowManagerMP
        self.logger = logging.getLogger('FlowManagerMP')
        self.logger.info("Initializing FlowManagerMP")
        if not serial_processing and result_processing_function:
            self._check_picklable(result_processing_function)
        # tasks are created by submit_task(), with [fq_name] added to the task dict
        # tasks are then sent to queue for processing
        self._task_queue: mp.Queue[Task] = mp.Queue()  
        # INTERNAL USE ONLY. DO NOT ACCESS DIRECTLY.
        # This queue is for internal communication between the job executor and result processor.
        # To process results, use the result_processing_function parameter in the FlowManagerMP constructor.
        # See test_result_processing.py for examples of proper result handling.
        self._result_queue = mp.Queue()  # type: mp.Queue
        self.job_executor_process = None
        self.result_processor_process = None
        self._result_processing_function = result_processing_function
        self._serial_processing = serial_processing
        # This holds a map of job name to job, 
        # when _execute is called on the job, the task must have a job_name
        # associated with it, if there is more than one job in the job_map
        #self.job_map: OrderedDict[str, JobABC] = OrderedDict()
        
        # Create a manager for sharing objects between processes
        self._manager = mp.Manager()
        # !! This object allows us to pass the job name map between processes
        #     so we don't have to pickle the entire job map
        self._fq_name_map = self._manager.dict()
        # Create an event to signal when jobs are loaded
        self._jobs_loaded = mp.Event()

        if dsl:
            self.create_job_graph_map(dsl)
            self._fq_name_map.clear()
            self._fq_name_map.update({job.name: job.job_set_str() for job in self.job_graph_map.values()})
        
        self._start()

    # We will not to use context manager as it makes semantics of FlowManagerMP use less flexible
    # def __enter__(self):
    #     """Initialize resources when entering the context."""
    #     self._start()
    #     return self

    # def __exit__(self, exc_type, exc_val, exc_tb):
    #     """Clean up resources when exiting the context."""
    #     self._cleanup()

    # belt and braces, _cleanup is called by _wait_for_completion() via mark_input_completed()
    def __del__(self):
        self._cleanup

    def _cleanup(self):
        """Clean up resources when the object is destroyed."""
        self.logger.info("Cleaning up FlowManagerMP resources")
        
        if self.job_executor_process:
            if self.job_executor_process.is_alive():
                self.logger.debug("Terminating job executor process")
                self.job_executor_process.terminate()
                self.logger.debug("Joining job executor process")
                self.job_executor_process.join()
                self.logger.debug("Job executor process joined")
        
        if self.result_processor_process:
            if self.result_processor_process.is_alive():
                self.logger.debug("Terminating result processor process")
                self.result_processor_process.terminate()
                self.logger.debug("Joining result processor process")
                self.result_processor_process.join()
                self.logger.debug("Result processor process joined")
        
        if hasattr(self, '_task_queue'):
            self.logger.debug("Closing task queue")
            self._task_queue.close()
            self.logger.debug("Joining task queue thread")
            self._task_queue.join_thread()
            self.logger.debug("Task queue thread joined")
        
        if hasattr(self, '_result_queue'):
            self.logger.debug("Closing result queue")
            self._result_queue.close()
            self.logger.debug("Joining result queue thread")
            self._result_queue.join_thread()
            self.logger.debug("Result queue thread joined")
        
        self.logger.debug("Cleanup completed")

    def _check_picklable(self, result_processing_function):
        try:
            # Try to pickle just the function itself
            pickle.dumps(result_processing_function)
            
            # Try to pickle any closure variables
            if hasattr(result_processing_function, '__closure__') and result_processing_function.__closure__:
                for cell in result_processing_function.__closure__:
                    pickle.dumps(cell.cell_contents)
                    
        except Exception as e:
            self.logger.error(f"""Result processing function or its closure variables cannot be pickled: {e}.  
                              Use serial_processing=True for unpicklable functions.""")
            raise TypeError(f"Result processing function must be picklable in parallel mode: {e}")

    
    def _start(self):
        """Start the job executor and result processor processes - non-blocking."""
        self.logger.debug("Starting job executor process")
        self.job_executor_process = mp.Process(
            target=self._async_worker,
            args=(self.job_graph_map, self._task_queue, self._result_queue, self._fq_name_map, self._jobs_loaded, ConfigLoader.directories),
            name="JobExecutorProcess"
        )
        self.job_executor_process.start()
        self.logger.info(f"Job executor process started with PID {self.job_executor_process.pid}")

        if self._result_processing_function and not self._serial_processing:
            self.logger.debug("Starting result processor process")
            self.result_processor_process = mp.Process(
                target=self._result_processor,
                args=(self._result_processing_function, self._result_queue),
                name="ResultProcessorProcess"
            )
            self.result_processor_process.start()
            self.logger.info(f"Result processor process started with PID {self.result_processor_process.pid}")

    # TODO: add resource usage monitoring which returns False if resource use is too high.
    def submit_task(self, task: Union[Dict[str, Any], List[Dict[str, Any]], str], fq_name: Optional[str] = None):
        # Wait for jobs to be loaded and the self._job_name_map to be populated
        if not self._jobs_loaded.wait(timeout=self.JOB_MAP_LOAD_TIME):
            raise TimeoutError("Timed out waiting for jobs to be loaded")

        if task is None:
            self.logger.warning("Received None task, skipping")
            return

        fq_name = self.check_fq_name_and_job_graph_map(fq_name, self._fq_name_map)

        if isinstance(task, list):
            for single_task in task:
                self._submit_single_task(single_task, fq_name)
        else:
            self._submit_single_task(task, fq_name)

    def _submit_single_task(self, task: Union[Dict[str, Any], str], fq_name: str) -> None:
        """Process and submit a single task to the task queue.
        
        Args:
            task: The task to process, either as a dictionary or string
            fq_name: The fully qualified name for the task
        """
        if not isinstance(task, dict):
            task = {'task': str(task)}
        task_obj = Task(task, fq_name)
        # Check the string based _fq_name_map to ensure the fq_name is valid
        # Once the task is sent to the separate process the job graph will be 
        # looked up by the job name
        job_name = self._fq_name_map.get(task_obj.get_fq_name())
        if job_name is None:
            raise ValueError(f"Job not found for fq_name: {task_obj.get_fq_name()}")
        self._task_queue.put(task_obj)  


    def wait_for_completion(self):
        """Signal completion of input and wait for all processes to finish and shut down."""
        self.logger.debug("Marking input as completed")
        self.logger.info("*** task_queue ended ***")
        self._task_queue.put(None)
        self._wait_for_completion()

    # Must be static because it's passed as a target to multiprocessing.Process
    # Instance methods can't be pickled properly for multiprocessing
    # TODO: it may be necessary to put a flag to execute this using asyncio event loops
    #          for example, when handing off to an async web service
    @staticmethod
    def _result_processor(process_fn: Callable[[Any], None], result_queue: 'mp.Queue'):
        """Process that handles processing results as they arrive."""
        logger = logging.getLogger('ResultProcessor')
        logger.debug("Starting result processor")

        while True:
            try:
                result = result_queue.get()
                if result is None:
                    logger.debug("Received completion signal from result queue")
                    break
                logger.debug(f"ResultProcessor received result: {result}")
                try:
                    # Handle both dictionary and non-dictionary results
                    task_id = result.get('task', str(result)) if isinstance(result, dict) else str(result)
                    logger.debug(f"Processing result for task {task_id}")
                    process_fn(result)
                    logger.debug(f"Finished processing result for task {task_id}")
                except Exception as e:
                    logger.error(f"Error processing result: {e}")
                    logger.info("Detailed stack trace:", exc_info=True)
            except queue.Empty:
                continue

        logger.debug("Result processor shutting down")

    def _wait_for_completion(self):
        """Wait for completion of all processing."""
        self.logger.debug("Entering wait for completion")

        if self._result_processing_function and self._serial_processing:
            self._process_serial_results()
        
        # Wait for job executor to finish
        if self.job_executor_process and self.job_executor_process.is_alive():
            self.logger.debug("Waiting for job executor process")
            self.job_executor_process.join()
            self.logger.debug("Job executor process completed")

        # Wait for result processor to finish
        if self.result_processor_process and self.result_processor_process.is_alive():
            self.logger.debug("Waiting for result processor process")
            self.result_processor_process.join()
            self.logger.debug("Result processor process completed")
        
        self._cleanup()

    def _process_serial_results(self):
        while True:
            try:
                self.logger.debug("Attempting to get result from queue")
                result = self._result_queue.get(timeout=0.1)
                if result is None:
                    self.logger.debug("Received completion signal (None) from result queue")
                    self.logger.info("No more results to process.")
                    break
                if self._result_processing_function:
                    try:
                        # Handle both dictionary and non-dictionary results
                        task_id = result.get('task', str(result)) if isinstance(result, dict) else str(result)
                        self.logger.debug(f"Processing result for task {task_id}")
                        self._result_processing_function(result)
                        self.logger.debug(f"Finished processing result for task {task_id}")
                    except Exception as e:
                        self.logger.error(f"Error processing result: {e}")
                        self.logger.info("Detailed stack trace:", exc_info=True)
            except queue.Empty:
                job_executor_is_alive = self.job_executor_process and self.job_executor_process.is_alive()
                self.logger.debug(f"Queue empty, job executor process alive status = {job_executor_is_alive}")
                if not job_executor_is_alive:
                    self.logger.debug("Job executor process is not alive, breaking wait loop")
                    break
                continue

    @staticmethod
    def _replace_pydantic_models(data: Any) -> Any:
        """Recursively replace pydantic.BaseModel instances with their JSON dumps."""
        logger = logging.getLogger('FlowManagerMP')
        logger.debug(f'Processing data type: {type(data)}')

        if isinstance(data, dict):
            return {k: FlowManagerMP._replace_pydantic_models(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [FlowManagerMP._replace_pydantic_models(item) for item in data]
        elif isinstance(data, BaseModel):
            logger.info(f'Converting pydantic model {data.__class__.__name__}')
            return data.model_dump_json()
        return data

    # Must be static because it's passed as a target to multiprocessing.Process
    # Instance methods can't be pickled properly for multiprocessing
    @staticmethod
    def _async_worker(job_graph_map: Dict[str, JobABC], task_queue: 'mp.Queue', result_queue: 'mp.Queue', 
                     job_name_map: 'mp.managers.DictProxy', jobs_loaded: 'mp.Event', 
                     directories: list[str] = []):
        """Process that handles making workflow calls using asyncio."""
        # Get logger for AsyncWorker
        logger = logging.getLogger('AsyncWorker')
        logger.debug("Starting async worker")

        # If job_map is empty, create it from SimpleJobLoader
        if not job_graph_map:
            # logger.info("Creating job map from SimpleJobLoader")
            # job = SimpleJobFactory.load_job({"type": "file", "params": {}})
            # job_map = {job.name: job}
            logger.info("Creating job map from JobLoader")
            logger.info(f"Using directories from process: {directories}")
            ConfigLoader._set_directories(directories)
            ConfigLoader.reload_configs()
            head_jobs = JobFactory.get_head_jobs_from_config()
            job_graph_map = {job.name: job for job in head_jobs}
            # Update the shared job_name_map with each head job's complete set of reachable jobs
            job_name_map.clear()
            job_name_map.update({job.name: job.job_set_str() for job in head_jobs})
            logger.info(f"Created job map with head jobs: {list(job_name_map.keys())}")

        # Signal that jobs are loaded
        jobs_loaded.set()

        async def process_task(task: Task):
            """Process a single task and return its result"""
            task_id = task.task_id  # task_id is not held in the dictionary itself i.e. NOT task['task_id']
            logger.debug(f"[TASK_TRACK] Starting task {task_id}")
            try:
                # If there's only one job, use it directly
                if len(job_graph_map) == 1:
                    job = next(iter(job_graph_map.values()))
                else:
                    # Otherwise, get the job from the map using fq_name
                    fq_name = task.get('fq_name')
                    if not fq_name:
                        raise ValueError("Task missing fq_name when multiple jobs are present")
                    job = job_graph_map[fq_name]
                job_set = JobABC.job_set(job) #TODO: create a map of job to jobset in _async_worker
                async with job_graph_context_manager(job_set):
                    result = await job._execute(task)
                    processed_result = FlowManagerMP._replace_pydantic_models(result)
                    logger.info(f"[TASK_TRACK] Completed task {task_id}, returned by job {processed_result[JobABC.RETURN_JOB]}")

                    result_queue.put(processed_result)
                    logger.debug(f"[TASK_TRACK] Result queued for task {task_id}")
            except Exception as e:
                logger.error(f"[TASK_TRACK] Failed task {task_id}: {e}")
                logger.info("Detailed stack trace:", exc_info=True)
                raise

        async def queue_monitor():
            """Monitor the task queue and create tasks as they arrive"""
            logger.debug("Starting queue monitor")
            tasks = set()
            pending_tasks = []
            tasks_created = 0
            tasks_completed = 0
            end_signal_received = False

            while not end_signal_received or tasks:
                # Get all available tasks from the queue
                while True:
                    try:
                        task = task_queue.get_nowait()
                        if task is None:
                            logger.info("Received end signal in task queue")
                            end_signal_received = True
                            break
                        pending_tasks.append(task)
                    except queue.Empty:
                        break

                # Create tasks in batch if we have any pending
                if pending_tasks:
                    logger.debug(f"Creating {len(pending_tasks)} new tasks")
                    new_tasks = {asyncio.create_task(process_task(pending_tasks[i])) for i in range(len(pending_tasks))}
                    tasks.update(new_tasks)
                    tasks_created += len(new_tasks)
                    logger.debug(f"Total tasks created: {tasks_created}")
                    pending_tasks.clear()

                # Clean up completed tasks
                done_tasks = {t for t in tasks if t.done()}
                if done_tasks:
                    for done_task in done_tasks:
                        try:
                            # Check if task raised an exception
                            exc = done_task.exception()
                            if exc:
                                logger.error(f"Task failed with exception: {exc}")
                                logger.info("Detailed stack trace:", exc_info=True)
                        except asyncio.InvalidStateError:
                            pass  # Task was cancelled or not done
                    tasks_completed += len(done_tasks)
                    logger.debug(f"Cleaned up {len(done_tasks)} completed tasks. Total completed: {tasks_completed}")
                    logger.debug(f"Active tasks remaining: {len(tasks)}")
                tasks.difference_update(done_tasks)

                # Log task stats periodically
                if tasks_completed != 0 and tasks_completed % 5 == 0:
                    if should_log_task_stats(queue_monitor, tasks_created, tasks_completed):
                        logger.info(f"Tasks stats - Created: {tasks_created}, Completed: {tasks_completed}, Active: {len(tasks)}")

                # A short pause to reduce CPU usage and avoid a busy-wait state.             
                await asyncio.sleep(0.0001)

            # Wait for remaining tasks to complete
            if tasks:
                logger.debug(f"Waiting for {len(tasks)} remaining tasks")
                await asyncio.gather(*tasks)
                logger.debug("All remaining tasks completed")

            # Signal completion
            logger.debug("Sending completion signal to result queue")
            logger.debug(f"Final stats - Created: {tasks_created}, Completed: {tasks_completed}")
            logger.info("*** result_queue ended ***")
            result_queue.put(None)

        # Run the event loop
        logger.debug("Creating event loop")
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            logger.debug("Running queue monitor")
            loop.run_until_complete(queue_monitor())
        except Exception as e:
            import traceback
            logger.error(f"Error in async worker: {e}\n{traceback.format_exc()}")
            logger.info("Detailed stack trace:", exc_info=True)
        finally:
            logger.info("Closing event loop")
            loop.close()

    def get_fq_names(self) -> list[str]:
        """
        Returns a list of fully qualified job names after ensuring the fq_name_map is loaded.

        Returns:
            list[str]: List of fully qualified job names from the fq_name_map

        Raises:
            TimeoutError: If waiting for jobs to be loaded exceeds timeout
        """
        self.logger.debug("Waiting for jobs to be loaded before returning job names")
        if not self._jobs_loaded.wait(timeout=self.JOB_MAP_LOAD_TIME):
            raise TimeoutError("Timed out waiting for jobs to be loaded")
        
        return list(self._fq_name_map.keys())


class FlowManagerMPFactory:
    _instance = None
    _flowmanagerMP = None

    def __init__(self, *args, **kwargs):
        if not FlowManagerMPFactory._instance:
            self._flowmanagerMP = FlowManagerMP(*args, **kwargs)
            FlowManagerMPFactory._instance = self

    @classmethod
    def init(cls, start_method="spawn", *args, **kwargs):
      """
      Initializes the FlowManagerMPFactory using the given start method.
      args and kwargs are passed down to the FlowManagerMP constructor.

      Args:
        start_method: The start method of multiprocessing. Defaults to "spawn".
        args: The parameters to be passed to the FlowManagerMP's constructor
        kwargs: The keyword parameters to be passed to the FlowManagerMP's constructor
        
      """
      freeze_support()
      set_start_method(start_method)
      if not cls._instance:
        cls._instance = cls(*args, **kwargs)
      return cls._instance

    @staticmethod
    def get_instance()->FlowManagerMP:
        if not FlowManagerMPFactory._instance:
            raise RuntimeError("FlowManagerMPFactory not initialized")
        return FlowManagerMPFactory._instance._flowmanagerMP



================================================
FILE: src/flow4ai/graph_pic.py
================================================
"""
Graph visualization module for Flow4AI.

This module provides functionality to visualize job graphs from adjacency list
representations using NetworkX and Matplotlib. It can generate visual representations
of job dependencies and workflows.
"""

import os
import tempfile
from collections import defaultdict
from typing import Any, Dict, List, Optional, Set, Tuple, Union

import matplotlib.pyplot as plt
import networkx as nx
import numpy as np

# Define some color schemes for the graphs
COLOR_SCHEMES = {
    "default": {
        "node_color": "#1f78b4",  # Blue
        "edge_color": "#333333",  # Dark Gray
        "font_color": "black",
        "border_color": "#000000",  # Black
    },
    "light": {
        "node_color": "#a6cee3",  # Light Blue
        "edge_color": "#666666",  # Gray
        "font_color": "black",
        "border_color": "#000000",  # Black
    },
    "dark": {
        "node_color": "#1f78b4",  # Blue
        "edge_color": "#cccccc",  # Light Gray
        "font_color": "white",
        "border_color": "#ffffff",  # White
        "background": "#333333",  # Dark Gray
    },
    "colorful": {
        "node_color": "#b2df8a",  # Light Green
        "edge_color": "#33a02c",  # Green
        "font_color": "black",
        "border_color": "#000000",  # Black
    }
}


def adjacency_to_nx_graph(graph_definition: Dict[str, Any]) -> nx.DiGraph:
    """
    Convert a Flow4AI adjacency list to a NetworkX directed graph.
    
    Args:
        graph_definition: Dictionary representing the graph as an adjacency list.
                         Format: {node_id: {"next": [list_of_next_node_ids]}}
    
    Returns:
        A NetworkX DiGraph object representing the graph.
    """
    G = nx.DiGraph()
    
    # Add all nodes first
    for node_id in graph_definition:
        G.add_node(node_id)
    
    # Add all edges
    for node_id, edges in graph_definition.items():
        next_nodes = edges.get("next", [])
        for next_node in next_nodes:
            G.add_edge(node_id, next_node)
    
    return G


def get_topological_generations(G: nx.DiGraph) -> List[List[str]]:
    """
    Get nodes arranged in topological generations (levels).
    Each generation contains nodes that have the same "distance" from the source nodes.
    
    Args:
        G: A NetworkX DiGraph object
        
    Returns:
        List of lists where each inner list contains nodes in the same generation
    """
    # Find root nodes (nodes with no incoming edges)
    root_nodes = [n for n, d in G.in_degree() if d == 0]
    
    if not root_nodes:
        # If no root found, just pick a random node
        root_nodes = [list(G.nodes())[0]]
    
    # Initialize generations
    generations = [root_nodes]
    visited = set(root_nodes)
    
    # Continue until all nodes are assigned to a generation
    while len(visited) < len(G.nodes()):
        # Get the last generation
        last_gen = generations[-1]
        
        # Find all successors of nodes in the last generation
        # that haven't been visited yet
        next_gen = []
        for node in last_gen:
            for succ in G.successors(node):
                if succ not in visited:
                    next_gen.append(succ)
                    visited.add(succ)
        
        # If we found new nodes, add them to the generations
        if next_gen:
            generations.append(next_gen)
        else:
            # Find any remaining unvisited nodes and add them to a new generation
            # This handles disconnected components
            remaining = [n for n in G.nodes() if n not in visited]
            if remaining:
                generations.append(remaining)
                visited.update(remaining)
            else:
                break
    
    return generations


def identify_paths(G: nx.DiGraph) -> Dict[str, List[List[str]]]:
    """
    Identify all paths from source nodes to sink nodes in the graph.
    This helps in maintaining straight-line paths in the visualization.
    
    Args:
        G: A NetworkX DiGraph object
        
    Returns:
        Dictionary mapping node IDs to the paths they belong to
    """
    # Find source nodes (nodes with no incoming edges)
    sources = [n for n, d in G.in_degree() if d == 0]
    # Find sink nodes (nodes with no outgoing edges)
    sinks = [n for n, d in G.out_degree() if d == 0]
    
    # Get all paths from each source to each sink
    all_paths = []
    for source in sources:
        for sink in sinks:
            try:
                # Find all simple paths between source and sink
                paths = list(nx.all_simple_paths(G, source, sink))
                all_paths.extend(paths)
            except nx.NetworkXNoPath:
                continue
    
    # If we couldn't find any paths, return an empty dictionary
    if not all_paths:
        return {}
    
    # Map each node to the paths it belongs to
    node_paths = {}
    for i, path in enumerate(all_paths):
        for node in path:
            if node not in node_paths:
                node_paths[node] = []
            node_paths[node].append((i, path))
    
    return node_paths

def custom_hierarchical_layout(G: nx.DiGraph) -> Dict[str, Tuple[float, float]]:
    """
    A custom hierarchical layout that arranges nodes in levels based on topology,
    with special attention to maintaining straight paths, minimizing edge crossings,
    and creating visually balanced layouts.
    
    Key features:
    - Head nodes are positioned equidistant between their child nodes
    - Tail nodes are positioned at the rightmost side
    - The algorithm minimizes edge crossings using node relationships and path analysis
    - Nodes that belong to the same path are vertically aligned when possible
    - Nodes are positioned based on the graph structure, not the arbitrary dict order
    
    Args:
        G: A NetworkX DiGraph object
        
    Returns:
        Dictionary mapping node names to (x, y) coordinates
    """
    # Get nodes arranged in topological generations
    generations = get_topological_generations(G)
    
    # Identify source nodes (no incoming edges) and sink nodes (no outgoing edges)
    source_nodes = [n for n, d in G.in_degree() if d == 0]
    sink_nodes = [n for n, d in G.out_degree() if d == 0]
    
    # Calculate positions
    pos = {}
    
    # First pass: assign x positions based on generation
    for i, gen in enumerate(generations):
        for node in gen:
            # Standard x position is determined by generation index (level)
            x = i
            
            # Special case: ensure sink nodes are positioned at the rightmost side
            if node in sink_nodes:
                x = len(generations) # Put at the far right
                
            pos[node] = (x, 0)  # Y will be assigned later
    
    # Group nodes by their x position/level
    x_groups = {}
    for node, (x, _) in pos.items():
        if x not in x_groups:
            x_groups[x] = []
        x_groups[x].append(node)
    
    # Calculate the longest path through each node (used for vertical ordering)
    path_lengths = {}
    for node in G.nodes():
        # Calculate distance from source
        if node in source_nodes:
            path_lengths[node] = 0
        else:
            # Find the maximum path length from any source to this node
            pred_lengths = [path_lengths.get(p, 0) + 1 for p in G.predecessors(node) if p in path_lengths]
            path_lengths[node] = max(pred_lengths) if pred_lengths else 0
    
    # Sort nodes within each level based on their network position and connectivity patterns
    for level in sorted(x_groups.keys()):
        nodes = x_groups[level]
        
        # For the first level (source nodes), use a special ordering to create visual balance
        if level == 0:
            # Sort by number of outgoing connections (more central nodes in the middle)
            source_nodes_sorted = sorted(source_nodes, key=lambda n: (len(list(G.successors(n))), str(n)))
            
            # Apply a more balanced ordering - place nodes with more connections in the middle
            # This creates a more aesthetically pleasing and balanced layout
            balanced_nodes = []
            left = 0
            right = len(source_nodes_sorted) - 1
            position = 0  # 0 = middle, 1 = left, 2 = right, and alternating
            
            while left <= right:
                if position == 0:  # Middle, take from right (higher connection count)
                    balanced_nodes.append(source_nodes_sorted[right])
                    right -= 1
                    position = 1
                elif position == 1:  # Left
                    balanced_nodes.append(source_nodes_sorted[left])
                    left += 1
                    position = 2
                else:  # Right
                    balanced_nodes.append(source_nodes_sorted[right])
                    right -= 1
                    position = 1
            
            # Update the nodes in this level with the balanced ordering
            x_groups[level] = balanced_nodes
            continue
            
        # For other levels, use a more sophisticated sorting approach to minimize crossings
        def node_sort_key(node):
            # Get predecessors and successors
            preds = list(G.predecessors(node))
            succs = list(G.successors(node))
            
            # Calculate average positions of predecessors (for vertical alignment)
            pred_positions = []
            for p in preds:
                if p in pos:
                    # Get position of predecessor in its group
                    p_level = pos[p][0]
                    if p_level in x_groups and p in x_groups[p_level]:
                        # Use normalized position (0 to 1 range)
                        p_idx = x_groups[p_level].index(p) 
                        p_norm_pos = p_idx / max(1, len(x_groups[p_level]) - 1)
                        pred_positions.append(p_norm_pos)
            
            # If no predecessor positions, use path length for ordering
            if not pred_positions:
                # Handle edge case where all nodes are both source and sink (all path lengths are 0)
                max_path_value = max(path_lengths.values()) if path_lengths else 0
                pred_position = 0 if max_path_value == 0 else path_lengths.get(node, 0) / max_path_value
            else:
                pred_position = sum(pred_positions) / len(pred_positions)
                
            # Return sorting criteria tuple
            return (pred_position, path_lengths.get(node, 0), len(succs), len(preds), str(node))
        
        # Sort the nodes in this level
        x_groups[level] = sorted(nodes, key=node_sort_key)
    
    # Second pass: assign y positions
    # Track vertical slots for each column to prevent overlap
    x_slots = {x: set() for x in x_groups}
    
    # Process source nodes specially to center them between their children
    for node in source_nodes:
        # Find direct children
        children = list(G.successors(node))
        if children:
            # If this source node has children, we position it equidistant from them
            # First, make sure the children have y-positions assigned
            child_level = pos[children[0]][0]  # All children should be at the same level
            
            # Get existing y-positions of the children, if any
            child_ys = [pos[child][1] for child in children if pos[child][1] != 0]
            
            if child_ys:
                # Center the source node between its children
                center_y = sum(child_ys) / len(child_ys)
            else:
                # If children don't have positions yet, use a centered position
                # This will be adjusted later when child positions are calculated
                center_y = 0
                
            # Assign the position
            x, _ = pos[node]
            pos[node] = (x, center_y)
    
    # Assign y positions to nodes based on sorted order within levels
    for level, nodes in x_groups.items():
        # Set y-coordinates based on the index within sorted nodes
        for i, node in enumerate(nodes):
            x, _ = pos[node]
            pos[node] = (x, i)
    
    # Refine positions to create better vertical alignment and distribution
    # Create a map to track used Y positions in each column
    x_slots = {x: set() for x in x_groups}
    
    # Second pass: refine y positions to create better alignment
    for level in sorted(x_groups.keys()):
        for node in x_groups[level]:
            # Skip nodes that already have been processed
            if node in source_nodes and level == 0:
                continue
            
            # Try to align with predecessors
            aligned_y = None
            preds = list(G.predecessors(node))
            
            if preds:
                # Try to use the average y position of predecessors
                pred_ys = [pos[p][1] for p in preds if p in pos]
                if pred_ys:
                    aligned_y = sum(pred_ys) / len(pred_ys)
            
            # If unable to align with predecessors, try with successors
            if aligned_y is None:
                succs = list(G.successors(node))
                succ_ys = [pos[s][1] for s in succs if s in pos and pos[s][1] != 0]
                if succ_ys:
                    aligned_y = sum(succ_ys) / len(succ_ys)
            
            # If we still don't have a position, keep the original one
            if aligned_y is None:
                aligned_y = pos[node][1]
            
            # Check for conflicts with existing positions and adjust
            while any(abs(aligned_y - existing) < 0.8 for existing in x_slots[level]):
                aligned_y += 0.8
            
            # Mark this position as used
            x_slots[level].add(aligned_y)
            
            # Update the position
            x, _ = pos[node]
            pos[node] = (x, aligned_y)
    
    # Special pass for source nodes: center them between their children
    for node in source_nodes:
        children = list(G.successors(node))
        if children:
            # Get positions of all children with valid y-coordinates
            child_ys = [pos[child][1] for child in children if child in pos]
            
            if child_ys:
                # Center the source node between its children
                center_y = sum(child_ys) / len(child_ys)
                x, _ = pos[node]
                pos[node] = (x, center_y)
    
    # Normalize the positions
    x_values = [p[0] for p in pos.values()]
    y_values = [p[1] for p in pos.values()]
    
    # Get min/max for normalization
    x_min, x_max = min(x_values), max(x_values)
    y_min, y_max = min(y_values), max(y_values)
    
    # Avoid division by zero
    x_range = x_max - x_min if x_max > x_min else 1
    y_range = y_max - y_min if y_max > y_min else 1
    
    # Normalize to [0,1] range and apply spacing
    normalized_pos = {}
    for node, (x, y) in pos.items():
        norm_x = (x - x_min) / x_range if x_range else 0.5
        norm_y = (y - y_min) / y_range if y_range else 0.5
        normalized_pos[node] = (norm_x * 1.5, norm_y * 1.5)  # Add scaling for clarity
    
    return normalized_pos


def visualize_graph(graph_definition: Dict[str, Any],
                   title: Optional[str] = None,
                   save_path: Optional[str] = None) -> Union[plt.Figure, str]:
    """
    Visualize a graph with automatically tuned parameters based on graph characteristics.
    This function analyzes the graph structure and selects optimal visualization parameters.
    
    Key features:
    - Automatically selects appropriate node size, edge width, and font size based on graph size
    - Uses the hierarchical layout by default for clear visualization of process flow
    - Adjusts figure size based on graph complexity and node count
    
    Args:
        graph_definition: Dictionary representing the graph as an adjacency list.
                         Format: {node_id: {"next": [list_of_next_node_ids]}}
        title: Optional title for the plot
        save_path: Optional path to save the figure
        
    Returns:
        If show is True, returns the figure. If save_path is provided, returns the save path.
    """
    # Create a NetworkX graph to analyze structure
    G = adjacency_to_nx_graph(graph_definition)
    
    # Analyze graph characteristics to determine optimal parameters
    node_count = len(G.nodes())
    edge_count = len(G.edges())
    max_connections = max([max(G.out_degree(n), G.in_degree(n)) for n in G.nodes()]) if G.nodes() else 0
    
    # Auto-tune parameters based on graph size and complexity
    if node_count <= 5:  # Small graph
        node_size = 1800
        edge_width = 2.0
        font_size = 14
        figsize = (10, 7)
    elif node_count <= 10:  # Medium graph
        node_size = 1500
        edge_width = 1.8
        font_size = 12
        figsize = (12, 8)
    elif node_count <= 20:  # Large graph
        node_size = 1200
        edge_width = 1.5
        font_size = 11
        figsize = (14, 9)
    else:  # Very large graph
        node_size = 800
        edge_width = 1.0
        font_size = 10
        figsize = (16, 10)
    
    # Adjust for graphs with many connections
    if max_connections > 5:
        node_size = max(600, node_size - 200)  # Reduce node size for highly connected graphs
        figsize = (figsize[0] + 2, figsize[1] + 1)  # Increase figure size
    
    # For single-level graphs with no connections, adjust layout
    if edge_count == 0:
        node_size = 1500  # Larger nodes for better visibility
        figsize = (10, 6)  # Smaller figure (less space needed)
    
    # Call the detailed function with optimized parameters
    return visualize_graph_detail(
        graph_definition=graph_definition,
        layout="hierarchical",  # Always use hierarchical layout for best results
        node_size=node_size,
        edge_width=edge_width,
        font_size=font_size,
        figsize=figsize,
        title=title,
        save_path=save_path,
        show=True  # Always show by default
    )


def visualize_graph_detail(graph_definition: Dict[str, Any], 
                        layout: str = "hierarchical", 
                        color_scheme: str = "default",
                        node_size: int = 1500,
                        title: Optional[str] = None,
                        figsize: Tuple[int, int] = (12, 8),
                        dpi: int = 100,
                        node_shape: str = 'o',
                        show_labels: bool = True,
                        font_size: int = 12,
                        edge_width: float = 1.5,
                        save_path: Optional[str] = None,
                        show: bool = True) -> Union[plt.Figure, str]:
    """
    Visualize a graph defined by an adjacency list using NetworkX and Matplotlib.
    
    Args:
        graph_definition: Dictionary representing the graph as an adjacency list.
                         Format: {node_id: {"next": [list_of_next_node_ids]}}
        layout: Layout algorithm to use:
                - 'hierarchical' (default): Custom hierarchical layout with left-to-right flow
                - 'dot', 'neato', 'fdp', 'sfdp', 'twopi', 'circo': Graphviz layouts
                - 'spring', 'circular', 'random', 'shell', 'spectral': NetworkX layouts
        color_scheme: Color scheme to use ('default', 'light', 'dark', 'colorful')
        node_size: Size of the nodes in the visualization
        title: Optional title for the plot
        figsize: Figure size as a tuple (width, height)
        dpi: DPI for the figure
        node_shape: Shape of the nodes ('o', 's', 'D', 'v', '^', '<', '>', 'p', 'h', '8')
        show_labels: Whether to show labels on nodes
        font_size: Font size for node labels
        edge_width: Width of the edges
        save_path: Optional path to save the figure
        show: Whether to display the figure
        
    Returns:
        If show is True, returns the figure. If save_path is provided, returns the save path.
    """
    # Create a NetworkX graph from the adjacency list
    G = adjacency_to_nx_graph(graph_definition)
    
    # If there are no nodes, return early
    if not G.nodes():
        print("Graph has no nodes to visualize.")
        return plt.figure(figsize=figsize, dpi=dpi)
    
    # Get the color scheme
    colors = COLOR_SCHEMES.get(color_scheme, COLOR_SCHEMES["default"])
    
    # Create figure
    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)
    
    # Set background color if specified
    if "background" in colors:
        ax.set_facecolor(colors["background"])
        fig.set_facecolor(colors["background"])
    
    # Get the layout positions
    if layout == 'hierarchical':
        # Use our custom hierarchical layout
        pos = custom_hierarchical_layout(G)
    elif layout in ['dot', 'neato', 'fdp', 'sfdp', 'twopi', 'circo']:
        # Use graphviz layout if available
        try:
            pos = nx.nx_agraph.graphviz_layout(G, prog=layout)
        except (ImportError, Exception) as e:
            print(f"Could not use {layout} layout, using custom hierarchical layout instead: {e}")
            pos = custom_hierarchical_layout(G)
    else:
        # Use NetworkX layout
        if layout == 'spring':
            pos = nx.spring_layout(G, seed=42)
        elif layout == 'circular':
            pos = nx.circular_layout(G)
        elif layout == 'random':
            pos = nx.random_layout(G, seed=42)
        elif layout == 'shell':
            pos = nx.shell_layout(G)
        elif layout == 'spectral':
            pos = nx.spectral_layout(G)
        else:
            # Default to our custom hierarchical layout
            pos = custom_hierarchical_layout(G)
    
    # Draw nodes
    nx.draw_networkx_nodes(
        G, pos,
        node_color=colors["node_color"],
        node_size=node_size,
        node_shape=node_shape,
        edgecolors=colors["border_color"],
        linewidths=2,
        ax=ax,
    )
    
    # Draw edges with more prominent arrows
    nx.draw_networkx_edges(
        G, pos,
        edge_color=colors["edge_color"],
        width=edge_width,
        arrowsize=30,  # Larger arrows for better visibility
        arrowstyle='-|>', 
        connectionstyle='arc3,rad=0.0',  # Straight connection lines
        min_source_margin=10,  # Margin from source node
        min_target_margin=15,  # Margin from target node to better show arrows
        ax=ax,
    )
    
    # Draw labels if requested
    if show_labels:
        nx.draw_networkx_labels(
            G, pos,
            font_size=font_size,
            font_color=colors["font_color"],
            ax=ax,
        )
    
    # Set the title if provided
    if title:
        plt.title(title, color=colors.get("font_color", "black"), fontsize=font_size + 4)
    
    # Remove axes
    plt.axis('off')
    
    # Tight layout for better spacing
    plt.tight_layout()
    
    # Save the figure if requested
    if save_path:
        plt.savefig(save_path, dpi=dpi, bbox_inches='tight', facecolor=fig.get_facecolor())
        print(f"Graph saved to {save_path}")
        
    # Show the figure if requested
    if show:
        plt.show()
    
    # Return the figure or save path
    if save_path:
        return save_path
    return fig


def save_graph_as_temp_image(graph_definition: Dict[str, Any], 
                            format: str = 'png', 
                            **kwargs) -> str:
    """
    Save a graph as a temporary image file and return the path.
    
    Args:
        graph_definition: Dictionary representing the graph as an adjacency list.
        format: Image format ('png', 'svg', 'pdf', 'jpg')
        **kwargs: Additional arguments to pass to visualize_graph
        
    Returns:
        Path to the temporary image file
    """
    # Create a temporary file
    fd, path = tempfile.mkstemp(suffix=f'.{format}')
    os.close(fd)  # Close the file descriptor
    
    # Set show to False to prevent displaying the graph
    kwargs['show'] = False
    kwargs['save_path'] = path
    
    # Visualize the graph and save it
    visualize_graph(graph_definition, **kwargs)
    
    return path


def visualize_to_display(graph_definition: Dict[str, Any],
                        width: int = 800,
                        height: int = 600,
                        **kwargs) -> None:
    """
    Visualize a graph and display it using IPython display (for Jupyter notebooks).
    
    Args:
        graph_definition: Dictionary representing the graph as an adjacency list.
        width: Width of the displayed image
        height: Height of the displayed image
        **kwargs: Additional arguments to pass to visualize_graph
    """
    try:
        from IPython.display import Image, display

        # Save the graph as a temporary image
        path = save_graph_as_temp_image(graph_definition, **kwargs)
        
        # Display the image
        display(Image(filename=path, width=width, height=height))
        
        # Clean up the temporary file
        try:
            os.unlink(path)
        except:
            pass
            
    except ImportError:
        print("IPython not available. Use visualize_graph() instead.")
        visualize_graph(graph_definition, **kwargs)


def compare_layouts(graph_definition: Dict[str, Any],
                   layouts: List[str] = None,
                   color_scheme: str = "default",
                   node_size: int = 1000,
                   figsize: Tuple[int, int] = (15, 10),
                   save_path: Optional[str] = None) -> plt.Figure:
    """
    Compare different layouts for the same graph.
    
    Args:
        graph_definition: Dictionary representing the graph as an adjacency list.
        layouts: List of layout algorithms to compare
        color_scheme: Color scheme to use
        node_size: Size of the nodes
        figsize: Figure size
        save_path: Optional path to save the figure
        
    Returns:
        Matplotlib figure with subplots for each layout
    """
    if layouts is None:
        layouts = ['hierarchical', 'dot', 'spring', 'circular', 'spectral']
    
    # Create figure
    num_layouts = len(layouts)
    rows = (num_layouts + 2) // 3  # 3 per row, rounded up
    cols = min(3, num_layouts)
    
    fig, axes = plt.subplots(rows, cols, figsize=figsize)
    if rows * cols == 1:
        axes = np.array([[axes]])
    elif rows == 1 or cols == 1:
        axes = axes.reshape(-1, 1) if cols == 1 else axes.reshape(1, -1)
        
    # Flatten axes for easier indexing
    axes_flat = axes.flatten()
    
    # Get the color scheme
    colors = COLOR_SCHEMES.get(color_scheme, COLOR_SCHEMES["default"])
    
    # Create the graph
    G = adjacency_to_nx_graph(graph_definition)
    
    # Draw each layout
    for i, layout in enumerate(layouts):
        if i < len(axes_flat):
            ax = axes_flat[i]
            
            # Get layout positions
            try:
                if layout == 'hierarchical':
                    pos = custom_hierarchical_layout(G)
                elif layout in ['dot', 'neato', 'fdp', 'sfdp', 'twopi', 'circo']:
                    pos = nx.nx_agraph.graphviz_layout(G, prog=layout)
                else:
                    if layout == 'spring':
                        pos = nx.spring_layout(G, seed=42)
                    elif layout == 'circular':
                        pos = nx.circular_layout(G)
                    elif layout == 'random':
                        pos = nx.random_layout(G, seed=42)
                    elif layout == 'shell':
                        pos = nx.shell_layout(G)
                    elif layout == 'spectral':
                        pos = nx.spectral_layout(G)
                    else:
                        pos = custom_hierarchical_layout(G)
            except Exception as e:
                print(f"Layout '{layout}' failed: {e}. Using custom hierarchical layout.")
                pos = custom_hierarchical_layout(G)
                
            # Draw graph with more prominent arrows
            nx.draw_networkx(
                G, pos,
                node_color=colors["node_color"],
                node_size=node_size,
                edge_color=colors["edge_color"],
                font_color=colors["font_color"],
                ax=ax,
                arrows=True,
                arrowstyle='-|>',
                arrowsize=20,  # Larger arrowsize for better visibility
                font_size=10,
                font_weight='bold',
                connectionstyle='arc3,rad=0.0',  # Straight connection lines
                min_source_margin=15,  # Margin from source node
                min_target_margin=15,  # Margin from target node
            )
            
            # Set title
            ax.set_title(layout.capitalize(), fontsize=12, fontweight='bold')
            
            # Turn off axis
            ax.axis('off')
    
    # Hide any unused subplots
    for i in range(len(layouts), len(axes_flat)):
        axes_flat[i].axis('off')
        
    # Adjust layout
    plt.tight_layout()
    
    # Save if requested
    if save_path:
        plt.savefig(save_path, bbox_inches='tight')
        print(f"Comparison saved to {save_path}")
    
    # Show the plot
    plt.show()
    
    return fig


# Example usage function
def example_usage():
    """Example of how to use the graph_pic module."""
    # Example graph definition
    graph_definition = {
        "1": {"next": ["2", "3", "4", "5"]},
        "2": {"next": ["6"]},
        "3": {"next": ["7"]},
        "4": {"next": ["7"]},
        "5": {"next": ["7"]},
        "6": {"next": ["8"]},
        "7": {"next": ["9"]},
        "8": {"next": ["10"]},
        "9": {"next": ["11"]},
        "10": {"next": ["11"]},
        "11": {"next": []}
    }
    
    # Basic visualization using our custom hierarchical layout
    visualize_graph(graph_definition, 
                   layout="hierarchical",
                   title="Precedence Graph", 
                   node_size=1200,
                   edge_width=2.0)
    
    # Compare different layouts
    compare_layouts(graph_definition, 
                   layouts=['hierarchical', 'spring', 'circular', 'spectral'])
    
    # For Jupyter Notebooks
    # visualize_to_display(graph_definition, layout='hierarchical', color_scheme='default')
    
    return "Example completed"


if __name__ == "__main__":
    example_usage()



================================================
FILE: src/flow4ai/job.py
================================================
import asyncio
import uuid
from abc import ABC, ABCMeta, abstractmethod
from contextlib import asynccontextmanager
from contextvars import ContextVar
from typing import Any, Dict, Optional, Type, Union

from . import f4a_logging as logging
from .utils.otel_wrapper import trace_function

SPLIT_STR = "$$"


# DSL imports moved inline to avoid circular imports


def _is_traced(method):
    """Helper function to check if a method is traced."""
    return hasattr(method, '_is_traced') and method._is_traced


def _has_own_traced_execute(cls):
    """Helper function to check if a class has its own traced _execute (not inherited)."""
    return '_execute' in cls.__dict__ and _is_traced(cls.__dict__['_execute'])


def _mark_traced(method):
    """Helper function to mark a method as traced."""
    method._is_traced = True
    return method


def traced_job(cls: Type) -> Type:
    """
    Class decorator that ensures the execute method is traced.
    This is only applied to the JobABC class itself.
    """
    if hasattr(cls, '_execute'):
        original_execute = cls._execute
        traced_execute = trace_function(original_execute, detailed_trace=True)
        traced_execute = _mark_traced(traced_execute)
        # Store original as executeNoTrace
        cls.executeNoTrace = original_execute
        # Replace execute with traced version
        cls._execute = traced_execute
    return cls


class JobMeta(ABCMeta):
    """Metaclass that automatically applies the traced_job decorator to JobABC only."""
    def __new__(mcs, name, bases, namespace):
        cls = super().__new__(mcs, name, bases, namespace)
        if name == 'JobABC':  # Only decorate the JobABC class itself
            return traced_job(cls)
        # For subclasses, ensure they inherit JobABC's traced _execute
        if '_execute' in namespace:
            # If subclass defines its own _execute, ensure it's not traced again
            # but still inherits the tracing from JobABC
            del namespace['_execute']
            cls = super().__new__(mcs, name, bases, namespace)
        return cls


class Task(dict):
    """A task dictionary with a unique identifier.
    
    Args:
        data (Union[Dict[str, Any], str]): 
            The task data as a dictionary or string. If a string,
            it will be converted to a dictionary with a 'task' key.
        fq_name (Optional[str], optional): 
            The name of the job graph that will process this task.
            Required if there is more than one job graph in the
            FlowManagerMP class. If "fq_name" key is already in
            the data dictionary, it will be used instead of the
            fq_name parameter.
    """
    def __init__(self, data: Dict[str, Any], fq_name: Optional[str] = None):
        # Convert string input to dict
        if isinstance(data, dict):
            data = data.copy()  # Create a copy to avoid modifying the original
        else:
            raise ValueError("Task data must be a dictionary")
        
        super().__init__(data)
        self.task_id:str = str(uuid.uuid4())
        if fq_name is not None and self.get('fq_name') is None:
            self['fq_name'] = fq_name

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, Task):
            return NotImplemented
        return self.task_id == other.task_id
    
    def get_fq_name(self) -> str:
        return self.get('fq_name')

    # mypy highlights this as an error because dicts are mutable
    #   and so not hashable, but I want each Task to have a unique id
    #   so it is hashable.
    def __hash__(self) -> int:
        return hash(self.task_id)

    def __repr__(self) -> str:
        fq_name = self.get('fq_name', 'None')
        task_preview = str(dict(self))[:50] + '...' if len(str(dict(self))) > 50 else str(dict(self))
        return f"Task(id={self.task_id}, fq_name={fq_name}, data={task_preview})"

class JobState:
  def __init__(self):
      self.inputs: Dict[str, Dict[str, Any]] = {}
      self.input_event = asyncio.Event()
      self.execution_started = False

job_graph_context : ContextVar[dict] = ContextVar('job_graph_context')

@asynccontextmanager
async def job_graph_context_manager(job_set: set['JobABC']):
  """Create a new context for job execution, with a new JobState."""
  new_state = {}
  for job in job_set:
      new_state[job.name] = JobState()
  new_state[JobABC.CONTEXT] = {}
  new_state[JobABC.CONTEXT][JobABC.SAVED_RESULTS] = {}
  token = job_graph_context.set(new_state)
  try:
      yield new_state
  finally:
      job_graph_context.reset(token)

class JobABC(ABC, metaclass=JobMeta):
    """
    Abstract base class for jobs. Only this class will have tracing enabled through the JobMeta metaclass.
    Subclasses will inherit the traced version of _execute but won't add additional tracing.
    """

    # class variable to keep track of instance counts for each class
    _instance_counts: Dict[Type, int] = {}
    
    # Key used to pass task metadata through the job graph
    TASK_PASSTHROUGH_KEY: str = 'task_pass_through'
    RETURN_JOB='RETURN_JOB'
    CONTEXT='CONTEXT'
    SAVED_RESULTS='SAVED_RESULTS'

    def __init__(self, name: Optional[str] = None, properties: Dict[str, Any] = {}):
        """
        Initialize an JobABC instance.

        Args:
            name (Optional[str], optional): Must be a unique identifier for this job within the context of a FlowManager.
                                            If not provided, a unique name will be auto-generated.
            properties (Dict[str, Any], optional): configuration properties passed in by jobs.yaml
        """
        self.name:str = self._getUniqueName() if name is None else name
        self.save_result: bool = bool(properties.get("save_result", False))
        self.properties:Dict[str, Any] = properties
        self.expected_inputs:set[str] = set()
        self.next_jobs:list[JobABC] = [] 
        self.timeout = 3000
        self.logger = logging.getLogger(self.__class__.__name__)
        self.global_ctx = None

    def __or__(self, other):
        """Implements the | operator for parallel composition"""
        # Import DSL classes inline to avoid circular imports
        from .dsl import Parallel, Serial
        from .jobs.wrapping_job import WrappingJob

        if isinstance(other, Parallel):
            # If right side is already a parallel component, add to its components
            return Parallel(*([self] + other.components))
        elif isinstance(other, JobABC):
            return Parallel(self, other)
        elif isinstance(other, Serial):
            return Parallel(self, other)
        else:
            # If other is a raw object, wrap it first
            return Parallel(self, WrappingJob(other))
    
    def __rshift__(self, other):
        """Implements the >> operator for serial composition"""
        # Import DSL classes inline to avoid circular imports
        from .dsl import Parallel, Serial
        from .jobs.wrapping_job import WrappingJob

        if isinstance(other, Serial):
            # If right side is already a serial component, add to its components
            return Serial(*([self] + other.components))
        elif isinstance(other, JobABC):
            return Serial(self, other)
        elif isinstance(other, Parallel):
            return Serial(self, other)
        else:
            # If other is a raw object, wrap it first
            return Serial(self, WrappingJob(other))
    
    @classmethod
    def create_FQName(cls, graph_name, parameter_name, short_graph_job_name, dsl_id=None):
        """
        Creates a unique, fully qualified name from the graph, parameter and job names.
        
        Args:
            graph_name: Name of the graph
            parameter_name: Parameter name (usually variant)
            short_graph_job_name: The job name (short form)
            dsl_id: Optional unique identifier for the source DSL to prevent FQ name collisions
            
        Returns:
            str: A fully qualified name in format graph_name$$parameter_name$$short_graph_job_name$$
                 or graph_name$$parameter_name-dsl_id$$short_graph_job_name$$ if dsl_id is provided
        """
        # If a DSL identifier is provided, incorporate it into the parameter name
        # This ensures unique FQ names while maintaining compatibility with existing parsing logic
        if dsl_id:
            # Embed the DSL ID in the parameter name to maintain compatibility with parsers
            if parameter_name:
                enhanced_param_name = f"{parameter_name}-{dsl_id}"
            else:
                enhanced_param_name = dsl_id
        else:
            enhanced_param_name = parameter_name
            
        unique_job_name = graph_name + SPLIT_STR + enhanced_param_name + SPLIT_STR + short_graph_job_name + SPLIT_STR
        return unique_job_name

    @classmethod
    def parse_job_loader_name(cls, name: str) -> Dict[str, str]:
        """Parse a job loader name into its constituent parts.
        
        Args:
            name: The full job loader name string in format:
                 graph_name$$param_name$$job_name$$
                 
        Returns:
            dict: A dictionary containing graph_name, param_name, and job_name,
                 or {'parsing_message': 'UNSUPPORTED NAME FORMAT'} if invalid
        """
        try:
            parts = name.split(SPLIT_STR)
            if len(parts) != 4 or parts[3] != "" or not parts[0]:
                return {"parsing_message": "UNSUPPORTED NAME FORMAT"}
                
            return {
                "graph_name": parts[0],
                "param_name": parts[1],
                "job_name": parts[2]
            }
        except:
            return {"parsing_message": "UNSUPPORTED NAME FORMAT"}

    @classmethod
    def parse_graph_name(cls, name: str) -> str:
        """Parse and return the graph name from a job loader name.
        
        Args:
            name: The full job loader name string
            
        Returns:
            str: The graph name or 'UNSUPPORTED NAME FORMAT' if invalid
        """
        result = cls.parse_job_loader_name(name)
        return result.get("graph_name", "UNSUPPORTED NAME FORMAT")

    @classmethod
    def parse_param_name(cls, name: str) -> str:
        """Parse and return the parameter name from a job loader name.
        
        Args:
            name: The full job loader name string
            
        Returns:
            str: The parameter name or 'UNSUPPORTED NAME FORMAT' if invalid
        """
        result = cls.parse_job_loader_name(name)
        return result.get("param_name", "UNSUPPORTED NAME FORMAT")

    @classmethod
    def parse_job_name(cls, name: str) -> str:
        """Parse and return the job name from a job loader name.
        
        Args:
            name: The full job loader name string
            
        Returns:
            str: The job name or 'UNSUPPORTED NAME FORMAT' if invalid
        """
        result = cls.parse_job_loader_name(name)
        return result.get("job_name", "UNSUPPORTED NAME FORMAT")

    @classmethod
    def _getUniqueName(cls):
        # Increment the counter for the current class
        cls._instance_counts[cls] = cls._instance_counts.get(cls, 0) + 1
        # Return a unique name based on the current class
        return f"{cls.__name__}_{cls._instance_counts[cls]}"

    @classmethod
    def get_input_from(cls, inputs: Dict[str, Any], job_name: str) -> Dict[str, Any]:
        """Get input data from a specific job in the inputs dictionary.
        
        Args:
            inputs (Dict[str, Any]): Dictionary of inputs from various jobs
            job_name (str): Name of the job whose input we want to retrieve
            
        Returns:
            Dict[str, Any]: The input data from the specified job, or empty dict if not found
        """
        for key in inputs.keys():
            if cls.parse_job_name(key) == job_name:
                return inputs[key]
        return {}


    @classmethod
    def job_set(cls, job) -> set['JobABC']:
        """
        Returns a set of all unique job instances in the job graph by recursively traversing
        all possible paths through next_jobs.
        
        Returns:
            set[JobABC]: A set containing all unique job instances in the graph
        """
        result = {job}  # Start with current job instance
        
        # Base case: if no next jobs, return current set
        if not job.next_jobs:
            return result
            
        # Recursive case: add all jobs from each path
        for job in job.next_jobs:
            result.update(cls.job_set(job))
            
        return result

    def __repr__(self):
        next_jobs_str = [job.name for job in self.next_jobs]
        expected_inputs_str = [input_name for input_name in self.expected_inputs]
        return (f"name: {self.name}\n"
                f"next_jobs: {next_jobs_str}\n"
                f"expected_inputs: {expected_inputs_str}\n"
                f"properties: {self.properties}")

    async def _execute(self, task: Union[Task, None]) -> Dict[str, Any]:
        """ Responsible for executing the job graph, maintaining state of the graph
        by updating the JobState object and propagating the tail results back up the graph
        when a tail job is reached.

        This is a classic dataflow execution model, where computation proceeds based on data 
        availability rather than a predetermined sequence.

        WARNING: DO NOT OVERRIDE THIS METHOD IN CUSTOM JOB CLASSES.
        This method is part of the core Flow4AI execution flow and handles critical operations
        including job graph traversal, state management, and result propagation.
        
        Instead, implement the abstract 'run' method to define custom job behavior.

        Can only be used within a job_graph_context set up with:
           ```python
            async with job_graph_context_manager(job_set):
                        result = await job._execute(task)
            ```
        The recursive nature of the algorithm means that results flow upwards, with the head job appearing
        to return the result of the tail job.

        Args:
            task (Union[Task, None]): the input to the first (head) job of the job graph, is None in child jobs.

        Returns:
            Dict[str, Any]: The output of the job graph execution
        """
        job_state_dict:dict = job_graph_context.get()
        job_state = job_state_dict.get(self.name)
        if self.is_head_job() and  isinstance(task, dict):
            job_state.inputs.update(task)
            self.get_context()[JobABC.TASK_PASSTHROUGH_KEY] = task
        elif task is None:
            pass 
        else:
            job_state.inputs[self.name] = task

        if self.expected_inputs:
            if job_state.execution_started:
                return None
            
            job_state.execution_started = True
            try:
                await asyncio.wait_for(job_state.input_event.wait(), self.timeout)
            except asyncio.TimeoutError:
                job_state.execution_started = False
                raise TimeoutError(
                    f"Timeout waiting for inputs in {self.name}. "
                    f"Expected: {self.expected_inputs}, "
                    f"Received: {list(job_state.inputs.keys())}"
                )

        result = await self.run(task)
        self.logger.debug(f"Job {self.name} finished running")

        if self.save_result:
            saved_results = self.get_context()[JobABC.SAVED_RESULTS]
            saved_results[self.name] = result

        if not isinstance(result, dict):
            result = {'result': result}

        # Clear state for potential reuse
        job_state.inputs.clear()
        job_state.input_event.clear()
        job_state.execution_started = False

        # Store the job name that returns the result
        result[JobABC.RETURN_JOB] = self.name

        # If this is a tail job, return immediately
        if not self.next_jobs:
            self.logger.debug(f"Tail Job {self.name} returning result: {result}")
            task = self.get_context()[JobABC.TASK_PASSTHROUGH_KEY]
            result[JobABC.TASK_PASSTHROUGH_KEY] = task
            saved_results = self.get_context().get(JobABC.SAVED_RESULTS, {})
            if saved_results:
                result[JobABC.SAVED_RESULTS] = {JobABC.parse_job_name(k): v for k, v in saved_results.items()}
            return result

        # Check if any child jobs are ready to execute once given this result as input
        executing_jobs = []
        for next_job in self.next_jobs:
            input_data = result.copy()
            # add result data from this job as an input to the next job
            await next_job.receive_input(self.name, input_data)
            next_job_inputs = job_state_dict.get(next_job.name).inputs
            # if the next job has all its inputs add coroutine to list to execute
            if next_job.expected_inputs.issubset(set(next_job_inputs.keys())):
                the_task = self.get_task()
                executing_jobs.append(next_job._execute(task=the_task))

        # If there are any child jobs ready to execute, execute them, else return None 
        if executing_jobs:
            # await for all futures to return results
            child_results = await asyncio.gather(*executing_jobs)
            not_none_results = [r for r in child_results if r is not None]
            if not_none_results:
                # Return the first valid result.
                # The recursive nature of the algorithm means that results flow upwards, with the head job appearing
                # to return the result of the tail job, so returning the first valid result will return the tail job
                # result up the stack.
                first_valid_result = not_none_results[0]
                self.logger.debug(f"Job {self.name} propagating first valid result: {first_valid_result}")
                return first_valid_result

        # If no child jobs executed or no valid result found, return None
        return None

    async def receive_input(self, from_job: str, data: Dict[str, Any]) -> None:
        """Receive input from a predecessor job"""
        job_state_dict:dict = job_graph_context.get()
        job_state = job_state_dict.get(self.name)
        job_state.inputs[from_job] = data
        if self.expected_inputs.issubset(set(job_state.inputs.keys())):
            job_state.input_event.set()

    def job_set_str(self) -> set[str]:
        """
        Returns a set of all unique job names in the job graph by recursively traversing
        all possible paths through next_jobs.
        
        Returns:
            set[str]: A set containing all unique job names in the graph
        """
        result = {self.name}  # Start with current job's name
        
        # Base case: if no next jobs, return current set
        if not self.next_jobs:
            return result
            
        # Recursive case: add all jobs from each path
        for job in self.next_jobs:
            result.update(job.job_set_str())
            
        return result

    def is_head_job(self) -> bool:
        """
        Check if this job is a head job (has no expected inputs).

        Returns:
            bool: True if this is a head job (no expected inputs), False otherwise
        """
        return len(self.expected_inputs) == 0

    def get_context(self) -> Dict[str, Any]:
        """
        Returns an object that can be used to store context across jobs in a graph for a single coroutine.
           can only be used within a job_graph_context set up with:
           ```python
            async with job_graph_context_manager(job_set):
                        result = await job._execute(task)
        """
        job_state_dict:dict = job_graph_context.get()
        context = job_state_dict[JobABC.CONTEXT]
        return context

    def _get_long_name_inputs(self) -> Dict[str, Any]:
        """
        Returns the inputs for this job.

        Returns:
            Dict[str, Any]: Returns the inputs to this job with long fully qualified job names as keys
        """
        job_state_dict:dict = job_graph_context.get()
        jobstate:JobState = job_state_dict[self.name]
        inputs: Dict[str, Dict[str, Any]] = jobstate.inputs
        return inputs   
    
    def get_inputs(self) -> Dict[str, Dict[str, Any]]:
        """
        Returns the inputs to this job with short job names as keys.

        Returns:
            Dict[str, Dict[str, Any]]: The inputs to this job.
        """
        inputs: Dict[str, Dict[str, Any]] = self._get_long_name_inputs()
        inputs_with_short_job_name = {JobABC.parse_job_name(k): v for k, v in inputs.items()}
        self.logger.debug(f"Returning inputs: {inputs_with_short_job_name}")
        return inputs_with_short_job_name

    def get_task(self) -> Union[Dict[str, Any], Task]:
        """
        Get the task associated with this job.

        Returns:
            Union[Dict[str, Any], Task]: The task associated with this job.
        """
        task = self.get_context()[JobABC.TASK_PASSTHROUGH_KEY]
        return task
        
        # if not self.is_head_job(): 
        #     first_parent_result = next(iter(inputs.values()))
        #     task = first_parent_result[JobABC.TASK_PASSTHROUGH_KEY]
        # else:
        #     task = inputs
        # return task

    def update_context(self, new_context: Dict[str, Any]) -> None:
        """
        Update the context dictionary with new values.
        
        Args:
            new_context: Dictionary with new context values
        """
        self.global_ctx.update(new_context)

    @abstractmethod
    async def run(self, task: Union[Dict[str, Any], Task]) -> Dict[str, Any]:
        """Execute the job on the given task. Must be implemented by subclasses."""
        pass

# SimpleJob and SimpleJobFactory have been moved to tests/test_utils/simple_job.py
# They are only used for testing purposes and not for production code.



================================================
FILE: src/flow4ai/job_loader.py
================================================
import copy
import importlib.util
import inspect
import os
import sys
from pathlib import Path
from typing import Any, Collection, Dict, List, Type, Union

import yaml
from pydantic import BaseModel

from . import f4a_logging as logging
from .f4a_graph import validate_graph
from .job import JobABC

logger = logging.getLogger(__name__)


class JobValidationError(Exception):
    """Raised when a custom job fails validation"""
    pass


class ConfigurationError(Exception):
    """Exception raised when configuration is malformed."""
    pass


class PythonLoader:
    JOBS = "jobs"
    PYDANTIC = "pydantic"

    @staticmethod
    def validate_pydantic_class(job_class: Type) -> bool:
        """Validate that a class meets the requirements to be a valid pydantic model:
        - Inherits from BaseModel
        """
        return inspect.isclass(job_class) and issubclass(job_class, BaseModel)

    @staticmethod
    def validate_job_class(job_class: Type) -> bool:
        """
        Validate that a class meets the requirements to be a valid job:
        - Inherits from JobABC
        - Has required methods
        - Has required attributes
        """
        # Check if it's a class and inherits from JobABC
        if not (inspect.isclass(job_class) and issubclass(job_class, JobABC)):
            return False

        # Check for required async run method
        if not hasattr(job_class, 'run'):
            return False

        # Check if run method is async
        run_method = getattr(job_class, 'run')
        if not inspect.iscoroutinefunction(run_method):
            return False

        return True

    @classmethod
    def load_python(cls, python_dir: str, type_name: str = JOBS) -> Dict[str, Union[Type[JobABC], Type[BaseModel]]]:
        """
        Load all custom job classes from the specified directory
        """
        python_classes = {}
        python_path = Path(python_dir)

        if not python_path.exists():
            logger.info(f"Python directory not found: {python_dir}")
            return python_classes

        # Add the custom jobs directory to Python path
        logger.debug(f"Python path before: {sys.path}")
        sys.path.append(str(python_path))
        logger.info(f"Added {python_path} to Python path")
        logger.debug(f"Python path after: {sys.path}")

        # Scan for Python files
        for file_path in python_path.glob("**/*.py"):
            if file_path.name.startswith("__"):
                continue

            logger.info(f"Loading python classes from {file_path}")
            try:
                # Load the module
                module_name = file_path.stem
                spec = importlib.util.spec_from_file_location(module_name, str(file_path))
                if spec is None or spec.loader is None:
                    logger.warning(f"Could not create module spec for {file_path}")
                    continue

                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)

                # Find all classes in the module that inherit from JobABC
                for name, obj in inspect.getmembers(module):
                    if inspect.isclass(obj) and obj.__module__ == module.__name__:
                        try:
                            if ((type_name == cls.JOBS and cls.validate_job_class(obj)) or
                                (type_name == cls.PYDANTIC and cls.validate_pydantic_class(obj))):
                                logger.info(f"Found valid python class: {name}")
                                python_classes[name] = obj
                        except Exception as e:
                            logger.error(f"Error validating python class {name} in {file_path}: {str(e)}")
                            raise JobValidationError(
                                f"Error validating python class {name} in {file_path}: {str(e)}"
                            )

            except Exception as e:
                logger.error(f"Error loading custom python class from {file_path}: {str(e)}")
                raise ImportError(
                    f"Error loading custom python class from {file_path}: {str(e)}"
                )

        return python_classes


class JobFactory:
    _job_types_registry: Dict[str, Type[JobABC]] = {}
    _pydantic_types_registry: Dict[str, Type[BaseModel]] = {}
    # Default jobs directory is always checked first
    _default_jobs_dir: str = os.path.join(os.path.dirname(__file__), "jobs") # site-package directory when this is a package
    _cached_job_graphs: List[JobABC] = None

    @classmethod
    def load_python_into_registries(cls, custom_python_dirs: list[str] = None):
        """
        Load and register all custom jobs from specified config directories.
        Will look for jobs in the 'jobs' subdirectory of each config directory.
        Loads jobs from all directories.

        Args:
            custom_python_dirs: List of config directory paths. Jobs will be loaded from the 'jobs' subdirectory
                            of each config directory.
        """
        loader = PythonLoader()
        # Create an iterable of job directories, including the default and any custom directories
        python_dirs = [cls._default_jobs_dir]
        if custom_python_dirs:
            # Add local jobs directories from each config directory
            for config_dir in custom_python_dirs:
                python_dir = os.path.join(config_dir, "jobs")
                if os.path.exists(python_dir):
                    python_dirs.append(python_dir)
            
        found_valid_jobs = False
        for python_dir in python_dirs:
            # Load and register jobs
            jobs = loader.load_python(python_dir, PythonLoader.JOBS)
            if jobs:
                found_valid_jobs = True
                # Register all valid custom jobs
                for job_name, job_class in jobs.items():
                    cls.register_job_type(job_name, job_class)
                    print(f"Registered custom job: {job_name}")
            
            # Load and register pydantic models
            pydantic_models = loader.load_python(python_dir, PythonLoader.PYDANTIC)
            if pydantic_models:
                for model_name, model_class in pydantic_models.items():
                    cls.register_pydantic_type(model_name, model_class)
                    print(f"Registered pydantic model: {model_name}")
            else:
                logger.info("No pydantic classes found")
        
        if not found_valid_jobs:
            # This is a critical error as we need at least one valid job directory
            raise FileNotFoundError(f"No valid jobs found in any of the directories: {python_dirs}")

    @classmethod
    def create_job(cls, name: str, job_type: str, job_def: Dict[str, Any]) -> JobABC:
        if job_type not in cls._job_types_registry:
            logger.error(f"*** Unknown job type: {job_type} ***")
            raise ValueError(f"Unknown job type: {job_type}")
        
        properties = job_def.get('properties', {})
        if not properties:
            logger.info(f"No properties specified for job {name} of type {job_type}")
            
        return cls._job_types_registry[job_type](name, properties)

    @classmethod
    def register_job_type(cls, type_name: str, job_class: Type[JobABC]):
        cls._job_types_registry[type_name] = job_class

    @classmethod
    def register_pydantic_type(cls, type_name: str, model_class: Type[BaseModel]):
        """Register a Pydantic model type with the factory"""
        cls._pydantic_types_registry[type_name] = model_class

    @classmethod
    def get_pydantic_class(cls, type_name: str) -> Type[BaseModel]:
        """Retrieve a registered Pydantic model class by its type name."""
        if type_name not in cls._pydantic_types_registry:
            raise ValueError(f"Pydantic type {type_name} not registered.")
        return cls._pydantic_types_registry[type_name]

    @classmethod
    def get_head_jobs_from_config(cls) -> Collection[JobABC]:
        JobFactory.load_python_into_registries(ConfigLoader.directories)
        """Create job graphs from configuration, using cache if available"""
        if cls._cached_job_graphs is None:
            job_graphs: list[JobABC] = []
            graphs_config = ConfigLoader.get_graphs_config()
            graph_names = list(graphs_config.keys())
            for graph_name in graph_names:
                graph_def = graphs_config[graph_name]
                job_names_in_graph = list(graph_def.keys())
                param_groups_for_graph_name = ConfigLoader.get_parameters_config().get(graph_name, {})
                if param_groups_for_graph_name:
                    param_jobs_graphs: List[JobABC] = cls.create_job_graph_using_parameters(graph_def, graph_name,
                                                                                            param_groups_for_graph_name,
                                                                                            job_names_in_graph)
                    job_graphs += param_jobs_graphs
                else:
                    job_graph_no_params: JobABC = cls.create_job_graph_no_params(graph_def, graph_name,
                                                                                 job_names_in_graph)
                    job_graphs.append(job_graph_no_params)
            cls._cached_job_graphs = job_graphs
        return cls._cached_job_graphs

    @classmethod
    def create_job_graph_using_parameters(cls, graph_def, graph_name, param_groups_for_graph_name,
                                           job_names_in_graph) -> List[JobABC]:
        job_graphs: list[JobABC] = []
        parameter_names_list = list(param_groups_for_graph_name.keys())
        for parameter_name in parameter_names_list:
            job_instances: dict[str, JobABC] = {}
            # Create a copy of graph_def for this parameter group
            # This ensures each parameter group has its own graph definition
            param_graph_def = copy.deepcopy(graph_def)
            
            # Create job instances for this parameter group
            for short_graph_job_name in job_names_in_graph:
                raw_job_def: Dict[str, Any] = ConfigLoader.get_jobs_config()[short_graph_job_name]
                if ConfigLoader.is_parameterized_job(raw_job_def):
                    job_def: Dict[str, Any] = ConfigLoader.fill_job_with_parameters(raw_job_def, graph_name, parameter_name)
                else:
                    job_def = raw_job_def
                unique_job_name = JobABC.create_FQName(graph_name, parameter_name, short_graph_job_name)
                job_type: str = job_def["type"]
                job: JobABC = cls.create_job(unique_job_name, job_type, job_def)
                job_instances[short_graph_job_name] = job
                
            # Create the job graph for this parameter group
            job_graph: JobABC = cls.create_job_graph(param_graph_def, job_instances)
            job_graphs.append(job_graph)
        return job_graphs



    @classmethod
    def create_job_graph_no_params(cls, graph_def, graph_name, job_names_in_graph)-> JobABC:
        job_instances: dict[str, JobABC] = {}
        for short_graph_job_name in job_names_in_graph:
                job_def: Dict[str, Any] = ConfigLoader.get_jobs_config()[short_graph_job_name]
                unique_job_name = JobABC.create_FQName(graph_name, "", short_graph_job_name)
                job_type: str = job_def["type"]
                job: JobABC = cls.create_job(unique_job_name, job_type, job_def)
                job_instances[short_graph_job_name] = job
        job_graph: JobABC = cls.create_job_graph(graph_def, job_instances)
        return job_graph

    @classmethod
    def create_job_graph(cls, graph_definition: dict[str, dict], job_instances: dict[str, JobABC]) -> JobABC:
        """
        Takes the below inputs and creates a job graph by adding next_jobs and expected_inputs fields to each job in 
        the job_instances dictionary, default head and tail jobs are added to the graph, if necessary.

        graph_definition: dict[str, Any] = {
            "A": {"next": ["B", "C"]},
            "B": {"next": ["D"]},
            "C": {"next": ["D"]},
            "D": {"next": []},
        }

        job instances are a dictionary of job instances in the job graph and looks like this:

        job_instances: dict[str, JobABC] = {
            "A": SimpleJob("A"),
            "B": SimpleJob("B"),
            "C": SimpleJob("C"),
            "D": SimpleJob("D"),
        }
        
        """
        nodes:dict[str, JobABC] = {} # nodes holds Jobs which will be hydrated with next_jobs 
                                    # and expected_inputs fields from the graph_definition.
        
        # determine the incoming edges i.e the Jobs that each Job depends on
        # so we can determine the head node ( which depends on no Jobs) 
        # and set the expected_inputs (i.e. the dependencies) for each Job.
        incoming_edges: dict[str, set[str]] = {job_name: set() for job_name in graph_definition}
        for job_name, config in graph_definition.items():
            for next_job in config['next']:
                incoming_edges[next_job].add(job_name)
        
        # 1) Find the head node (node with no incoming edges)
        head_jobs = [job_name for job_name, inputs in incoming_edges.items() if not inputs]
        
        # Handle multiple head jobs before creating nodes dictionary
        if len(head_jobs) > 1:
            head_job_name = cls.add_default_head(graph_definition, head_jobs, job_instances, nodes)
        elif len(head_jobs) == 1:
            head_job_name = head_jobs[0]
        else:
            raise ValueError("No head nodes found in graph definition")
            
        # Now populate the nodes dictionary after potential DefaultHeadJob addition
        for job_name in graph_definition:
            try:
                job_obj = job_instances[job_name]
            except KeyError:
                # Fallback mechanism for parameterized job names
                # Try to find the job by parsing the job name and looking for the base job name
                parsed_name = JobABC.parse_job_name(job_name)
                if parsed_name != "UNSUPPORTED NAME FORMAT" and parsed_name in job_instances:
                    job_obj = job_instances[parsed_name]
                    logger.debug(f"Found job {job_name} using parsed name {parsed_name}")
                else:
                    raise KeyError(f"Job {job_name} not found in job_instances and no fallback available")
            nodes[job_name] = job_obj
            
        # 1.5) Find the tail nodes (nodes with no outgoing edges)
        tail_jobs = [job_name for job_name, config in graph_definition.items() if not config['next']]
        
        if len(tail_jobs) > 1:
            # Add a default tail node if there are multiple tail nodes
            cls.add_default_tail(graph_definition, tail_jobs, job_instances, nodes)

        # 2) Set next_jobs for each node
        for job_name, config in graph_definition.items():
            nodes[job_name].next_jobs = [nodes[next_name] for next_name in config['next']]

        # 3) Set expected_inputs for each node using fully qualified names
        for job_name, input_job_names_set in incoming_edges.items():
            if input_job_names_set:  # if node has incoming edges
                # Transform short names to fully qualified names using the job_instances dictionary
                nodes[job_name].expected_inputs = {job_instances[input_name].name for input_name in input_job_names_set}

        # 4) Set reference to final node in head node -- not needed!
        # Find node with no next jobs
        # final_job_name = next(job_name for job_name, config in graph_definition.items() 
        #                    if not config['next'])
        # nodes[head_job_name].final_node = nodes[final_job_name]

        return nodes[head_job_name]

    @classmethod
    def add_default_head(cls, graph_definition, head_jobs, job_instances, nodes):
        from flow4ai.jobs.default_jobs import DefaultHeadJob

        # Get naming from first job instance in job_instances
        sample_job = next(iter(job_instances.values()))
        sample_name = sample_job.name
        parsed = JobABC.parse_job_name(sample_name)
        # Debug logging for sample name and parsed name
        logger.debug(f"DEBUG - Sample name (long): {sample_name}")
        logger.debug(f"DEBUG - Parsed name (short): {parsed}")
        
        # For each parameter group, we need to create a unique DefaultHeadJob
        # Extract parameter group from sample name
        parsed_parts = JobABC.parse_job_loader_name(sample_name)
        graph_name = parsed_parts.get("graph_name", "")
        param_name = parsed_parts.get("param_name", "")
        
        # Create a unique name for this DefaultHeadJob based on the parameter group
        if graph_name and param_name:
            # Use the same format as in create_job_graph_using_parameters
            unique_job_name = f"{graph_name}$${param_name}$$DefaultHeadJob$$"
            default_head = DefaultHeadJob(name=unique_job_name)
            logger.debug(f"Constructed DefaultHeadJob name: {unique_job_name}")
        elif graph_name:
            unique_job_name = f"{graph_name}$$$$DefaultHeadJob$$"
            default_head = DefaultHeadJob(name=unique_job_name)
            logger.debug(f"Constructed DefaultHeadJob name: {unique_job_name}")
        else:
            default_head = DefaultHeadJob()
            logger.warning("Falling back to default naming for head job")
        
        logger.debug(f"Created DefaultHeadJob with name: {default_head.name}")
        
        # Use 'DefaultHeadJob' as the key in graph_definition and job_instances
        # This is the key that will be used in the graph definition
        head_job_key = default_head.name #"DefaultHeadJob"
        
        # Add the job to job_instances with the DefaultHeadJob key
        job_instances[head_job_key] = default_head
        
        # Add to graph_definition with the same key
        graph_definition[head_job_key] = {"next": head_jobs}
        
        # Add the default head job to nodes dictionary
        nodes[head_job_key] = default_head
        
        return head_job_key
        
    @classmethod
    def add_default_tail(cls, graph_definition, tail_jobs, job_instances, nodes):
        """Add a default tail node to the graph when multiple tail nodes are detected.
        
        Args:
            graph_definition: The graph definition dictionary to modify
            tail_jobs: List of job names that are tail nodes (no 'next' nodes)
            job_instances: Dictionary of job instances
            nodes: Dictionary of nodes in the graph
            
        Returns:
            The name of the default tail job
        """
        from flow4ai.jobs.default_jobs import DefaultTailJob

        # Get naming from first job instance in job_instances
        sample_job = next(iter(job_instances.values()))
        sample_name = sample_job.name
        parsed = JobABC.parse_job_name(sample_name)
        
        # Debug logging for sample name and parsed name
        logger.debug(f"DEBUG - Sample name (long): {sample_name}")
        logger.debug(f"DEBUG - Parsed name (short): {parsed}")
        
        if parsed != 'UNSUPPORTED NAME FORMAT':
            # Replace the short job name with "DefaultTailJob" while maintaining the $$ format
            # Example: "multi_tail_demo$$params1$$tail_job_alpha$$" -> "multi_tail_demo$$params1$$DefaultTailJob$$"
            new_name = sample_name.replace(parsed + "$$", "DefaultTailJob$$")
            default_tail = DefaultTailJob(name=new_name)
            logger.debug(f"Constructed DefaultTailJob name: {new_name}")
        else:
            default_tail = DefaultTailJob()
            logger.warning("Falling back to default naming for tail job")
            
        logger.debug(f"Created DefaultTailJob with name: {default_tail.name}")
        
        # Add the default tail job to job_instances and nodes dictionaries
        job_instances[default_tail.name] = default_tail
        nodes[default_tail.name] = default_tail
        
        # Update the graph definition to make all tail nodes point to the default tail
        graph_definition[default_tail.name] = {"next": []}
        
        # Update all tail nodes to point to the default tail
        for tail_job in tail_jobs:
            graph_definition[tail_job]["next"] = [default_tail.name]
            
        return default_tail.name


class ConfigLoader:
    # Directories are searched in order. If a valid flow4ai directory is found,
    # the search stops and uses that directory.
    # TODO: Nice to have - Add support for merging configurations from multiple directories
    #       if required in the future.
    directories: List[str] = [
        os.path.join(os.getcwd(), "flow4ai"),  # flow4ai directory in current working directory
        os.path.join(os.path.expanduser("~"), "flow4ai"),  # ~/flow4ai
        "/etc/flow4ai"
    ]
    _cached_configs: Dict[str, dict] = None

    @classmethod
    def _set_directories(cls, directories):
        """Set the directories and clear the cache"""
        cls.directories = directories
        cls._cached_configs = None

    @classmethod
    def __setattr__(cls, name, value):
        """Clear cache when directories are changed"""
        super().__setattr__(name, value)
        if name == 'directories':
            cls._cached_configs = None

    @classmethod
    def load_configs_from_dirs(
            cls,
            directories: List[str] = [],
            config_bases: List[str] = ['graphs', 'jobs', 'parameters', 'flow4ai_all'],
            allowed_extensions: tuple = ('.yaml', '.yml', '.json')
    ) -> Dict[str, dict]:
        """
        Load configuration files from directories. Will search directories in order and stop
        at the first valid flow4ai directory found.
        
        Args:
            directories: List of directory paths to search
            config_bases: List of configuration file base names to look for
            allowed_extensions: Tuple of allowed file extensions
        
        Returns:
            Dictionary with config_base as key and loaded config as value
            
        Raises:
            FileNotFoundError: If no valid flow4ai directory is found in any of the directories
            ConfigurationError: If configuration files are malformed
        """
        configs: Dict[str, dict] = {}
        config_files: Dict[str, str] = {}  # Track which file each config came from

        # Convert directories to Path objects
        dir_paths = [Path(str(d)) for d in directories]
        logger.info(f"Looking for config files in directories: {dir_paths}")

        found_valid_dir = False
        for dir_path in dir_paths:
            if not dir_path.exists():
                logger.info(f"Directory not found, skipping: {dir_path}")
                continue

            # Check if any config files exist in this directory
            has_configs = False
            for config_base in config_bases:
                for ext in allowed_extensions:
                    if (dir_path / f"{config_base}{ext}").exists():
                        has_configs = True
                        break
                if has_configs:
                    break

            if has_configs:
                found_valid_dir = True
                logger.info(f"Found valid flow4ai directory: {dir_path}")
                # Load configs from this directory only
                for config_base in config_bases:
                    for ext in allowed_extensions:
                        config_path = dir_path / f"{config_base}{ext}"
                        if config_path.exists():
                            try:
                                with open(config_path) as f:
                                    configs[config_base] = yaml.safe_load(f)
                                    config_files[config_base] = str(config_path)
                            except yaml.YAMLError as e:
                                error_msg = "Configuration is malformed. Unable to proceed with job execution.\n\n" \
                                        "-------------------------------------------------------\n" \
                                        "          Configuration file malformed - cannot continue\n" \
                                        "-------------------------------------------------------\n" \
                                        f"File: {config_path}\n" \
                                        f"Error details: {str(e)}\n" \
                                        "-------------------------------------------------------"
                                raise ConfigurationError(error_msg) from e
                break  # Stop searching after finding first valid directory

        if not found_valid_dir:
            raise FileNotFoundError(f"No valid flow4ai directory found in search paths: {dir_paths}")

        # Store file paths in configs
        configs['__files__'] = config_files
        return configs

    @classmethod
    def _extract_config_section(cls, configs: Dict[str, dict], section_name: str) -> dict:
        """
        Extract a configuration section from either a dedicated file or flow4ai_all.
        
        Args:
            configs: Dictionary containing all configurations
            section_name: Name of the section to extract (e.g., 'graphs', 'jobs', 'parameters')
            
        Returns:
            Dictionary containing the configuration section, or empty dict if not found
        """
        # Try to get from dedicated file first
        if section_name in configs:
            return configs[section_name]

        # If not found, try to get from flow4ai_all
        if 'flow4ai_all' in configs and isinstance(configs['flow4ai_all'], dict):
            return configs['flow4ai_all'].get(section_name, {})

        # If nothing found, return empty dict
        return {}

    @classmethod
    def _find_parameterized_fields(cls, job_config: dict) -> set:
        """
        Find all parameterized fields in a job configuration.
        A field is parameterized if its value starts with '$'.
        
        Args:
            job_config: Job configuration dictionary
            
        Returns:
            Set of parameterized field names
        """
        params = set()

        def search_dict(d):
            for k, v in d.items():
                if isinstance(v, dict):
                    search_dict(v)
                elif isinstance(v, str) and v.startswith('$'):
                    params.add(v[1:])  # Remove the '$' prefix

        search_dict(job_config.get('properties', {}))
        return params

    @classmethod
    def _validate_graph_structure(cls, graph_def: dict, defined_jobs: set, graph_name: str) -> None:
        """
        Validate the structure of a job graph.
        - Checks for cycles
        - Ensures all referenced jobs exist
        - Validates head/tail nodes
        
        Args:
            graph_def: Graph definition from configuration
            defined_jobs: Set of all defined job names
            graph_name: Name of the graph being validated
            
        Raises:
            ValueError: If validation fails
        """
        # First validate all jobs exist and are properly connected
        for job, job_def in graph_def.items():
            if job not in defined_jobs:
                raise ValueError(f"Job '{job}' in graph '{graph_name}' is not defined")
            for next_job in job_def.get('next', []):
                if next_job not in defined_jobs:
                    raise ValueError(f"Job '{next_job}' referenced in 'next' field of job '{job}' in graph '{graph_name}' is not defined in jobs configuration")
                    
        # Build adjacency list after validating jobs
        adjacency = {job: job_def.get('next', []) for job, job_def in graph_def.items()}
        
        # Check for cycles using DFS
        visited = set()
        rec_stack = set()
        
        def has_cycle(node):
            visited.add(node)
            rec_stack.add(node)
            
            for neighbor in adjacency[node]:
                if neighbor not in visited:
                    if has_cycle(neighbor):
                        return True
                elif neighbor in rec_stack:
                    return True
                    
            rec_stack.remove(node)
            return False
            
        # Run cycle detection from each unvisited node
        for job in adjacency:
            if job not in visited:
                if has_cycle(job):
                    raise ValueError(f"Cycle detected in graph '{graph_name}'")
                    
    @classmethod
    def validate_configs(cls, configs: Dict[str, dict]) -> None:
        """
        Validate that:
        1. All jobs referenced in graphs exist in jobs configuration
        2. All parameterized jobs have corresponding parameter values
        3. Each graph structure is valid (no cycles, proper head/tail nodes, valid references)
        
        Args:
            configs: Dictionary containing all configurations
            
        Raises:
            ValueError: If validation fails
            ConfigurationError: If configuration is malformed (e.g. wrong types, missing required fields)
        """
        try:
            graphs_config = cls._extract_config_section(configs, 'graphs')
            jobs_config = cls._extract_config_section(configs, 'jobs')
            parameters_config = cls._extract_config_section(configs, 'parameters')
            config_files = configs.get('__files__', {})

            if not graphs_config or not jobs_config:
                return

            # First validate that all jobs in graphs exist
            defined_jobs = set(jobs_config.keys())

            # Track which config we're currently validating
            current_config = 'jobs'
            
            # Validate jobs config structure
            for job_name, job_config in jobs_config.items():
                if not isinstance(job_config, dict):
                    raise TypeError(f"Job '{job_name}' configuration must be a dictionary")
                
            current_config = 'graphs'
            for graph_name, graph_definition in graphs_config.items():
                # Validate graph structure (no cycles, etc)
                print(f"\nChecking {graph_name} for cycles...")
                cls._validate_graph_structure(graph_definition, defined_jobs, graph_name)
                print("No cycles detected")
                validate_graph(graph_definition, graph_name)

                # Find all parameterized jobs in this graph
                graph_parameterized_jobs = {}
                for job_name in graph_definition.keys():
                    job_config = jobs_config[job_name]
                    params = cls._find_parameterized_fields(job_config)
                    if params:
                        graph_parameterized_jobs[job_name] = params

                # If graph has parameterized jobs, it must have parameters
                if graph_parameterized_jobs:
                    current_config = 'parameters'
                    if graph_name not in parameters_config:
                        raise ValueError(
                            f"Graph '{graph_name}' contains parameterized jobs {list(graph_parameterized_jobs.keys())} but has no entry in parameters configuration")

                    parameters_for_graph = parameters_config[graph_name]

                    # Validate parameter groups
                    for param_name in parameters_for_graph.keys():
                        if not param_name.startswith('params'):
                            raise ValueError(
                                f"Invalid parameter group name '{param_name}' in graph '{graph_name}'. Parameter groups must start with 'params'")

                    # Validate that all parameters are filled for each group
                    for param_name, parameterized_jobs in parameters_for_graph.items():
                        for job_name, required_params in graph_parameterized_jobs.items():
                            if job_name not in parameterized_jobs:
                                raise ValueError(
                                    f"Job '{job_name}' in graph '{graph_name}' requires parameters {required_params} but has no entry in parameter group '{param_name}'")

                            # Each job should have a list of parameter sets
                            job_param_sets = parameterized_jobs[job_name]
                            if not isinstance(job_param_sets, list):
                                raise ValueError(
                                    f"Parameters for job '{job_name}' in graph '{graph_name}', group '{param_name}' should be a list of parameter sets")

                            # Validate each parameter set
                            for param_set in job_param_sets:
                                missing_params = required_params - set(param_set.keys())
                                if missing_params:
                                    raise ValueError(
                                        f"Parameter set for job '{job_name}' in graph '{graph_name}', group '{param_name}' is missing required parameters: {missing_params}")

                print(f"Graph {graph_name} passed all validations")

        except (AttributeError, TypeError, KeyError) as e:
            # Get the relevant file path based on which config we were validating
            error_file = config_files.get(current_config, 'unknown file')
            error_msg = "Configuration is malformed. Unable to proceed with job execution.\n\n" \
                        "-------------------------------------------------------\n" \
                        "          Configuration file malformed - cannot continue\n" \
                        "-------------------------------------------------------\n" \
                        f"File: {error_file}\n" \
                        f"Error details: {str(e)}\n" \
                        "-------------------------------------------------------"
            raise ConfigurationError(error_msg) from e

    @classmethod
    def load_all_configs(cls) -> Dict[str, dict]:
        """Load all configurations and validate them"""
        if cls._cached_configs is None:
            cls._cached_configs = cls.load_configs_from_dirs(cls.directories)
            cls.validate_configs(cls._cached_configs)
        return cls._cached_configs

    @classmethod
    def reload_configs(cls) -> Dict[str, dict]:
        """Force a reload of all configurations."""
        logger.info("Reloading configs...")
        cls._cached_configs = None
        return cls.load_all_configs()

    @classmethod
    def get_graphs_config(cls) -> dict:
        """
        Get graphs configuration from either dedicated graphs file or flow4ai_all.
        Returns empty dict if no configuration is found.
        """
        configs = cls.load_all_configs()
        return cls._extract_config_section(configs, 'graphs')

    @classmethod
    def get_jobs_config(cls) -> dict:
        """
        Get jobs configuration from either dedicated jobs file or flow4ai_all.
        Returns empty dict if no configuration is found.
        """
        configs = cls.load_all_configs()
        return cls._extract_config_section(configs, 'jobs')

    @classmethod
    def get_parameters_config(cls) -> dict:
        """
        Get parameters configuration from either dedicated parameters file or flow4ai_all.
        Returns empty dict if no configuration is found.
        """
        configs = cls.load_all_configs()
        return cls._extract_config_section(configs, 'parameters')

    @classmethod
    def is_parameterized_job(cls, raw_job_def):
        """
        Check if a job definition contains parameterized fields.
        
        Args:
            raw_job_def: Raw job definition from jobs.yaml
            
        Returns:
            bool: True if job has parameterized fields, False otherwise
        """
        if not isinstance(raw_job_def, dict):
            return False
            
        # Use existing method to find parameterized fields
        params = cls._find_parameterized_fields(raw_job_def)
        return len(params) > 0

    @classmethod
    def fill_job_with_parameters(cls, job_config: dict, graph_name: str, parameter_name: str) -> dict:
        """
        Fill a job configuration with parameters from parameters.yaml.
        
        Args:
            job_config: Raw job configuration from jobs.yaml
            graph_name: Name of the graph containing the job
            parameter_name: Name of the parameter group to use
            
        Returns:
            dict: Job configuration with parameters filled in
        """
        # Deep copy the job config to avoid modifying the original
        import copy
        filled_config = copy.deepcopy(job_config)
        
        # Get parameters for this job from parameters.yaml
        params_config = cls.get_parameters_config()
        if graph_name not in params_config or parameter_name not in params_config[graph_name]:
            raise ValueError(f"No parameters found for graph '{graph_name}' and parameter group '{parameter_name}'")
            
        # Get the job name by finding which job in the parameters matches this config
        job_name = None
        for job in params_config[graph_name][parameter_name].keys():
            if job_config == cls.get_jobs_config()[job]:
                job_name = job
                break
                
        if job_name is None:
            raise ValueError(f"Could not find job in parameters for graph '{graph_name}' and group '{parameter_name}'")
            
        # Get parameter values for this job
        param_sets = params_config[graph_name][parameter_name][job_name]
        if not param_sets or not isinstance(param_sets, list):
            raise ValueError(f"Invalid parameter sets for job '{job_name}' in graph '{graph_name}', group '{parameter_name}'")
            
        # Use the first parameter set (as defined in the spec)
        param_values = param_sets[0]
        
        def replace_params(obj, params):
            if isinstance(obj, dict):
                return {k: replace_params(v, params) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [replace_params(item, params) for item in obj]
            elif isinstance(obj, str) and obj.startswith('$'):
                param_name = obj[1:]  # Remove '$' prefix
                if param_name not in params:
                    raise ValueError(f"Parameter '{param_name}' not found in parameter set")
                return params[param_name]
            return obj
            
        # Replace all parameterized values in the config
        filled_config = replace_params(filled_config, param_values)
        return filled_config



================================================
FILE: src/flow4ai/jobs/__init__.py
================================================
from flow4ai.jobs.default_jobs import DefaultHeadJob, DefaultTailJob
from flow4ai.jobs.openai_jobs import OpenAIJob

__all__ = ['OpenAIJob', 'DefaultHeadJob', 'DefaultTailJob']



================================================
FILE: src/flow4ai/jobs/default_jobs.py
================================================
from typing import Any, Dict, Union

from flow4ai import f4a_logging as logging
from flow4ai.job import JobABC, Task

logger = logging.getLogger(__name__)


class DefaultHeadJob(JobABC):
    """A Job implementation that provides a simple default behavior."""
    
    async def run(self, task: Union[Dict[str, Any], Task]) -> Dict[str, Any]:
        """Run a simple job that logs and returns the task."""
        logger.info(f"Default head JOB for {task}")
        return {}

class DefaultTailJob(JobABC):
    """A Job implementation that provides a simple default behavior."""
    
    async def run(self, task: Union[Dict[str, Any], Task]) -> Dict[str, Any]:
        """Run a simple job that logs and returns the task."""
        logger.info(f"Default tail JOB for {task}")
        inputs_with_short_job_name = self.get_inputs()
        return inputs_with_short_job_name


================================================
FILE: src/flow4ai/jobs/openai_jobs.py
================================================
import os
from typing import Any, Dict, Optional, Union

from aiolimiter import AsyncLimiter
from openai import AsyncOpenAI

from flow4ai.f4a_logging import logging
from flow4ai.job import JobABC
from flow4ai.job_loader import JobFactory
from flow4ai.utils.api_utils import get_api_key
from flow4ai.utils.llm_utils import clean_prompt

logger = logging.getLogger("OpenAIJob")

class OpenAIClient:
    """
    Singleton class for AsyncOpenAI client.
    """
    _client = None

    @classmethod
    def get_client(cls, params: Dict[str, Any] = None):
        if cls._client is None:
            # Initialize params if None
            params = params or {}
            
            # Get API key using our utility function
            api_key = get_api_key(params, key_name='OPENAI_API_KEY')
            
            # Create client with remaining params
            cls._client = AsyncOpenAI(api_key=api_key, **params)
            logger.info(f"Created client with base_url: {params.get('base_url', 'default')}")
        return cls._client

class OpenAIJob(JobABC):

    # Shared AsyncLimiter for all jobs, default to 5,000 requests per minute
    default_rate_limit = {"max_rate": 5000, "time_period": 60}

    def __init__(self, name: Optional[str] = None, properties: Dict[str, Any] = {}):
        """
        Initialize an OpenAIJob instance with a properties dict containing three top-level keys, client, api, and rate_limit.
        All properties are optional.

        Args:
            name (Optional[str], optional): 
                A unique identifier for this job.
                The name must be unique among all jobs to ensure proper job identification 
                and dependency resolution. If not provided, a unique name will be auto-generated.

            properties (Dict[str, Any], optional): Optional properties for the job. A dictionary containing the following keys:

            {
                rate_limit: {
                    max_rate: Allow up to max_rate / time_period acquisitions before blocking.
                    time_period: duration of the time period in which to limit the rate. Note that up to max_rate acquisitions are allowed within this time period in a burst
                },
                client: {
                    api_key: str | None = None,
                    organization: str | None = None,
                    project: str | None = None,
                    base_url: str | URL | None = None,
                    websocket_base_url: str | URL | None = None,
                    timeout: float | Timeout | NotGiven | None = NOT_GIVEN,
                    max_retries: int = DEFAULT_MAX_RETRIES,
                    default_headers: Mapping[str, str] | None = None,
                    default_query: Mapping[str, object] | None = None,
                    http_client: AsyncClient | None = None,
                    _strict_response_validation: bool = False
                },
                api: {
                    messages: Iterable[ChatCompletionMessageParam],
                    model: ChatModel | str,
                    audio: ChatCompletionAudioParam | NotGiven | None = NOT_GIVEN,
                    frequency_penalty: float | NotGiven | None = NOT_GIVEN,
                    function_call: FunctionCall | NotGiven = NOT_GIVEN,
                    functions: Iterable[Function] | NotGiven = NOT_GIVEN,
                    logit_bias: Dict[str, int] | NotGiven | None = NOT_GIVEN,
                    logprobs: bool | NotGiven | None = NOT_GIVEN,
                    max_completion_tokens: int | NotGiven | None = NOT_GIVEN,
                    max_tokens: int | NotGiven | None = NOT_GIVEN,
                    metadata: Dict[str, str] | NotGiven | None = NOT_GIVEN,
                    modalities: List[ChatCompletionModality] | NotGiven | None = NOT_GIVEN,
                    n: int | NotGiven | None = NOT_GIVEN,
                    parallel_tool_calls: bool | NotGiven = NOT_GIVEN,
                    prediction: ChatCompletionPredictionContentParam | NotGiven | None = NOT_GIVEN,
                    presence_penalty: float | NotGiven | None = NOT_GIVEN,
                    reasoning_effort: ChatCompletionReasoningEffort | NotGiven = NOT_GIVEN,
                    response_format: ResponseFormat | NotGiven | None = NOT_GIVEN,
                    seed: int | NotGiven | None = NOT_GIVEN,
                    service_tier: NotGiven | Literal['auto', 'default'] | None = NOT_GIVEN,
                    stop: str | List[str] | NotGiven | None = NOT_GIVEN,
                    store: bool | NotGiven | None = NOT_GIVEN,
                    stream: NotGiven | Literal[False] | None = NOT_GIVEN,
                    stream_options: ChatCompletionStreamOptionsParam | NotGiven | None = NOT_GIVEN,
                    temperature: float | NotGiven | None = NOT_GIVEN,
                    tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven = NOT_GIVEN,
                    tools: Iterable[ChatCompletionToolParam] | NotGiven = NOT_GIVEN,
                    top_logprobs: int | NotGiven | None = NOT_GIVEN,
                    top_p: float | NotGiven | None = NOT_GIVEN,
                    user: str | NotGiven = NOT_GIVEN,
                    extra_headers: Headers | None = None,
                    extra_query: Query | None = None,
                    extra_body: Body | None = None,
                    timeout: float | Timeout | NotGiven | None = NOT_GIVEN
                }
            }
        """
        super().__init__(name, properties)
        
        # Initialize OpenAI client with properties
        self.client = OpenAIClient.get_client(self.properties.get("client", {}))
        
        # Rate limiter configuration
        rate_limit_config = self.properties.get("rate_limit", self.default_rate_limit)
        self.limiter = AsyncLimiter(**rate_limit_config)

        # Extract other relevant properties for OpenAI client
        self.api_properties = self.properties.get("api", {})

    async def run(self, task: Union[Dict[str, Any], Any]) -> Dict[str, Any]:
        """
        Perform an OpenAI API call while adhering to rate limits.
        
        Args:
            task: A dictionary containing either:
                - prompt: str - The prompt to send to the model
                - messages: list - Direct message format for the API
                Or any other valid parameters for the chat.completions.create API
        """
        # Start with default properties
        request_properties = {
            "model": "gpt-4o",
            "temperature": 0.7,
            "messages": [
                {"role": "system", "content": "You are a helpful assistant"},
                {"role": "user", "content": "You are a helpful assistant."}
            ]
        }
        
        # Add API properties from initialization
        request_properties.update(self.api_properties)

        # Check if response_format is a string and replace with the Pydantic class
        if "response_format" in request_properties and isinstance(request_properties["response_format"], str):
            try:
                response_format_name = request_properties["response_format"]
                request_properties["response_format"] = JobFactory.get_pydantic_class(response_format_name)
                logger.info(f"Successfully replaced response_format string with Pydantic class: {response_format_name}")
            except ValueError as e:
                logger.error(f"Could not find Pydantic class for response_format: {request_properties['response_format']}. Error: {e}")
            except Exception as e:
                logger.error(f"An unexpected error occurred while trying to get the Pydantic class: {e}")

        self.create_prompt(request_properties, task)

        # Acquire the rate limiter before making the request
        async with self.limiter:
            try:
                logger.info(f"{self.name} is making an OpenAI API call.")
                if "response_format" in request_properties:
                    response = await self.client.beta.chat.completions.parse(**request_properties)
                else:
                    response = await self.client.chat.completions.create(**request_properties)
                logger.info(f"{self.name} received a response.")
                
                # Handle the response
                if hasattr(response, 'choices') and response.choices:
                    if "response_format" in request_properties:
                        return response.choices[0].message.parsed
                    else:
                        return {"response": response.choices[0].message.content}
                else:
                    return {"error": "No valid response content found"}
            except Exception as e:
                logger.error(f"Error in {self.name}: {e}")
                return {"error": str(e)}

    def create_prompt(self, request_properties, task):
        # Handle the task input
        if isinstance(task, dict):
            # If task has a prompt, convert it to messages format
            if "prompt" in task:
                prompt = clean_prompt(task["prompt"])
                request_properties["messages"] = [
                    {"role": "system", "content": "You are a helpful assistant"},
                    {"role": "user", "content": prompt}
                ]
            # If task already has messages, use those
            elif "messages" in task:
                request_properties["messages"] = task["messages"]

            # Add any other valid API parameters from task
            # request_properties.update({k: v for k, v in task.items() if k not in ["prompt", "messages"]})
        elif task:  # If task is not empty and not a dict
            # If task is not a dict, treat it as the prompt
            prompt = clean_prompt(str(task))
            request_properties["messages"] = [
                {"role": "system", "content": "You are a helpful assistant"},
                {"role": "user", "content": prompt}
            ]


================================================
FILE: src/flow4ai/jobs/wrapping_job.py
================================================
import inspect
from typing import Any, Callable, Dict, List, Union

from flow4ai.job import JobABC
from flow4ai.job import Task


class WrappingJob(JobABC):
    FN_CONTEXT='j_ctx'

    def __init__(
        self,
        callable_obj: Callable,
        name: str = None
    ):
        """
        Initialize a wrapper for a callable object.

        Args:
            callable_obj: The function or method to wrap
            name: Identifier for this callable in parameter dictionaries
        Raises:
            TypeError: If callable_obj is not actually callable
        """

        is_callable = callable(callable_obj)
        self.is_callable = is_callable
        if not is_callable: #and not isinstance(callable_obj, (JobABC, Parallel, Serial))
            raise TypeError(f"WrappingJob will only wrap a callable, error due to {type(callable_obj).__name__}")
        self.callable = callable_obj
        super().__init__(name)
        self.default_args = []
        self.default_kwargs = {}

    async def run(self, task: Union[Dict[str, Any], Task]) -> Dict[str, Any]:
        """
        Execute the wrapped callable with parameters from the params dictionary.

        Args:
            task: legacy parameter dictionary

        Returns:
            The result of the callable execution
        """
        if not self.is_callable:
            raise ValueError(f"Callable '{self.callable}' is not callable")

        params = task if task else self.get_task()  # if calling run() directly in tests use get_task(), "if task" is falsey so fails on {}

        # Process shorthand dot notation params (e.g., "job.param": value)
        params = self._process_shorthand_params(params)

        # Check if the callable requires parameters
        sig = inspect.signature(self.callable)
        requires_params = bool(sig.parameters)
        
        # Check if the only parameter required is 'context' which is auto-provided
        requires_non_context_params = False
        if requires_params:
            non_context_params = [param for param in sig.parameters if param != self.FN_CONTEXT]
            requires_non_context_params = bool(non_context_params)

        parsed_name = JobABC.parse_job_name(self.name)
        short_name = self.name if parsed_name == "UNSUPPORTED NAME FORMAT" else parsed_name
        # Only check for parameters if the callable requires non-context parameters
        if requires_non_context_params and short_name not in params:
            raise ValueError(f"No parameters found for callable '{short_name}'")  

        # If no parameters are required, use empty args and kwargs
        if not requires_params or short_name not in params:
            callable_params = {"args": [], "kwargs": {}}
        else:
            callable_params = self._create_callable_params(params[short_name])

        # Add context to the kwargs if the callable accepts it
        if self.FN_CONTEXT in sig.parameters:
            callable_params["kwargs"][self.FN_CONTEXT] = {}
            callable_params["kwargs"][self.FN_CONTEXT]["global"] = self.global_ctx
            callable_params["kwargs"][self.FN_CONTEXT]["task"] = params
            callable_params["kwargs"][self.FN_CONTEXT]["inputs"] = self.get_inputs()

        # Validate parameters against the callable's signature
        self._validate_params(callable_params["args"], callable_params["kwargs"])

        # Apply type conversions based on callable's signature
        args, kwargs = self._convert_param_types(
            callable_params["args"],
            callable_params["kwargs"]
        )

        return await self._execute_callable(args, kwargs)

    def _process_shorthand_params(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process shorthand dot notation params (e.g., "job.param": value) and
        convert them to the standard nested format.
        
        Args:
            params: Dictionary of parameters that may contain shorthand notation
            
        Returns:
            Updated parameters dictionary with shorthand notation expanded
        """
        result = params.copy()
        dot_params = {}
        
        # Find and collect all shorthand dot notation parameters
        for key, value in params.items():
            if '.' in key and not key.startswith('fn.'):
                job_name, param_name = key.split('.', 1)
                if job_name not in dot_params:
                    dot_params[job_name] = {}
                dot_params[job_name][param_name] = value
                del result[key]  # Remove the dot notation key from the result
                
        # Merge the dot params with the existing job params
        for job_name, job_params in dot_params.items():
            if job_name in result:
                # If the job already exists in the params, merge the parameters
                result[job_name].update(job_params)
            else:
                # Otherwise, create a new entry
                result[job_name] = job_params
                
        return result
        
    def _create_callable_params(self, params: Dict[str, Any]) -> Dict[str, List[Any]]:
        """
        Extract and normalize parameters for the callable, supporting multiple styles.

        Args:
            params: Dictionary of parameters specific to this callable

        Returns:
            Dictionary containing 'args' and 'kwargs' keys
        """
        # Start with default values
        args = self.default_args.copy()
        kwargs = self.default_kwargs.copy()

        # Case 1: Explicit args list via 'fn.args' (legacy) or 'args' (new)
        if "fn.args" in params:
            args = params["fn.args"]
        elif "args" in params:
            args = params["args"]

        # Case 2: Explicit kwargs dictionary via 'fn.kwargs' (legacy) or 'kwargs' (new)
        if "fn.kwargs" in params:
            kwargs.update(params["fn.kwargs"])
        elif "kwargs" in params:
            kwargs.update(params["kwargs"])

        # Case 3: Named parameters with fn. prefix (legacy)
        for key, value in params.items():
            if key.startswith("fn.") and key not in ["fn.args", "fn.kwargs"]:
                param_name = key[3:]  # Remove "fn." prefix
                kwargs[param_name] = value
            # Case 4: Direct parameter passing (new)
            elif key not in ["args", "kwargs"]:
                kwargs[key] = value

        return {"args": args, "kwargs": kwargs}

    def _validate_params(self, args: List[Any], kwargs: Dict[str, Any]) -> None:
        """
        Validate that the provided parameters match the callable's signature.

        Args:
            args: Positional arguments to validate
            kwargs: Keyword arguments to validate

        Raises:
            ValueError: If parameters don't match the callable's signature
        """
        sig = inspect.signature(self.callable)
        try:
            sig.bind(*args, **kwargs)
        except TypeError as e:
            raise ValueError(f"Invalid parameters for {self.name}: {e}")

    def _convert_param_types(self, args: List[Any], kwargs: Dict[str, Any]) -> tuple:
        """
        Convert parameter types based on the callable's type annotations.

        Args:
            args: Positional arguments to convert
            kwargs: Keyword arguments to convert

        Returns:
            Tuple containing converted args and kwargs
        """
        sig = inspect.signature(self.callable)

        # Convert positional args
        converted_args = []
        for i, arg in enumerate(args):
            # Skip conversion if we have more args than parameters
            if i >= len(sig.parameters):
                converted_args.append(arg)
                continue

            param_name = list(sig.parameters.keys())[i]
            param = sig.parameters[param_name]

            # Skip if no annotation or if annotation is not a type
            if param.annotation == inspect.Parameter.empty or not isinstance(param.annotation, type):
                converted_args.append(arg)
                continue

            try:
                # Only convert if needed and possible
                if not isinstance(arg, param.annotation) and arg is not None:
                    converted_args.append(param.annotation(arg))
                else:
                    converted_args.append(arg)
            except (ValueError, TypeError):
                # If conversion fails, use original value
                converted_args.append(arg)

        # Convert kwargs
        converted_kwargs = {}
        for name, value in kwargs.items():
            if (name in sig.parameters and
                sig.parameters[name].annotation != inspect.Parameter.empty and
                isinstance(sig.parameters[name].annotation, type)):

                try:
                    # Only convert if needed and possible
                    if not isinstance(value, sig.parameters[name].annotation) and value is not None:
                        converted_kwargs[name] = sig.parameters[name].annotation(value)
                    else:
                        converted_kwargs[name] = value
                except (ValueError, TypeError):
                    # If conversion fails, use original value
                    converted_kwargs[name] = value
            else:
                converted_kwargs[name] = value

        return converted_args, converted_kwargs

    async def _execute_callable(self, args: List[Any], kwargs: Dict[str, Any]) -> Any:
        """
        Execute the callable with the given parameters.

        Args:
            args: Positional arguments to pass
            kwargs: Keyword arguments to pass

        Returns:
            Result of the callable execution
        """
        result = self.callable(*args, **kwargs)

        # Check if the result is a coroutine (from an async function)
        if inspect.iscoroutine(result):
            # Await the coroutine to get the actual result
            return await result

        return result



================================================
FILE: src/flow4ai/resources/__init__.py
================================================



================================================
FILE: src/flow4ai/resources/otel_config.yaml
================================================
exporter: file  # Default exporter is file; can be overridden by OTEL_TRACES_EXPORTER env variable.
service_name: MyService  # Can be overridden by OTEL_SERVICE_NAME env variable.
batch_processor:
  max_queue_size: 1000  # Batch processor will handle up to 1000 spans in queue.
  schedule_delay_millis: 1000  # 1-second timeout for exporting spans.
file_exporter:
  path: "~/.Flow4AI/otel_trace.json"  # Default path for trace export
  max_size_bytes: 5242880  # 5MB (5 * 1024 * 1024 bytes)
  rotation_time_days: 1  # Rotate daily



================================================
FILE: src/flow4ai/utils/__init__.py
================================================
# Make utils a Python package
from .otel_wrapper import TracerFactory, trace_function
from .timing import timing_decorator

__all__ = ['TracerFactory', 'trace_function', 'timing_decorator']



================================================
FILE: src/flow4ai/utils/api_utils.py
================================================
"""
Utility functions for handling API keys and authentication.
"""
import os
from typing import Any, Dict, Optional

from dotenv import load_dotenv

from flow4ai import f4a_logging as logging

logger = logging.getLogger(__name__)


def get_api_key(params: Optional[Dict[str, Any]] = None, 
               env_file: str = "api.env",
               key_name: str = None,
               required: bool = True) -> Optional[str]:
    """
    Resolves and returns an API key from parameters or environment variables.
    
    Args:
        params: Dictionary of parameters that may contain an 'api_key' entry
        env_file: Path to the .env file to load (defaults to "api.env")
        key_name: Default environment variable name to use if not specified in params
        required: Whether to raise an error if the API key is not found
        
    Returns:
        The API key string or None if not required and not found
        
    Raises:
        ValueError: If required is True and the API key is not found
    """
    # Initialize params if None
    params = params or {}
    
    # Load environment variables from env file
    load_dotenv(env_file)
    
    # Extract key name from params or use default
    env_var = params.pop("api_key", None) if params else key_name
    
    # Resolve API key: either from the env var specified in params or from default env var
    api_key = os.getenv(env_var)
    logger.info(f"Resolved API Key exists: {bool(api_key)}")
    
    # Check if the API key is not set and raise an error if required
    if not api_key and required:
        raise ValueError("API key is not set. Please provide an API key.")
        
    return api_key





================================================
FILE: src/flow4ai/utils/llm_utils.py
================================================
import re
import string

from flow4ai import f4a_logging as logging

logger = logging.getLogger(__name__)

def clean_prompt(text):
    # Keep only printable characters
    return ''.join(char for char in text if char in string.printable)


def clean_prompt(text):
    if not isinstance(text, str):
        logger.error("Input must be a string")
        raise ValueError("Input must be a string")
    
    # Remove control characters but keep normal whitespace
    cleaned_1 = ''.join(char for char in text if ord(char) >= 32 or char in '\n\r\t')
    cleaned = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F]', '', cleaned_1)
    # Optional: Check if the text was modified
    if cleaned != text:
        logger.info("Characters were cleaned from the prompt")
    
    # Optional: Ensure the text isn't empty after cleaning
    if not cleaned.strip():
        logger.error("Prompt is empty after cleaning")
        raise ValueError("Prompt is empty after cleaning")
    
    return cleaned

def check_response_errors(response:dict):
    if response.get("error"):
        logger.error(f"Response has an error: {response}")
        raise ValueError("Response has an error")
    elif response.get("status"):
        status = response.get("status")
        if status == "error":
            logger.error(f"Response has an error: {response}")
            raise ValueError("Response has an error")



================================================
FILE: src/flow4ai/utils/monitor_utils.py
================================================
"""Utilities for monitoring and logging task progress."""
import asyncio

NO_CHANGE_LOG_INTERVAL = 1.0

def should_log_task_stats(monitor_fn, tasks_created: int, tasks_completed: int) -> bool:
    """Check if task stats should be logged based on changes or time elapsed.
    
    Args:
        monitor_fn: The monitoring function to store state on
        tasks_created: Current count of created tasks
        tasks_completed: Current count of completed tasks
        
    Returns:
        bool: True if stats should be logged
    """
    if not hasattr(monitor_fn, '_last_log_time'):
        monitor_fn._last_log_time = 0
        monitor_fn._last_tasks_created = -1
        monitor_fn._last_tasks_completed = -1
    
    current_time = asyncio.get_event_loop().time()
    counts_changed = (tasks_created != monitor_fn._last_tasks_created or 
                     tasks_completed != monitor_fn._last_tasks_completed)
    
    should_log = counts_changed or (current_time - monitor_fn._last_log_time) >= NO_CHANGE_LOG_INTERVAL
    
    if should_log:
        monitor_fn._last_log_time = current_time
        monitor_fn._last_tasks_created = tasks_created
        monitor_fn._last_tasks_completed = tasks_completed
        
    return should_log



================================================
FILE: src/flow4ai/utils/otel_wrapper.py
================================================
import inspect
import json
import os
from functools import wraps
from importlib import resources
from threading import Lock
from typing import Any, Dict, Optional, Sequence

import yaml
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import \
    OTLPSpanExporter
from opentelemetry.sdk.trace import ReadableSpan, TracerProvider
from opentelemetry.sdk.trace.export import (BatchSpanProcessor,
                                            ConsoleSpanExporter, SpanExporter,
                                            SpanExportResult)

# Explicitly define exports
__all__ = ['TracerFactory', 'trace_function', 'AsyncFileExporter']
DEFAULT_OTEL_CONFIG = "otel_config.yaml"

class AsyncFileExporter(SpanExporter):
    """Asynchronous file exporter for OpenTelemetry spans with log rotation support."""
    
    def __init__(self, filepath: str, max_size_bytes: int = None, rotation_time_days: int = None):
        """Initialize the exporter with the target file path and rotation settings.
        
        Args:
            filepath: Path to the file where spans will be exported
            max_size_bytes: Maximum file size in bytes before rotation
            rotation_time_days: Number of days before rotating file
        """
        self.filepath = os.path.expanduser(filepath)
        self.max_size_bytes = max_size_bytes
        self.rotation_time_days = rotation_time_days
        self.last_rotation_time = None
        
        # Ensure directory exists
        os.makedirs(os.path.dirname(self.filepath), exist_ok=True)
        self._export_lock = Lock()
        
        # Initialize file with empty array if it doesn't exist
        if not os.path.exists(self.filepath):
            with open(self.filepath, 'w') as f:
                json.dump([], f)
                
        # Record initial rotation time
        if self.rotation_time_days:
            self.last_rotation_time = os.path.getmtime(self.filepath)
    
    def _should_rotate(self, additional_size: int = 0) -> bool:
        """Check if file should be rotated based on size or time.
        
        Args:
            additional_size: Additional size in bytes that will be added
        """
        if not os.path.exists(self.filepath):
            return False
            
        should_rotate = False
        
        # Check size-based rotation
        if self.max_size_bytes:
            current_size = os.path.getsize(self.filepath)
            if (current_size + additional_size) >= self.max_size_bytes:
                should_rotate = True
                
        # Check time-based rotation
        if self.rotation_time_days and self.last_rotation_time:
            current_time = os.path.getmtime(self.filepath)
            days_elapsed = (current_time - self.last_rotation_time) / (24 * 3600)
            if days_elapsed >= self.rotation_time_days:
                should_rotate = True
                
        return should_rotate
    
    def _rotate_file(self):
        """Rotate the current file if it exists."""
        if not os.path.exists(self.filepath):
            return
            
        # Generate rotation suffix based on timestamp
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        rotated_path = f"{self.filepath}.{timestamp}"
        
        # Rotate the file
        os.rename(self.filepath, rotated_path)
        
        # Create new empty file
        with open(self.filepath, 'w') as f:
            json.dump([], f)
            
        # Update rotation time
        self.last_rotation_time = os.path.getmtime(self.filepath)
    
    def _serialize_span(self, span: ReadableSpan) -> dict:
        """Convert a span to a JSON-serializable dictionary.
        
        Args:
            span: The span to serialize
        Returns:
            dict: JSON-serializable representation of the span
        """
        return {
            'name': span.name,
            'context': {
                'trace_id': format(span.context.trace_id, '032x'),
                'span_id': format(span.context.span_id, '016x'),
            },
            'parent_id': format(span.parent.span_id, '016x') if span.parent else None,
            'start_time': span.start_time,
            'end_time': span.end_time,
            'attributes': dict(span.attributes),
            'events': [
                {
                    'name': event.name,
                    'timestamp': event.timestamp,
                    'attributes': dict(event.attributes)
                }
                for event in span.events
            ],
            'status': {
                'status_code': str(span.status.status_code),
                'description': span.status.description
            }
        }

    def export(self, spans: Sequence[ReadableSpan]) -> SpanExportResult:
        """Export spans to file with rotation support.
        
        Args:
            spans: Sequence of spans to export
        Returns:
            SpanExportResult indicating success or failure
        """
        try:
            with self._export_lock:
                # Create serializable span data
                span_data = [self._serialize_span(span) for span in spans]
                
                # Read existing spans
                try:
                    with open(self.filepath, 'r') as f:
                        try:
                            existing_spans = json.load(f)
                        except json.JSONDecodeError:
                            existing_spans = []
                except FileNotFoundError:
                    existing_spans = []
                
                # Calculate size of new data
                new_data = existing_spans + span_data
                new_data_str = json.dumps(new_data, indent=2)
                additional_size = len(new_data_str.encode('utf-8'))
                
                # Check rotation after calculating new size
                if self._should_rotate(additional_size - os.path.getsize(self.filepath) if os.path.exists(self.filepath) else 0):
                    self._rotate_file()
                    existing_spans = []
                
                # Append new spans
                existing_spans.extend(span_data)
                
                # Write all spans back to file
                temp_file = f"{self.filepath}.tmp"
                try:
                    with open(temp_file, 'w') as f:
                        json.dump(existing_spans, f, indent=2)
                    # Atomic replace
                    os.replace(temp_file, self.filepath)
                finally:
                    if os.path.exists(temp_file):
                        os.unlink(temp_file)
                
            return SpanExportResult.SUCCESS
        except Exception as e:
            print(f"Error exporting spans to file: {e}")
            return SpanExportResult.FAILURE

    def shutdown(self) -> None:
        """Shutdown the exporter."""
        pass

class TestTracerProvider(TracerProvider):
    _instance = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(TestTracerProvider, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if not hasattr(self, '_initialized'):
            super().__init__()
            self._initialized = True
            
    def get_tracer(
        self,
        instrumenting_module_name: str,
        instrumenting_library_version: str = None,
        schema_url: str = None,
        attributes: dict = None,
    ) -> trace.Tracer:
        """Get a tracer for use in tests.
        
        Args:
            instrumenting_module_name: The name of the instrumenting module
            instrumenting_library_version: Optional version of the instrumenting module
            schema_url: Optional URL of the OpenTelemetry schema
            attributes: Optional attributes to add to the tracer
            
        Returns:
            A tracer instance for use in tests
        """
        return super().get_tracer(
            instrumenting_module_name,
            instrumenting_library_version,
            schema_url,
            attributes,
        )

# Singleton TracerFactory
class TracerFactory:
    _instance = None
    _config = None
    _lock = Lock()
    _is_test_mode = False
    
    @classmethod
    def set_test_mode(cls, enabled: bool = True):
        """Enable or disable test mode.
        
        Args:
            enabled: Whether to enable test mode
        """
        cls._is_test_mode = enabled
        cls._instance = None  # Reset instance to force recreation with new provider
    
    @classmethod
    def _load_config(cls, yaml_file=None):
        """Load configuration from YAML file.
        
        Args:
            yaml_file: Optional path override for the YAML configuration file
        Returns:
            dict: Configuration dictionary
        """
       # First try yaml_file parameter
        config_path = yaml_file
        if not config_path:
            # Then try environment variable
            config_path = os.environ.get('FLOW4AI_OT_CONFIG', "")
        
        if not config_path:
            # Finally use default path from package resources
            try:
                # Using files() instead of deprecated path() method
                config_path = str(resources.files('flow4ai.resources').joinpath(DEFAULT_OTEL_CONFIG))
            except Exception as e:
                raise RuntimeError(f"Could not find {DEFAULT_OTEL_CONFIG} in package resources: {e}")
        
        with open(config_path, 'r') as file:
                cls._config = yaml.safe_load(file)
        return cls._config
    
    @classmethod
    def get_tracer(cls, config=None):
        """Get or create the tracer instance.
        
        Args:
            config: Optional configuration override. If not provided, loads from file.
        Returns:
            Tracer instance
        """
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    # Use provided config or load from file
                    cfg = config if config is not None else cls._load_config()
                    
                    # Use TestTracerProvider in test mode
                    provider = TestTracerProvider() if cls._is_test_mode else TracerProvider()
                    
                    # Configure main exporter
                    main_exporter = cls._configure_exporter(cfg['exporter'])
                    batch_processor = BatchSpanProcessor(
                        main_exporter,
                        max_queue_size=cfg['batch_processor']['max_queue_size'],
                        schedule_delay_millis=cfg['batch_processor']['schedule_delay_millis']
                    )
                    provider.add_span_processor(batch_processor)
                    
                    trace.set_tracer_provider(provider)
                    cls._instance = trace.get_tracer(cfg["service_name"])
        return cls._instance

    @staticmethod
    def _configure_exporter(exporter_type):
        """Configure the appropriate exporter based on type.
        
        Args:
            exporter_type: Type of exporter to configure
        Returns:
            Configured exporter instance
        """
        if exporter_type == "otlp":
            return OTLPSpanExporter()  # OTEL_EXPORTER_OTLP_... environment variables apply here
        elif exporter_type == "console":
            return ConsoleSpanExporter()  # OTEL_EXPORTER_CONSOLE_... environment variables apply here
        elif exporter_type == "file":
            # Load config to get file path
            config = TracerFactory._load_config()
            file_path = config.get('file_exporter', {}).get('path', "~/.Flow4AI/otel_trace.json")
            max_size_bytes = config.get('file_exporter', {}).get('max_size_bytes')
            rotation_time_days = config.get('file_exporter', {}).get('rotation_time_days')
            return AsyncFileExporter(file_path, max_size_bytes, rotation_time_days)
        else:
            raise ValueError("Unsupported exporter type")

    @classmethod
    def trace(cls, message: str, detailed_trace: bool = False, attributes: Optional[Dict[str, Any]] = None):
        """Trace a message with OpenTelemetry tracing.
        
        Args:
            message: The message to trace
            detailed_trace: Whether to include detailed tracing information (args, kwargs, object fields)
            attributes: Optional dictionary of additional attributes to add to the span
        """
        tracer = cls.get_tracer()
        
        # Get the calling frame
        frame = inspect.currentframe()
        if frame:
            caller_frame = frame.f_back
            if caller_frame:
                # Get function info
                func_name = caller_frame.f_code.co_name
                module_name = inspect.getmodule(caller_frame).__name__ if inspect.getmodule(caller_frame) else "__main__"
                
                # Get local variables including 'self' if it exists
                local_vars = caller_frame.f_locals
                args = []
                kwargs = {}
                
                # If this is a method call (has 'self')
                if 'self' in local_vars:
                    args.append(local_vars['self'])
                    # Add other arguments if they exist
                    if len(local_vars) > 1:
                        # Filter out 'self' and get remaining arguments
                        args.extend([v for k, v in local_vars.items() if k != 'self'])
                
                span_name = f"{module_name}.{func_name}"
                with tracer.start_as_current_span(span_name) as span:
                    span.set_attribute("trace.message", message)
                    if detailed_trace:
                        span.set_attribute("function.args", str(tuple(args)))
                        span.set_attribute("function.kwargs", str(kwargs))
                        if args and hasattr(args[0], "__dict__"):
                            span.set_attribute("object.fields", str(vars(args[0])))
                    if attributes:
                        for key, value in attributes.items():
                            span.set_attribute(key, str(value))
                    print(message)
                
                # Clean up
                del frame
                del caller_frame
                return
        
        # Fallback if not in a function context
        with tracer.start_as_current_span("trace_message") as span:
            span.set_attribute("trace.message", message)
            if attributes:
                for key, value in attributes.items():
                    span.set_attribute(key, str(value))
            print(message)

# Decorator for OpenTelemetry tracing
def trace_function(func=None, *, detailed_trace: bool = False, attributes: Optional[Dict[str, Any]] = None):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            tracer = TracerFactory.get_tracer()
            span_name = f"{func.__module__}.{func.__name__}"
            with tracer.start_as_current_span(span_name) as span:
                # Record function arguments only if detailed_trace is True
                if detailed_trace:
                    span.set_attribute("function.args", str(args))
                    span.set_attribute("function.kwargs", str(kwargs))
                    if args and hasattr(args[0], "__dict__"):
                        span.set_attribute("object.fields", str(vars(args[0])))
                if attributes:
                    for key, value in attributes.items():
                        span.set_attribute(key, str(value))
                try:
                    result = func(*args, **kwargs)
                    return result
                except Exception as e:
                    span.record_exception(e)
                    raise
        return wrapper
    
    if func is None:
        return decorator
    return decorator(func)



================================================
FILE: src/flow4ai/utils/print_utils.py
================================================
from .. import f4a_logging as logging


def printh(text):
    """
    Log the given text surrounded by asterisks.
    """
    logger = logging.getLogger('PrintUtils')
    logger.info("*** " + text + " ***")



================================================
FILE: src/flow4ai/utils/timing.py
================================================
import time
from functools import wraps


def timing_decorator(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.perf_counter()
        result = func(*args, **kwargs)
        end_time = time.perf_counter()
        elapsed_time = end_time - start_time
        print(f"Elapsed time: {elapsed_time:.6f} seconds")
        return result
    return wrapper



================================================
FILE: tests/README.md
================================================
# Test Suite Documentation

## Running Tests

### Running All Tests
To run all tests:
```bash
python3 -m pytest
```
Note: By default, performance-intensive tests (like those in `test_parallel_load.py`) will be skipped to ensure quick test execution during normal development.

### Running All Tests Including Performance Tests
To run all tests including performance tests:
```bash
python3 -m pytest --full-suite
```
This will execute all tests, including the normally skipped performance and parallel load tests.

### Running Specific Tests
When running specific test files or individual tests that are normally skipped (like parallel load tests), you must include the `--full-suite` flag:

```bash
# Running a specific test file with skipped tests
python3 -m pytest tests/test_parallel_load.py -v --full-suite

# Running a specific test function
python3 -m pytest tests/test_parallel_load.py::test_maximum_parallel_file_trace -v --full-suite
```

The `--full-suite` flag is only required when running tests that are normally skipped. Regular tests can be run individually without this flag:

```bash
# Running a regular test file
python3 -m pytest tests/test_job_loader.py -v

# Running a specific regular test
python3 -m pytest tests/test_job_loader.py::test_load_job -v
```

## Test Categories

### Regular Tests
All the test modules run by default except test_parallel_load.py which takes a little more time than the other tests.

### Performance Tests (Requires --full-suite)
These tests are skipped by default as they are time and resource-intensive:
- test_parallel_load.py

These tests are valuable for verifying system performance and stability but are separated to maintain fast test execution during normal development.

To add more files to the skipped tests list, update the following section in `conftest.py`:
```python
def pytest_collection_modifyitems(config, items):
    if not config.getoption("--full-suite"):
        # Skip load tests by default
        for item in items:
            if "test_parallel_load.py" in str(item.fspath):
                item.add_marker(pytest.mark.skip(reason="Load test - use --full-suite to include"))
```
Additional test files can be added to the skip condition using the same pattern.



================================================
FILE: tests/conftest.py
================================================
import pytest

from flow4ai.utils.otel_wrapper import TracerFactory

# set the TracerFactory up with the TestTracerProvider so the TracerProvider
#  can be overridden by code which normally isn't possible.
TracerFactory.set_test_mode(True)

def pytest_configure(config):
    config.addinivalue_line(
        "markers", "isolated: marks tests to run in isolation"
    )

def pytest_addoption(parser):
    parser.addoption(
        "--full-suite",
        action="store_true",
        default=False,
        help="run full test suite including load tests"
    )
    parser.addoption(
        "--isolated",
        action="store_true",
        default=False,
        help="run only tests marked as isolated"
    )

def pytest_collection_modifyitems(config, items):
    # Handle isolated tests
    # if config.getoption("--isolated"):
    #     selected = []
    #     deselected = []
    #     for item in items:
    #         if item.get_closest_marker("isolated"):
    #             selected.append(item)
    #         else:
    #             deselected.append(item)
    #     config.hook.pytest_deselected(items=deselected)
    #     items[:] = selected
    #     return
    # else:
    #     # Skip isolated tests when not running with --isolated
    #     for item in items:
    #         if item.get_closest_marker("isolated"):
    #             item.add_marker(pytest.mark.skip(reason="Isolated test - use --isolated to run"))

    # Handle full suite option
    if not config.getoption("--full-suite"):
        # Skip load tests by default
        for item in items:
            if "test_fmmp_parallel_load.py" in str(item.fspath) or item.function.__name__ == "test_parallel_load":
                item.add_marker(pytest.mark.skip(reason="Load test - use --full-suite to include"))



================================================
FILE: tests/test_aa.py
================================================
from flow4ai.flowmanagerMP import FlowManagerMP
from flow4ai.job import JobABC
from tests.test_utils.simple_job import SimpleJob


class AsyncTestJob(JobABC):
    """Job to confirm the basics of async functionality are working: """
    def __init__(self):
        super().__init__(name="AsyncTestJob")
    
    async def run(self, task):
        if isinstance(task, dict) and task.get('fail'):
            raise ValueError("Simulated task failure")
        if isinstance(task, dict) and task.get('delay'):
            await asyncio.sleep(task['delay'])
        return {'task': task, 'completed': True}

def test_imports_are_working():
  job = SimpleJob("Test Job")
  flowmanagerMP = FlowManagerMP(job)
  testvar="test"
  assert testvar == "test"



================================================
FILE: tests/test_concurrency.py
================================================
import asyncio
import contextvars
import multiprocessing as mp
import os
from functools import partial
from typing import Any, Dict

import pytest

from flow4ai import f4a_logging as logging
from flow4ai.flowmanagerMP import FlowManagerMP
from flow4ai.job import JobABC, Task, job_graph_context_manager
from flow4ai.job_loader import ConfigLoader, JobFactory


def returns_collector(shared_results, result):
    shared_results.append(result)

#@pytest.mark.skip("Skipping test due to working yet")
@pytest.mark.asyncio
async def test_concurrency_by_expected_returns():
    # Create a manager for sharing the results list between processes
    manager = mp.Manager()
    shared_results = manager.list()
    
    # Create a partial function with our shared results list
    collector = partial(returns_collector, shared_results)
    
    # Set config directory for test
    config_dir = os.path.join(os.path.dirname(__file__), "test_configs/test_concurrency_by_returns")
    ConfigLoader._set_directories([config_dir])
    
    # Create FlowManagerMP with parallel processing
    flowmanagerMP = FlowManagerMP(result_processing_function=collector)

    def submit_task(range_val:int):
        for i in range(range_val):
            flowmanagerMP.submit_task({'task': f'{i}'})

    def check_results():
        for result in shared_results:
            #logging.info(f"Result: {result}")
            assert result['result'] == 'A.A.B.C.E.A.D.F.G'

        shared_results[:] = []  # Clear the shared_results using slice assignment
    

    submit_task(300)

    flowmanagerMP.wait_for_completion() # this waits for all results to be returned

    check_results()



class A(JobABC):
  async def run(self, task: Dict[str, Any]) -> Any:
    inputs = self.get_inputs()
    print(f"\nA expected inputs: {self.expected_inputs}")
    print(f"A data inputs: {inputs}")
    dataA:dict = {
        'dataA1': {},
        'dataA2': {}
    }
    print(f"A returned: {dataA}")
    return dataA

class B(JobABC):
  async def run(self, task: Dict[str, Any]) -> Any:
    inputs = self.get_inputs()
    print(f"\nB expected inputs: {self.expected_inputs}")
    print(f"B data inputs: {inputs}")
    dataB:dict = {
        'dataB1': {},
        'dataB2': {}
    }
    print(f"B returned: {dataB}")
    return dataB

class C(JobABC):
  async def run(self, task: Dict[str, Any]) -> Any:
    inputs = self.get_inputs()
    print(f"\nC expected inputs: {self.expected_inputs}")
    print(f"C data inputs: {inputs}")
    dataC:dict = {
        'dataC1': {},
        'dataC2': {}
    } 
    print(f"C returned: {dataC}")
    return dataC

class D(JobABC):
  async def run(self, task: Dict[str, Any]) -> Any:
    inputs = self.get_inputs()
    print(f"\nD expected inputs: {self.expected_inputs}")
    print(f"D data inputs: {inputs}")
    dataD:dict = {
        'dataD1': {},
        'dataD2': {}
    } 
    print(f"D returned: {dataD}")
    return dataD

jobs = {
    'A': A('A'),
    'B': B('B'),
    'C': C('C'),
    'D': D('D')
}

graph_definition1 = {
    'A': {'next': ['B', 'C']},
    'B': {'next': ['C', 'D']},
    'C': {'next': ['D']},
    'D': {'next': []}
} 



@pytest.mark.asyncio
async def test_simple_graph():
    head_job:JobABC = JobFactory.create_job_graph(graph_definition1, jobs)
    job_set = JobABC.job_set(head_job)
    # Create 50 tasks to run concurrently
    tasks = []
    for _ in range(50):
      async with job_graph_context_manager(job_set):
        task = asyncio.create_task(head_job._execute(Task({'1': {},'2': {}})))
        tasks.append(task)
    
    # Run all tasks concurrently and gather results
    results = await asyncio.gather(*tasks)
    
    # Verify each result matches the expected final output from job D
    for final_result in results:
        # Extract just the job result data, ignoring task_pass_through
        result_data = {k: v for k, v in final_result.items() if k not in ['task_pass_through', 'RETURN_JOB']}
        assert result_data == {
                'dataD1': {},
                'dataD2': {}
            }



================================================
FILE: tests/test_dsl.py
================================================
"""
Tests for the DSL (Domain Specific Language) module of Flow4AI.

This test suite covers:
- Wrapping callables with the wrap/w function
- Parallel composition using | operator and parallel/p function
- Serial composition using >> operator and serial/s function
- Graph evaluation with GraphCreator.evaluate
- WrappingJob functionality with mock LLM and data processing functions
- Integration with custom JobABC subclasses
"""

import json
import types
from unittest.mock import AsyncMock, MagicMock

import pytest

from flow4ai import f4a_logging as logging
from flow4ai.dsl import (DSLComponent, JobsDict, Parallel, Serial, p, parallel,
                         s, serial, w, wrap)
from flow4ai.job import JobABC
from flow4ai.jobs.wrapping_job import WrappingJob
from tests.test_utils.graph_evaluation import evaluate

logger = logging.getLogger(__name__)


# Define mock functions for testing
def mock_llm_completion() -> str:
    """Mock LLM completion function that returns a fixed response."""
    return "This is a response from the LLM model."

def mock_data_extraction() -> dict:
    """Mock data extraction function that returns structured data."""
    return {"entities": ["Apple", "Google", "Microsoft"], "sentiment": "positive"}

def mock_text_processing() -> str:
    """Mock text processing function that transforms text."""
    return "Processed text output"

def mock_json_formatter() -> str:
    """Mock JSON formatter function that formats data as JSON."""
    return json.dumps({"formatted": True, "timestamp": "2025-03-19"})

# Custom JobABC subclass implementations
class LLMSummarizer(JobABC):
    """A custom JobABC implementation that simulates summarizing text."""
    
    def __init__(self, name="LLMSummarizer"):
        super().__init__(name)
    
    async def run(self, task):
        return {"summary": "This is a summary of the input text."}

class DataProcessor(JobABC):
    """A custom JobABC implementation that processes data."""
    
    def __init__(self, name="DataProcessor"):
        super().__init__(name)
    
    async def run(self, task):
        return {"processed_data": [1, 2, 3, 4, 5], "status": "success"}

class TestWrapping:
    """Tests for the wrap/w function and WrappingJob class."""
    
    def test_wrap_callable(self):
        """Test wrapping a callable object."""
        wrapped = wrap(mock_llm_completion)
        assert isinstance(wrapped, WrappingJob)
        assert wrapped.callable == mock_llm_completion
    
    def test_wrap_job_object(self):
        """Test wrapping a JobABC object (should return the object unchanged)."""
        job = LLMSummarizer("test_summarizer")
        wrapped = wrap(job)
        assert wrapped is job  # Should return the same object
    
    def test_w_alias(self):
        """Test that w is an alias for wrap."""
        wrapped1 = wrap(mock_data_extraction)
        wrapped2 = w(mock_data_extraction)
        assert isinstance(wrapped1, WrappingJob)
        assert isinstance(wrapped2, WrappingJob)
        assert wrapped1.callable == wrapped2.callable
        
    def test_wrap_named_callable_kwargs(self):
        """Test wrapping a callable with a name using kwargs syntax."""
        wrapped = wrap(extractor=mock_data_extraction)
        assert isinstance(wrapped, WrappingJob)
        assert wrapped.callable == mock_data_extraction
        assert wrapped.name == "extractor"
        
    def test_wrap_named_callable_dict(self):
        """Test wrapping a callable with a name using dict syntax."""
        wrapped = wrap({"formatter": mock_json_formatter})
        assert isinstance(wrapped, WrappingJob)
        assert wrapped.callable == mock_json_formatter
        assert wrapped.name == "formatter"
        
    def test_wrap_job_object_with_name(self):
        """Test wrapping a JobABC object with a name (should set name and return the object)."""
        job = LLMSummarizer()
        wrapped = wrap(custom_summarizer=job)
        assert wrapped is job  # Should return the same object
        assert wrapped.name == "custom_summarizer"  # But with updated name
        
        # Test dict syntax as well
        job2 = DataProcessor()
        wrapped2 = wrap({"data_proc": job2})
        assert wrapped2 is job2
        assert wrapped2.name == "data_proc"
        
    def test_wrap_composite_with_name(self):
        """Test wrapping Serial/Parallel objects with names (should return object unchanged)."""
        # Create Serial and Parallel objects
        serial_obj = serial(mock_llm_completion, mock_text_processing)
        parallel_obj = parallel(mock_data_extraction, mock_json_formatter)
        
        # Wrap them with names
        wrapped_serial = wrap(pipeline=serial_obj)
        wrapped_parallel = wrap({"processors": parallel_obj})
        
        # Should return the same objects
        assert wrapped_serial is serial_obj
        assert wrapped_parallel is parallel_obj
        
    def test_wrap_multiple_objects(self):
        """Test wrapping multiple objects at once."""
        # Wrap multiple objects using kwargs syntax
        wrapped = wrap(
            extractor=mock_data_extraction,
            formatter=mock_json_formatter,
            processor=mock_text_processing
        )
        
        # Result should be a dictionary with wrapped objects
        assert isinstance(wrapped, dict)
        assert len(wrapped) == 3
        assert isinstance(wrapped["extractor"], WrappingJob)
        assert isinstance(wrapped["formatter"], WrappingJob)
        assert isinstance(wrapped["processor"], WrappingJob)
        assert wrapped["extractor"].callable == mock_data_extraction
        assert wrapped["formatter"].callable == mock_json_formatter
        assert wrapped["processor"].callable == mock_text_processing
        assert wrapped["extractor"].name == "extractor"
        assert wrapped["formatter"].name == "formatter"
        assert wrapped["processor"].name == "processor"
        
    def test_wrap_multiple_objects_dict(self):
        """Test wrapping multiple objects using dictionary syntax."""
        # Create a mix of different object types
        job1 = LLMSummarizer()
        job2 = DataProcessor()
        serial_obj = serial(mock_llm_completion, mock_text_processing)
        
        # Wrap them all in a dictionary
        wrapped = wrap({
            "summarizer": job1,
            "data_processor": job2,
            "formatter": mock_json_formatter,
            "pipeline": serial_obj
        })
        
        # Check the results
        assert isinstance(wrapped, dict)
        assert len(wrapped) == 4
        
        # JobABC instances should be the same objects with names set
        assert wrapped["summarizer"] is job1
        assert wrapped["summarizer"].name == "summarizer"
        assert wrapped["data_processor"] is job2
        assert wrapped["data_processor"].name == "data_processor"
        
        # Callable should be wrapped
        assert isinstance(wrapped["formatter"], WrappingJob)
        assert wrapped["formatter"].callable == mock_json_formatter
        assert wrapped["formatter"].name == "formatter"
        
        # Serial object should be returned as is
        assert wrapped["pipeline"] is serial_obj
        
    def test_wrap_single_item_in_collection(self):
        """Test that wrapping a single item in a collection returns just that item."""
        # With kwargs syntax
        wrapped1 = wrap(processor=mock_text_processing)
        assert isinstance(wrapped1, WrappingJob)
        assert wrapped1.name == "processor"
        
        # With dict syntax
        wrapped2 = wrap({"extractor": mock_data_extraction})
        assert isinstance(wrapped2, WrappingJob)
        assert wrapped2.name == "extractor"


class TestParallelComposition:
    """Tests for parallel composition using | operator and parallel/p function."""
    
    def test_parallel_operator(self):
        """Test parallel composition using | operator with callables."""
        # Create parallel composition of two callables
        composition = wrap(mock_llm_completion) | wrap(mock_data_extraction)
        
        assert isinstance(composition, Parallel)
        assert len(composition.components) == 2
        assert composition.components[0].callable == mock_llm_completion
        assert composition.components[1].callable == mock_data_extraction
    
    def test_parallel_with_job_objects(self):
        """Test parallel composition with JobABC objects."""
        # Create instances of custom JobABC subclasses
        llm_job = LLMSummarizer()
        data_job = DataProcessor()
        
        # Create parallel composition
        composition = llm_job | data_job
        
        assert isinstance(composition, Parallel)
        assert len(composition.components) == 2
        assert composition.components[0] is llm_job
        assert composition.components[1] is data_job
    
    def test_parallel_function(self):
        """Test parallel composition using parallel function."""
        # Create parallel composition with three callables
        composition = parallel(mock_llm_completion, mock_data_extraction, mock_text_processing)
        
        assert isinstance(composition, Parallel)
        assert len(composition.components) == 3
        callables = [mock_llm_completion, mock_data_extraction, mock_text_processing]
        for i, func in enumerate(callables):
            assert composition.components[i].callable == func
    
    def test_p_alias(self):
        """Test that p is an alias for parallel."""
        funcs = [mock_llm_completion, mock_data_extraction]
        
        composition1 = parallel(funcs)
        composition2 = p(funcs)
        
        assert isinstance(composition1, Parallel)
        assert isinstance(composition2, Parallel)
        assert len(composition1.components) == len(composition2.components)
    
    def test_parallel_empty_list(self):
        """Test that parallel raises ValueError for empty list."""
        with pytest.raises(ValueError):
            parallel([])
    
    def test_parallel_single_item(self):
        """Test that parallel with a single item returns a wrapped item."""
        result = parallel([mock_llm_completion])
        assert isinstance(result, WrappingJob)
        assert result.callable == mock_llm_completion
        
    def test_parallel_mixed_components(self):
        """Test parallel composition with a mix of JobABC subclasses and wrapped callables."""
        llm_job = LLMSummarizer()
        wrapped_func = wrap(mock_data_extraction)
        
        composition = llm_job | wrapped_func
        
        assert isinstance(composition, Parallel)
        assert len(composition.components) == 2
        assert composition.components[0] is llm_job
        assert composition.components[1] is wrapped_func
        
    def test_parallel_with_kwargs(self):
        """Test parallel composition using keyword arguments."""
        # Create parallel composition with named objects using kwargs
        composition = parallel(
            llm=mock_llm_completion,
            extractor=mock_data_extraction,
            processor=mock_text_processing
        )
        
        assert isinstance(composition, Parallel)
        assert len(composition.components) == 3
        
        # Check that each component is a WrappingJob with the correct name and callable
        assert isinstance(composition.components[0], WrappingJob)
        assert composition.components[0].name == "llm"
        assert composition.components[0].callable == mock_llm_completion
        
        assert isinstance(composition.components[1], WrappingJob)
        assert composition.components[1].name == "extractor"
        assert composition.components[1].callable == mock_data_extraction
        
        assert isinstance(composition.components[2], WrappingJob)
        assert composition.components[2].name == "processor"
        assert composition.components[2].callable == mock_text_processing
    
    def test_parallel_with_dict(self):
        """Test parallel composition using dictionary argument."""
        # Create parallel composition with named objects using dict syntax
        composition = parallel({
            "formatter": mock_json_formatter,
            "extractor": mock_data_extraction
        })
        
        assert isinstance(composition, Parallel)
        assert len(composition.components) == 2
        
        # Names should be preserved in the created WrappingJob objects
        components_by_name = {comp.name: comp for comp in composition.components}
        assert "formatter" in components_by_name
        assert "extractor" in components_by_name
        
        assert components_by_name["formatter"].callable == mock_json_formatter
        assert components_by_name["extractor"].callable == mock_data_extraction
        
    def test_parallel_mixed_job_types_with_names(self):
        """Test parallel composition with named JobABC objects and callables."""
        # Create a mix of JobABC objects and callables
        llm_job = LLMSummarizer()
        data_job = DataProcessor()
        
        # Create parallel composition with named objects
        composition = parallel(
            summarizer=llm_job,
            processor=data_job,
            formatter=mock_json_formatter
        )
        
        assert isinstance(composition, Parallel)
        assert len(composition.components) == 3
        
        # Names should be set correctly for all components
        components_by_name = {comp.name: comp for comp in composition.components}
        assert "summarizer" in components_by_name
        assert "processor" in components_by_name
        assert "formatter" in components_by_name
        
        # JobABC objects should be returned as is with names set
        assert components_by_name["summarizer"] is llm_job
        assert components_by_name["processor"] is data_job
        assert llm_job.name == "summarizer"
        assert data_job.name == "processor"
        
        # Callables should be wrapped
        assert isinstance(components_by_name["formatter"], WrappingJob)
        assert components_by_name["formatter"].callable == mock_json_formatter
        
    def test_parallel_single_kwarg(self):
        """Test parallel with a single keyword argument."""
        # Single named object should be returned directly (not in a Parallel object)
        result = parallel(processor=mock_text_processing)
        
        assert isinstance(result, WrappingJob)
        assert result.name == "processor"
        assert result.callable == mock_text_processing
        
    def test_parallel_single_dict_item(self):
        """Test parallel with a dictionary containing a single item."""
        # Single dictionary item should be returned directly
        result = parallel({"formatter": mock_json_formatter})
        
        assert isinstance(result, WrappingJob)
        assert result.name == "formatter"
        assert result.callable == mock_json_formatter
        
    def test_parallel_empty_kwargs(self):
        """Test that parallel raises ValueError for empty kwargs."""
        with pytest.raises(ValueError):
            parallel()
        
    def test_parallel_empty_dict(self):
        """Test that parallel raises ValueError for empty dictionary."""
        with pytest.raises(ValueError):
            parallel({})
            
    def test_parallel_invalid_object(self):
        """Test parallel with an object that cannot be wrapped."""
        # Create an invalid object (not callable)
        class InvalidObject:
            def __init__(self):
                self.run = None  # Not a callable
                
        invalid_obj = InvalidObject()
        
        # When we try to wrap a non-callable object in parallel with other valid objects,
        # it should raise a TypeError because WrappingJob validates that objects are callable
        with pytest.raises(TypeError) as excinfo:
            parallel(
                valid=mock_text_processing,
                invalid=invalid_obj
            )
            
        # Check that the error message indicates the issue is with the invalid object
        assert "WrappingJob will only wrap a callable" in str(excinfo.value)
        assert "InvalidObject" in str(excinfo.value)
        
    def test_parallel_with_jobabc_object(self):
        """Test parallel with a JobABC object that isn't callable but is valid."""
        # JobABC objects are valid for parallel even though they may not be callable
        # because they are handled specially in the wrap function
        llm_job = LLMSummarizer()
        
        # This should work fine
        composition = parallel(
            processor=mock_text_processing,
            summarizer=llm_job
        )
        
        assert isinstance(composition, Parallel)
        assert len(composition.components) == 2
        
        components_by_name = {comp.name: comp for comp in composition.components}
        assert "processor" in components_by_name
        assert "summarizer" in components_by_name
        
        # The JobABC object should be used directly, not wrapped
        assert components_by_name["summarizer"] is llm_job
        assert llm_job.name == "summarizer"


class TestSerialComposition:
    """Tests for serial composition using >> operator and serial/s function."""
    
    def test_serial_operator(self):
        """Test serial composition using >> operator with callables."""
        # Create a serial pipeline that simulates text extraction followed by formatting
        composition = wrap(mock_text_processing) >> wrap(mock_json_formatter)
        
        assert isinstance(composition, Serial)
        assert len(composition.components) == 2
        assert composition.components[0].callable == mock_text_processing
        assert composition.components[1].callable == mock_json_formatter
    
    def test_serial_with_job_objects(self):
        """Test serial composition with JobABC objects."""
        # Create a pipeline that first summarizes and then processes data
        llm_job = LLMSummarizer()
        data_job = DataProcessor()
        
        # Create serial composition
        composition = llm_job >> data_job
        
        assert isinstance(composition, Serial)
        assert len(composition.components) == 2
        assert composition.components[0] is llm_job
        assert composition.components[1] is data_job
    
    def test_serial_function(self):
        """Test serial composition using serial function."""
        # Create a three-stage pipeline: extraction -> processing -> formatting
        composition = serial(mock_data_extraction, mock_text_processing, mock_json_formatter)
        
        assert isinstance(composition, Serial)
        assert len(composition.components) == 3
        funcs = [mock_data_extraction, mock_text_processing, mock_json_formatter]
        for i, func in enumerate(funcs):
            assert composition.components[i].callable == func
    
    def test_s_alias(self):
        """Test that s is an alias for serial."""
        funcs = [mock_llm_completion, mock_json_formatter]
        
        composition1 = serial(funcs)
        composition2 = s(funcs)
        
        assert isinstance(composition1, Serial)
        assert isinstance(composition2, Serial)
        assert len(composition1.components) == len(composition2.components)
    
    def test_serial_empty_list(self):
        """Test that serial raises ValueError for empty list."""
        with pytest.raises(ValueError):
            serial([])
    
    def test_serial_single_item(self):
        """Test that serial with a single item returns a wrapped item."""
        result = serial(mock_llm_completion)
        assert isinstance(result, WrappingJob)
        assert result.callable == mock_llm_completion
        
    def test_serial_mixed_components(self):
        """Test serial composition with a mix of JobABC subclasses and wrapped callables."""
        # Create a pipeline: wrapped callable -> JobABC subclass
        wrapped_func = wrap(mock_llm_completion)
        data_job = DataProcessor()
        
        composition = wrapped_func >> data_job
        
        assert isinstance(composition, Serial)
        assert len(composition.components) == 2
        assert composition.components[0] is wrapped_func
        assert composition.components[1] is data_job
        
    def test_serial_with_kwargs(self):
        """Test serial composition using kwargs syntax."""
        # Create a three-stage pipeline with named components
        composition = serial(
            extractor=mock_data_extraction,
            processor=mock_text_processing,
            formatter=mock_json_formatter
        )
        
        assert isinstance(composition, Serial)
        assert len(composition.components) == 3
        
        # Verify each component is properly wrapped and named
        component_dict = {comp.name: comp for comp in composition.components}
        assert "extractor" in component_dict
        assert "processor" in component_dict
        assert "formatter" in component_dict
        assert component_dict["extractor"].callable == mock_data_extraction
        assert component_dict["processor"].callable == mock_text_processing
        assert component_dict["formatter"].callable == mock_json_formatter
        
    def test_serial_with_dict(self):
        """Test serial composition using dictionary syntax."""
        # Create a pipeline with dictionary syntax
        composition = serial({
            "extractor": mock_data_extraction,
            "processor": mock_text_processing,
            "formatter": mock_json_formatter
        })
        
        assert isinstance(composition, Serial)
        assert len(composition.components) == 3
        
        # Verify each component is properly wrapped and named
        component_dict = {comp.name: comp for comp in composition.components}
        assert "extractor" in component_dict
        assert "processor" in component_dict
        assert "formatter" in component_dict
        assert component_dict["extractor"].callable == mock_data_extraction
        assert component_dict["processor"].callable == mock_text_processing
        assert component_dict["formatter"].callable == mock_json_formatter
    
    def test_serial_invalid_object(self):
        """Test that serial raises TypeError for non-callable objects."""
        # Create an invalid object that is not callable
        class InvalidObject:
            pass
            
        invalid_obj = InvalidObject()
        
        # Test using kwargs syntax
        with pytest.raises(TypeError) as excinfo:
            serial(
                valid=mock_text_processing,
                invalid=invalid_obj
            )
            
        # Check that the error message indicates the issue is with the invalid object
        assert "WrappingJob will only wrap a callable" in str(excinfo.value)
        assert "InvalidObject" in str(excinfo.value)
        
        # Test using dict syntax
        with pytest.raises(TypeError) as excinfo:
            serial({
                "valid": mock_text_processing,
                "invalid": invalid_obj
            })
            
        # Check that the error message indicates the issue is with the invalid object
        assert "WrappingJob will only wrap a callable" in str(excinfo.value)
        assert "InvalidObject" in str(excinfo.value)
        
    def test_serial_with_jobabc_object(self):
        """Test serial with a JobABC object that isn't callable but is valid."""
        # JobABC objects are valid for serial even though they may not be callable
        # because they are handled specially in the wrap function
        llm_job = LLMSummarizer()
        
        # This should work fine with kwargs syntax
        composition = serial(
            processor=mock_text_processing,
            summarizer=llm_job
        )
        
        assert isinstance(composition, Serial)
        assert len(composition.components) == 2
        
        components_by_name = {comp.name: comp for comp in composition.components}
        assert "processor" in components_by_name
        assert "summarizer" in components_by_name
        assert components_by_name["summarizer"] is llm_job
        assert components_by_name["summarizer"].name == "summarizer"
        
        # This should also work with dict syntax
        composition2 = serial({
            "processor": mock_text_processing,
            "summarizer": llm_job
        })
        
        assert isinstance(composition2, Serial)
        assert len(composition2.components) == 2
        
        components_by_name2 = {comp.name: comp for comp in composition2.components}
        assert "processor" in components_by_name2
        assert "summarizer" in components_by_name2
        assert components_by_name2["summarizer"] is llm_job
        assert components_by_name2["summarizer"].name == "summarizer"
    
    def test_serial_single_kwarg(self):
        """Test that serial with a single kwarg returns a wrapped item with the correct name."""
        # With kwargs syntax
        result = serial(processor=mock_text_processing)
        assert isinstance(result, WrappingJob)
        assert result.callable == mock_text_processing
        assert result.name == "processor"
    
    def test_serial_single_dict_item(self):
        """Test that serial with a single dict item returns a wrapped item with the correct name."""
        # With dict syntax
        result = serial({"extractor": mock_data_extraction})
        assert isinstance(result, WrappingJob)
        assert result.callable == mock_data_extraction
        assert result.name == "extractor"
    
    def test_serial_empty_kwargs(self):
        """Test that serial raises ValueError for empty kwargs."""
        with pytest.raises(ValueError):
            serial()
    
    def test_serial_empty_dict(self):
        """Test that serial raises ValueError for empty dict."""
        with pytest.raises(ValueError):
            serial({})
    
    def test_serial_composite_with_kwargs(self):
        """Test serial with mix of Serial/Parallel objects and callables using kwargs."""
        # Create some nested compositions
        parallel_obj = parallel(mock_data_extraction, mock_llm_completion)
        
        # Mix with callables in kwargs format
        composition = serial(
            extractors=parallel_obj,
            processor=mock_text_processing,
            formatter=mock_json_formatter
        )
        
        assert isinstance(composition, Serial)
        assert len(composition.components) == 3
        
        # Find each component by examining them directly
        # Parallel objects won't have a name attribute
        extractors_component = None
        processor_component = None
        formatter_component = None
        
        for comp in composition.components:
            if isinstance(comp, Parallel):
                extractors_component = comp
            elif isinstance(comp, WrappingJob):
                if comp.name == "processor":
                    processor_component = comp
                elif comp.name == "formatter":
                    formatter_component = comp
        
        # Verify each component is as expected
        assert extractors_component is parallel_obj
        assert processor_component is not None
        assert formatter_component is not None
        assert processor_component.callable == mock_text_processing
        assert formatter_component.callable == mock_json_formatter

@pytest.mark.asyncio
class TestGraphEvaluation:
    """Tests for graph evaluation with various job types."""
    
    async def test_evaluate_single_callable(self):
        """Test evaluating a single wrapped callable."""
        # Create a simple callable that returns a fixed value
        mock_callable = MagicMock(return_value="callable result")
        job = WrappingJob(mock_callable, name="test_callable")
        
        # Set up context access to work in tests
        job.get_task = MagicMock(return_value={job.name: {}})
        
        result = await evaluate(job)
        assert result == "0) callable result"
    
    async def test_evaluate_single_job(self):
        """Test evaluating a custom JobABC subclass."""
        job = LLMSummarizer()
        
        result = await evaluate(job)
        logger.info(result)
        assert "summary" in result
    
    async def test_evaluate_parallel_callables(self):
        """Test evaluating a parallel composition of callables."""
        # Create mockable callables for testing
        mock_callable1 = MagicMock(return_value="result1")
        mock_callable2 = MagicMock(return_value="result2")
        
        job1 = WrappingJob(mock_callable1, name="callable1")
        job2 = WrappingJob(mock_callable2, name="callable2")
        
        # Set up context access to work in tests
        job1.get_task = MagicMock(return_value={job1.name: {}})
        job2.get_task = MagicMock(return_value={job2.name: {}})
        
        composition = job1 | job2
        
        result = await evaluate(composition)
        logger.info(result)
        assert "Executed in parallel" in result
        assert "result1" in result
        assert "result2" in result
    
    async def test_evaluate_serial_callables(self):
        """Test evaluating a serial composition of callables."""
        # Create mockable callables for testing
        mock_callable1 = MagicMock(return_value="result1")
        mock_callable2 = MagicMock(return_value="result2")
        
        job1 = WrappingJob(mock_callable1, name="callable1")
        job2 = WrappingJob(mock_callable2, name="callable2")
        
        # Set up context access to work in tests
        job1.get_task = MagicMock(return_value={job1.name: {}})
        job2.get_task = MagicMock(return_value={job2.name: {}})
        
        composition = job1 >> job2
        
        result = await evaluate(composition)
        logger.info(result)
        assert "Executed in series" in result
        assert "result1" in result
        assert "result2" in result
    
    async def test_evaluate_mixed_job_types(self):
        """Test evaluating a mixed composition of JobABC subclasses and wrapped callables."""
        # Create a complex workflow: 
        # (LLM summarizer | wrapped callable) >> Data processor
        
        mock_callable = MagicMock(return_value="callable result")
        callable_job = WrappingJob(mock_callable, name="mock_callable")
        callable_job.get_task = MagicMock(return_value={callable_job.name: {}})
        
        llm_job = LLMSummarizer()
        data_job = DataProcessor()
        
        # Build the composition
        parallel_stage = llm_job | callable_job
        workflow = parallel_stage >> data_job
        
        result = await evaluate(workflow)
        logger.info(result)
        
        # Verify result structure
        assert "Executed in series" in result
        assert "Executed in parallel" in result
        assert "summary" in result
        assert "callable result" in result
        assert "processed_data" in result or "status" in result
        
class TestMixedComposition:
    """Tests for mixed serial and parallel compositions."""
    
    def test_mixed_operators(self):
        """Test mixed serial and parallel operators with callables."""
        # Create a complex workflow:
        # (LLM completion >> JSON formatting) | Data extraction
        serial_part = wrap(mock_llm_completion) >> wrap(mock_json_formatter)
        data_extraction_job = wrap(mock_data_extraction)
        composition1 = serial_part | data_extraction_job
        
        assert isinstance(composition1, Parallel)
        assert len(composition1.components) == 2
        assert isinstance(composition1.components[0], Serial)
        assert composition1.components[0].components[0].callable == mock_llm_completion
        assert composition1.components[0].components[1].callable == mock_json_formatter
        assert composition1.components[1].callable == mock_data_extraction
        
        # Another complex workflow:
        # Text processing | (Data extraction >> JSON formatting)
        serial_comp = wrap(mock_data_extraction) >> wrap(mock_json_formatter)
        text_processing_job = wrap(mock_text_processing)
        composition2 = text_processing_job | serial_comp
        
        assert isinstance(composition2, Parallel)
        assert len(composition2.components) == 2
        assert composition2.components[0].callable == mock_text_processing
        assert isinstance(composition2.components[1], Serial)
    
    def test_complex_llm_workflow(self):
        """Test a complex LLM and data processing workflow."""
        # Create a workflow that processes data in parallel and then feeds to an LLM
        # (Data extraction | Text processing) >> LLM summarizer
        
        parallel_processing = wrap(mock_data_extraction) | wrap(mock_text_processing)
        llm_summarizer = LLMSummarizer()
        
        workflow = parallel_processing >> llm_summarizer
        
        assert isinstance(workflow, Serial)
        assert len(workflow.components) == 2
        assert isinstance(workflow.components[0], Parallel)
        assert workflow.components[1] is llm_summarizer
        
    def test_mixed_job_types(self):
        """Test mixing custom JobABC subclasses with wrapped callables."""
        # Create a complex workflow with both custom jobs and wrapped callables
        # (LLM summarizer | Data processor) >> JSON formatter
        
        parallel_jobs = LLMSummarizer() | DataProcessor()
        formatter = wrap(mock_json_formatter)
        
        workflow = parallel_jobs >> formatter
        
        assert isinstance(workflow, Serial)
        assert len(workflow.components) == 2
        assert isinstance(workflow.components[0], Parallel)
        assert isinstance(workflow.components[0].components[0], LLMSummarizer)
        assert isinstance(workflow.components[0].components[1], DataProcessor)
        assert workflow.components[1] is formatter
        
    @pytest.mark.asyncio
    async def test_with_lambdas(self):
        """Test complex composition with lambdas, JobABC instances, and functions."""
        # Create lambda functions
        lambda1 = lambda x: f"Lambda1 processed: {x}"
        lambda2 = lambda x: f"Lambda2 processed: {x}"
        lambda3 = lambda x: {"lambda3_result": x}
        
        # Create JobABC instances
        llm_job = LLMSummarizer() #return {"summary": "This is a summary of the input text."}
        data_job = DataProcessor() # return {"processed_data": [1, 2, 3, 4, 5], "status": "success"}
        
        # Create a complex composition with lambdas, JobABC instances, and functions
        # (lambda1 | data_job) >> (lambda2 >> mock_text_processing) >> (llm_job | lambda3)
        lambda1_job = wrap(lambda1)
        lambda2_job = wrap(lambda2)
        lambda3_job = wrap(lambda3)
        text_proc_job = wrap(mock_text_processing) # return "Processed text output"
        
        first_stage = lambda1_job | data_job
        second_stage = lambda2_job >> text_proc_job
        third_stage = llm_job | lambda3_job
        
        dsl:DSLComponent = first_stage >> second_stage >> third_stage
        
        # Set up context access for the wrapped jobs with appropriate parameters
        lambda1_job.get_task = MagicMock(return_value={lambda1_job.name: {"args": ["input_data1"]}})
        lambda2_job.get_task = MagicMock(return_value={lambda2_job.name: {"args": ["input_data2"]}})
        lambda3_job.get_task = MagicMock(return_value={lambda3_job.name: {"args": ["input_data3"]}})
        text_proc_job.get_task = MagicMock(return_value={}) # text_proc_job.name: {}
        
        # Execute the workflow
        result = await evaluate(dsl)
        logger.info(result)
        
        # Verify the result contains expected components
        assert "Executed in series" in result
        assert "Executed in parallel" in result
        assert "Lambda1 processed" in result
        assert "Lambda2 processed" in result
        assert "{'lambda3_result': 'input_data3'}" in result
        assert "Processed text output" in result
        assert "This is a summary of the input text." in result
        
    @pytest.mark.asyncio
    async def test_combining_everything(self):
        """Test combining everything together in a complex graph structure.
        
        Based on the example: p(w("T1") >> w(1), "T2", 3) >> w(4) | w(s(5, "T3", w(6)))
        """
        # Create functions and JobABC instances to use in the graph
        fn1 = lambda x: f"Function 1: {x}"
        fn2 = lambda x: f"Function 2: {x}"
        fn3 = lambda x: {"result": f"Function 3 result with {x}"}
        fn4 = lambda x: f"Function 4: {x}"
        fn5 = lambda x: f"Function 5: {x}"
        fn6 = lambda x: f"Function 6: {x}"
        
        llm_job = LLMSummarizer("T2")
        data_job = DataProcessor("T3")
        
        # Create the complex graph structure
        # p(w("T1") >> w(1), "T2", 3) >> w(4) | w(s(5, "T3", w(6)))
        fn1_job = wrap(fn1)
        fn1_job.name = "T1"
        fn2_job = wrap(fn2)
        fn3_job = wrap(fn3)
        fn4_job = wrap(fn4)
        fn5_job = wrap(fn5)
        fn6_job = wrap(fn6)
        
        # Build the graph in parts
        part1 = fn1_job >> fn2_job
        part2 = parallel(part1, llm_job, fn3_job)
        part3 = part2 >> fn4_job
        part4 = serial(fn5_job, data_job, fn6_job)
        
        # Combine into final graph
        dsl:DSLComponent = part3 | wrap(part4)
        
        # Set up context access for all WrappingJob instances
        for job in [fn1_job, fn2_job, fn3_job, fn4_job, fn5_job, fn6_job]:
            job.get_task = MagicMock(return_value={job.name: {"args": ["test_input"]}})
        
        # Execute the workflow
        result = await evaluate(dsl)
        
        # Verify the result contains expected components
        assert "Executed in parallel" in result
        assert "Executed in series" in result
        
    @pytest.mark.asyncio
    async def test_precedence_graph(self):
        """Test creating a precedence graph based on the example:
        graph = w(1) >> ((p(5,4,3) >> 7 >> 9) | (w(2) >> 6 >> 8>> 10)) >> w(11)
        """
        # Create functions and JobABC instances to use in the graph
        fn1 = lambda x: f"Function 1: {x}"
        fn2 = lambda x: f"Function 2: {x}"
        fn3 = lambda x: f"Function 3: {x}"
        fn4 = lambda x: f"Function 4: {x}"
        fn5 = lambda x: f"Function 5: {x}"
        fn6 = lambda x: f"Function 6: {x}"
        fn7 = lambda x: f"Function 7: {x}"
        fn8 = lambda x: f"Function 8: {x}"
        fn9 = lambda x: f"Function 9: {x}"
        fn10 = lambda x: f"Function 10: {x}"
        fn11 = lambda x: f"Function 11: {x}"
        
        # Create the WrappingJob instances
        job1 = wrap(fn1)
        job2 = wrap(fn2)
        job3 = wrap(fn3)
        job4 = wrap(fn4)
        job5 = wrap(fn5)
        job6 = wrap(fn6)
        job7 = wrap(fn7)
        job8 = wrap(fn8)
        job9 = wrap(fn9)
        job10 = wrap(fn10)
        job11 = wrap(fn11)
        
        # Build the complex graph structure: w(1) >> ((p(5,4,3) >> 7 >> 9) | (w(2) >> 6 >> 8>> 10)) >> w(11)
        parallel_part1 = parallel(job5, job4, job3)
        serial_part1 = parallel_part1 >> job7 >> job9
        
        serial_part2 = job2 >> job6 >> job8 >> job10
        
        middle_part = serial_part1 | serial_part2
        
        # Complete graph
        graph: DSLComponent = job1 >> middle_part >> job11
        
        # Set up context access for all WrappingJob instances
        for job in [job1, job2, job3, job4, job5, job6, job7, job8, job9, job10, job11]:
            job.get_task = MagicMock(return_value={job.name: {"args": ["test_input"]}})
        
        # Execute the workflow
        result = await evaluate(graph)
        
        # Verify the result contains expected components
        assert "Executed in series" in result
        assert "Executed in parallel" in result


class TestWrappingJob:
    """Tests for WrappingJob class with callable objects."""
    
    @pytest.mark.asyncio
    async def test_wrapping_job_run_with_callable(self):
        """Test that WrappingJob.run executes the callable and returns its result."""
        # Create a simple callable for testing
        mock_callable = MagicMock(return_value="callable result")
        
        # Create a WrappingJob with the callable
        job = WrappingJob(mock_callable, name="test_callable")
        
        # Prepare the task with the necessary structure
        task = {job.name: {}}
        
        # Set up context access to work in tests
        job.get_task = MagicMock(return_value=task)
        
        # Execute the job
        result = await job.run(task)
        
        # Verify the callable was executed
        mock_callable.assert_called_once()
        assert result == "callable result"
    
    @pytest.mark.asyncio
    async def test_wrapping_job_run_with_async_callable(self):
        """Test that WrappingJob.run works with async callables."""
        # Create a mock async callable
        mock_async_callable = AsyncMock(return_value="async result")
        
        # Create a WrappingJob with the async callable
        job = WrappingJob(mock_async_callable, name="test_async")
        
        # Prepare the task with necessary structure
        task = {job.name: {}}
        
        # Set up context access to work in tests
        job.get_task = MagicMock(return_value=task)
        
        # Execute the job
        result = await job.run(task)
        
        # Verify the async callable was executed
        mock_async_callable.assert_called_once()
        assert result == "async result"
    
    @pytest.mark.asyncio
    async def test_wrapped_function_with_multiple_args(self):
        """Test wrapped function that accepts multiple positional arguments."""
        # Create a function that processes multiple arguments
        def process_multiple_args(a, b, c):
            return {"sum": a + b + c, "product": a * b * c}
        
        # Mock the function to verify calls
        mock_func = MagicMock(side_effect=process_multiple_args)
        
        # Create a WrappingJob with the function
        job = WrappingJob(mock_func, name="multi_arg_func")
        
        # Prepare task with positional arguments
        task = {"multi_arg_func": {"args": [5, 7, 3]}}
        
        # Set up context access to work in tests
        job.get_task = MagicMock(return_value=task)
        
        # Execute the job
        result = await job.run(task)
        
        # Verify the function was called with correct arguments
        mock_func.assert_called_once_with(5, 7, 3)
        
        # Check the results
        assert result["sum"] == 15  # 5 + 7 + 3
        assert result["product"] == 105  # 5 * 7 * 3
    
    @pytest.mark.asyncio
    async def test_wrapped_function_with_custom_objects(self):
        """Test wrapped function that accepts custom object arguments."""
        # Define a custom class
        class CustomData:
            def __init__(self, value, name):
                self.value = value
                self.name = name
                
            def process(self):
                return self.value * 2
        
        # Create a function that works with custom objects
        def process_custom_objects(data_obj, multiplier):
            processed_value = data_obj.process() * multiplier
            return {
                "name": data_obj.name,
                "processed_value": processed_value
            }
        
        # Mock the function to verify calls
        mock_func = MagicMock(side_effect=process_custom_objects)
        
        # Create custom object instance
        custom_obj = CustomData(10, "test_object")
        
        # Create a WrappingJob with the function
        job = WrappingJob(mock_func, name="custom_obj_func")
        
        # Prepare task with custom object as argument
        task = {"custom_obj_func": {"args": [custom_obj, 3]}}
        
        # Set up context access to work in tests
        job.get_task = MagicMock(return_value=task)
        
        # Execute the job
        result = await job.run(task)
        
        # Verify the function was called with correct arguments
        mock_func.assert_called_once_with(custom_obj, 3)
        
        # Check the results - custom object's process method returns 20 (10*2), then multiplied by 3
        assert result["name"] == "test_object"
        assert result["processed_value"] == 60  # (10*2) * 3
    
    @pytest.mark.asyncio
    async def test_wrapped_function_with_kwargs(self):
        """Test wrapped function that accepts keyword arguments."""
        # Create a function that processes keyword arguments
        def config_formatter(prefix="", **settings):
            formatted = {}
            for key, value in settings.items():
                formatted[f"{prefix}{key}"] = value
            return formatted
        
        # Mock the function to verify calls
        mock_func = MagicMock(side_effect=config_formatter)
        
        # Create a WrappingJob with the function
        job = WrappingJob(mock_func, name="config_formatter")
        
        # Prepare task with keyword arguments - two ways to provide kwargs
        task = {
            "config_formatter": {
                # Method 1: Using kwargs dictionary
                "kwargs": {
                    "prefix": "setting_", 
                    "color": "blue", 
                    "size": "large", 
                    "enabled": True
                }
            }
        }
        
        # Set up context access to work in tests
        job.get_task = MagicMock(return_value=task)
        
        # Execute the job
        result = await job.run(task)
        
        # Verify the function was called with correct keyword arguments
        mock_func.assert_called_once_with(
            prefix="setting_", 
            color="blue", 
            size="large", 
            enabled=True
        )
        
        # Check the results contain properly formatted settings
        assert result == {
            "setting_color": "blue",
            "setting_size": "large",
            "setting_enabled": True
        }
        
    @pytest.mark.asyncio
    async def test_wrapped_function_with_fn_prefix_kwargs(self):
        """Test wrapped function with kwargs using  prefix notation."""
        # Create a function that processes keyword arguments
        def config_processor(mode, timeout=30, debug=False):
            return {
                "config": {
                    "mode": mode,
                    "timeout": timeout,
                    "debug": debug
                }
            }
        
        # Mock the function to verify calls
        mock_func = MagicMock(side_effect=config_processor)
        
        # Create a WrappingJob with the function
        job = WrappingJob(mock_func, name="processor")
        
        # Prepare task with  prefix notation for kwargs
        task = {
            "processor": {
                # Method 2: Using  prefix for individual parameters
                "mode": "production",
                "timeout": 60,
                "debug": True
            }
        }
        
        # Set up context access to work in tests
        job.get_task = MagicMock(return_value=task)
        
        # Execute the job
        result = await job.run(task)
        
        # Verify the function was called with correct keyword arguments
        mock_func.assert_called_once_with(
            mode="production", 
            timeout=60, 
            debug=True
        )
        
        # Check the results
        assert result["config"]["mode"] == "production"
        assert result["config"]["timeout"] == 60
        assert result["config"]["debug"] is True
    
    @pytest.mark.asyncio
    async def test_wrapped_lambda_with_multiple_args(self):
        """Test wrapped lambda function with multiple arguments."""
        # Create a lambda function that processes multiple arguments
        # Lambda that calculates statistics from a list of numbers
        stats_lambda = lambda numbers, calc_median=False, round_to=2: {
            "mean": round(sum(numbers) / len(numbers), round_to),
            "min": min(numbers),
            "max": max(numbers),
            "median": round(sorted(numbers)[len(numbers) // 2], round_to) if calc_median else None
        }
        
        # We need to use MagicMock with side_effect to verify lambda calls
        mock_func = MagicMock(side_effect=stats_lambda)
        
        # Create a WrappingJob with the lambda
        job = WrappingJob(mock_func, name="stats_calculator")
        
        # Prepare task with arguments for the lambda
        task = {
            "stats_calculator": {
                "args": [[10, 15, 7, 22, 8, 11]],  # List of numbers as first arg
                "kwargs": {
                    "calc_median": True,
                    "round_to": 1
                }
            }
        }
        
        # Set up context access to work in tests
        job.get_task = MagicMock(return_value=task)
        
        # Execute the job
        result = await job.run(task)
        
        # Verify the lambda was called with correct arguments
        mock_func.assert_called_once_with(
            [10, 15, 7, 22, 8, 11], 
            calc_median=True, 
            round_to=1
        )
        
        # Check the results
        assert result["mean"] == 12.2  # (10+15+7+22+8+11)/6 = 73/6 = 12.166... rounded to 12.2
        assert result["min"] == 7
        assert result["max"] == 22
        assert result["median"] == 11.0  # Median of [7, 8, 10, 11, 15, 22] is 11 (rounded to 11.0)
        

    @pytest.mark.asyncio
    async def test_wrapped_jobs_with_context(self):
        """Test wrapped lambda function with multiple arguments."""
        # Create a lambda function that processes multiple arguments
        # Lambda that calculates statistics from a list of numbers

        times = lambda x: x*2
        add = lambda x: x+3
        square = lambda x: x**2
        
        def collate(j_ctx):
            task = j_ctx["task"]
            inputs = j_ctx["inputs"]

            return {"task": task, "inputs": inputs}

        jobs = wrap ({
            "times": times,
            "add": add,
            "square": square,
            "collate": collate
        })

        task = {"times": {"x": 1}, "add": {"x": 2}, "square": {"x": 3}}
        
        for job in jobs.values():
            job.get_task = MagicMock(return_value=task)
        jobs["collate"].get_inputs = MagicMock(return_value={"test_job_input": "test_value"})

        dsl:DSLComponent = p(jobs["times"], jobs["add"], jobs["square"]) >> jobs["collate"]
        
        result = await evaluate(dsl)
        
        assert "{'task': {'times': {'x': 1}, 'add': {'x': 2}, 'square': {'x': 3}}, 'inputs': {'test_job_input': 'test_value'}}" in result
        assert "2" in result
        assert "5" in result
        assert "9" in result
        



class TestComplexDSLExpressions:
    """Tests for complex, highly nested DSL expressions."""
    
    @pytest.mark.asyncio
    async def test_complex_nested_dsl_with_mixed_params(self):
        """Test a highly nested, complex DSL expression with various parameter types."""
        # Define a series of functions with different parameter patterns
        
        # Simple function that takes multiple args
        def math_op(a, b, c):
            return {"result": a * b + c}
        
        # Function with custom object parameter
        class DataPoint:
            def __init__(self, x, y):
                self.x = x
                self.y = y
                
            def distance(self):
                return (self.x**2 + self.y**2)**0.5
        
        def process_data_point(point, scaling_factor=1.0):
            return {"distance": point.distance() * scaling_factor}
        
        # Function with kwargs
        def config_builder(**settings):
            return {"config": settings}
        
        # Lambda with multiple args
        format_lambda = lambda text, prefix="", suffix="": f"{prefix}{text}{suffix}"
        
        # Function with args and kwargs
        def transform_data(data, *transforms, metadata=None, **options):
            result = data.copy() if isinstance(data, dict) else {"value": data}
            
            for transform in transforms:
                if transform == "uppercase" and isinstance(result.get("value"), str):
                    result["value"] = result["value"].upper()
                elif transform == "double":
                    for key in result:
                        if isinstance(result[key], (int, float)):
                            result[key] *= 2
            
            if metadata:
                result["metadata"] = metadata
                
            for key, value in options.items():
                result[f"option_{key}"] = value
                
            return result
        
        # Mock results for each function to avoid needing to evaluate them
        # This allows us to test the parameter passing mechanism directly
        math_op_result = "{'result': 38}" # 5 * 7 + 3 = 38
        process_data_point_result = "{'distance': 10.0}" # 5 * 2.0 = 10.0
        format_lambda_result = "<< test >>"
        config_builder_result = "{'config': {'mode': 'testing', 'debug': True}}"
        transform_data_result = "{'value': 'SAMPLE', 'score': 20, 'metadata': {'source': 'test'}, 'option_format': 'json', 'option_version': 2}"
        
        # Create a custom data point
        data_point = DataPoint(3, 4)  # 3-4-5 triangle, distance = 5
        
        # Create the complex task with parameters for all the jobs
        task = {
            # Parameters for math_op
            "math_op": {
                "args": [5, 7, 3]
            },
            # Parameters for process_data_point
            "process_data_point": {
                "args": [data_point],
                "kwargs": {"scaling_factor": 2.0}
            },
            # Parameters for format_lambda
            "format_lambda": {
                "args": ["test"],
                "prefix": "<< ",
                "suffix": " >>"
            },
            # Parameters for config_builder
            "config_builder": {
                "kwargs": {
                    "mode": "testing",
                    "debug": True
                }
            },
            # Parameters for transform_data
            "transform_data": {
                "args": [{"value": "sample", "score": 10}, "uppercase", "double"],
                "kwargs": {
                    "metadata": {"source": "test"},
                    "format": "json",
                    "version": 2
                }
            }
        }
        
        # Create a complex nested DSL expression
        # This combines serial, parallel compositions with different job types and parameter patterns
        math_op_job = WrappingJob(math_op, name="math_op")
        process_data_point_job = WrappingJob(process_data_point, name="process_data_point")
        format_lambda_job = WrappingJob(format_lambda, name="format_lambda")
        config_builder_job = WrappingJob(config_builder, name="config_builder")
        transform_data_job = WrappingJob(transform_data, name="transform_data")
        
        # We don't mock the run methods because we want to test actual execution
        # Instead we'll verify the results match our expectations based on the provided parameters
        
        # Create the complex DSL graph with our DSL operators
        complex_dsl:DSLComponent = (
            # Start with a simple function taking multiple args
            math_op_job>> 
            # Then parallel branch with custom object and lambda
            p(
                # Branch 1: Process with custom object
                process_data_point_job,
                # Branch 2: Lambda with format args
                format_lambda_job >> 
                # Branch 2a: Nested with kwargs
                config_builder_job
            ) >> 
            # Finally, combine with function taking args and kwargs
            transform_data_job
        )
        
        # Set up tasks for each job
        math_op_job.get_task = MagicMock(return_value=task)
        process_data_point_job.get_task = MagicMock(return_value=task)
        format_lambda_job.get_task = MagicMock(return_value=task)
        config_builder_job.get_task = MagicMock(return_value=task)
        transform_data_job.get_task = MagicMock(return_value=task)
        
        # Execute each job individually to verify parameter passing
        math_result = await math_op_job.run({})
        process_result = await process_data_point_job.run({})
        format_result = await format_lambda_job.run({})
        config_result = await config_builder_job.run({})
        transform_result = await transform_data_job.run({})
        
        # Verify that get_task was called for each job
        math_op_job.get_task.assert_called()
        process_data_point_job.get_task.assert_called()
        format_lambda_job.get_task.assert_called()
        config_builder_job.get_task.assert_called()
        transform_data_job.get_task.assert_called()
        
        # Verify each function processed its parameters correctly
        # Check math_op result: should be 5 * 7 + 3 = 38
        assert math_result == {"result": 38}
        
        # Check process_data_point result: 5 (distance) * 2.0 (scaling_factor) = 10.0
        assert process_result == {"distance": 10.0}
        
        # Check format_lambda result: "<< test >>"
        assert format_result == "<< test >>"
        
        # Check config_builder result
        assert config_result == {"config": {"mode": "testing", "debug": True}}
        
        # Verify transform_data result - this shows the parameter handling for args and kwargs
        assert "value" in transform_result
        assert transform_result["value"] == "SAMPLE"  # Uppercase transform
        assert transform_result["score"] == 20  # Double transform
        assert transform_result["metadata"] == {"source": "test"}
        assert transform_result["option_format"] == "json"
        assert transform_result["option_version"] == 2
        
        # Now evaluate the full complex DSL graph
        # This tests that parameters are correctly passed through the entire graph
        # Use the built-in evaluate function to test the graph execution
        result_string = await evaluate(complex_dsl)
        logger.info(f"\n {result_string}")
        
        # Since evaluate returns a string representation, we just verify it completed
        # The individual tests above already verified the parameter handling works correctly
        assert result_string is not None
        assert isinstance(result_string, str)
        assert "Executed in parallel" in result_string
        assert "Executed in series" in result_string
        assert math_op_result in result_string
        assert process_data_point_result in result_string
        assert format_lambda_result in result_string
        assert config_builder_result in result_string
        assert transform_data_result in result_string

    @pytest.mark.asyncio
    async def test_dsl_with_upfront_wrapping(self):
        """Test DSL construction with upfront wrapping of all objects.
        
        This test explores option (i): Wrap all objects up front in a dsl = w({}) expression
        to give them names, and then use a more concise expression syntax.
        """
        
        # Define simple functions and a JobABC subclass for testing
        def add_numbers(a, b):
            return {"result": a + b}
        
        def multiply_by(number, factor=2):
            return {"result": number * factor}
            
        format_fn = lambda text, prefix="", suffix="": f"{prefix}{text}{suffix}"
        
        class DataProcessor(JobABC):
            def __init__(self, name=None):
                super().__init__(name=name)
                
            async def run(self, task):
                the_task = self.get_task()
                job_data = the_task.get(self.name, {})
                data = job_data.get("data", "default")
                logger.info(f"DataProcessor got data: {data}")
                return {"processed": f"Processed: {data.upper()}"}
        
        # Create the task with parameters using the names we'll use in our DSL
        task = {
            "add": {
                "args": [10, 5]
            },
            "mult": {
                "args": [15],
                "factor": 2
            },
            "fmt": {
                "args": ["Formatted"],
                "prefix": "[[ ",
                "suffix": " ]]"
            },
            "proc": {
                "data": "test data"
            }
        }
        
        jobs = wrap({
            "add": add_numbers,
            "mult": multiply_by,
            "fmt": format_fn,
            "proc": DataProcessor()
        })

        for job in jobs.values():
            job.get_task = MagicMock(return_value=task)
        
        # Use the dictionary to create a concise DSL expression
        dsl: DSLComponent = (jobs["add"] >> jobs["mult"]) | (jobs["fmt"] >> jobs["proc"])
        
        # Evaluate the DSL
        result_string = await evaluate(dsl)
        logger.info(f"\n {result_string}")
        
        # Verify the execution by checking expected values directly
        assert result_string is not None
        assert isinstance(result_string, str)
        assert "Executed in parallel" in result_string
        assert "Executed in series" in result_string
        
        # Check for specific expected results based on task parameters
        assert "{'result': 15}" in result_string  # add_numbers result
        assert "{'result': 30}" in result_string  # multiply_by result
        assert "[[ Formatted ]]" in result_string  # format_fn result
        assert "{'processed': 'Processed: TEST DATA'}" in result_string  # processor result




if __name__ == "__main__":
    pytest.main(["-v", "test_dsl.py"])



================================================
FILE: tests/test_dsl_graph.py
================================================
from typing import Any, Dict

from flow4ai.dsl import DSLComponent, JobsDict, p, wrap
from flow4ai.dsl_graph import dsl_to_precedence_graph, visualize_graph
from flow4ai.f4a_graph import validate_graph
from flow4ai.flowmanager import FlowManager
from flow4ai.job import JobABC
from tests.test_utils.graph_evaluation import print_diff


class ProcessorJob(JobABC):
    """Example component that implements JobABC interface."""
    def __init__(self, name, process_type):
        super().__init__(name)
        self.process_type = process_type
    
    async def run(self, task: Dict[str, Any]) -> Dict[str, Any]:
        return f"Processor {self.name} of type {self.process_type}"

def test_complex_JobABC_subclass():
    """Test a complex DSL with direct JobABC subclasses"""
    # Create various ProcessorJob instances (MockJobABC subclasses)
    preprocessor = ProcessorJob("Preprocessor", "preprocess")
    analyzer1 = ProcessorJob("Analyzer1", "analyze")
    analyzer2 = ProcessorJob("Analyzer2", "analyze")
    transformer = ProcessorJob("Transformer", "transform")
    aggregator = ProcessorJob("Aggregator", "aggregate")
    formatter = ProcessorJob("Formatter", "format")
    cache_manager = ProcessorJob("CacheManager", "cache")
    logger = ProcessorJob("Logger", "log")
    init = ProcessorJob("Init", "init")
    
    # Create a complex DSL with various combinations of wrapping and direct usage
    # Main pipeline has a preprocessor followed by parallel analyzers, then transforms and formats
    # Side pipeline handles caching and logging which can run in parallel with the main pipeline
    main_pipeline = preprocessor >> p(analyzer1, analyzer2) >> transformer >> formatter
    side_pipeline = init >> p(cache_manager, logger)
    
    # Combine pipelines and add an aggregator at the end
    dsl: DSLComponent = p(main_pipeline, side_pipeline) >> aggregator
    
    print(f"DSL: Complex expression with ProcessorJob instances combined with p() and >>")
    
    # Convert to adjacency list
    graph, jobs = dsl_to_precedence_graph(dsl)
    visualize_graph(graph)
    
    # Define expected graph structure
    expected_graph = {
        "Preprocessor": {"next": ["Analyzer1", "Analyzer2"]},
        "Init": {"next": ["CacheManager", "Logger"]},
        "Analyzer1": {"next": ["Transformer"]},
        "Analyzer2": {"next": ["Transformer"]},
        "CacheManager": {"next": ["Aggregator"]},
        "Logger": {"next": ["Aggregator"]},
        "Transformer": {"next": ["Formatter"]},
        "Formatter": {"next": ["Aggregator"]},
        "Aggregator": {"next": []}
    }
    
    # Verify graph structure
    assert graph == expected_graph or print_diff(graph, expected_graph, "test_complex_JobABC_subclass")
    
    validate_graph(graph, name="test_complex_JobABC_subclass")

    # DSL by brackets
    main_pipeline = preprocessor >> (analyzer1 | analyzer2) >> transformer >> formatter 
    side_pipeline = init >> (cache_manager | logger)
    dsl:DSLComponent = (main_pipeline | side_pipeline) >> aggregator

    # Convert to adjacency list
    graph, jobs = dsl_to_precedence_graph(dsl)
    visualize_graph(graph)
    

    assert graph == expected_graph or print_diff(graph, expected_graph, "test_complex_JobABC_subclass")
    
    validate_graph(graph, name="test_complex_JobABC_subclass")

def test_complex_mixed():
    """
    Test a complex DSL with a mix of JobABC and functions and lambdas.
    """
    times = lambda x: x*2
    add = lambda x: x+3
    square = lambda x: x**2
    
    def collate(j_ctx):
        task = j_ctx["task"]
        inputs = j_ctx["inputs"]
        return {"task": task, "inputs": inputs}
        
    analyzer2 = ProcessorJob("Analyzer2", "analyze")
    transformer = ProcessorJob("Transformer", "transform")
    aggregator = ProcessorJob("Aggregator", "aggregate")
    formatter = ProcessorJob("Formatter", "format")
    cache_manager = ProcessorJob("CacheManager", "cache")

    jobs:JobsDict = wrap({
            "analyzer2": analyzer2,
            "cache_manager": cache_manager,
            "times": times,
            "transformer": transformer,
            "formatter": formatter,
            "add": add,
            "square": square,
            "aggregator": aggregator,
            "collate": collate
        })

    dsl:DSLComponent = (
        p(jobs["analyzer2"], jobs["cache_manager"], jobs["times"]) 
        >> jobs["transformer"] 
        >> jobs["formatter"] 
        >> (jobs["add"] | jobs["square"]) 
        >> jobs["aggregator"] 
        >> jobs["collate"]
    )
        
    # Convert to adjacency list
    graph, jobs = dsl_to_precedence_graph(dsl)
    visualize_graph(graph)
        
    # Define expected graph structure
    expected_graph = {
            "analyzer2": {"next": ["transformer"]},
            "cache_manager": {"next": ["transformer"]},
            "times": {"next": ["transformer"]},
            "transformer": {"next": ["formatter"]},
            "formatter": {"next": ["add", "square"]},
            "add": {"next": ["aggregator"]},
            "square": {"next": ["aggregator"]},
            "aggregator": {"next": ["collate"]},
            "collate": {"next": []}
        }
        
    assert graph == expected_graph or print_diff(graph, expected_graph, "test_complex_mixed")
        
    validate_graph(graph, name="test_complex_mixed")


def test_execute_job_graph_from_dsl():
    """
    Test a complex DSL with a mix of JobABC and functions and lambdas.
    """
    times = lambda x: x*2
    add = lambda x: x+3
    square = lambda x: x**2
    
    def test_context(j_ctx):
        task = j_ctx["task"]
        inputs = j_ctx["inputs"]
        return {"task": task, "inputs": inputs}
        
    analyzer2 = ProcessorJob("Analyzer2", "analyze")
    transformer = ProcessorJob("Transformer", "transform")
    aggregator = ProcessorJob("Aggregator", "aggregate")
    formatter = ProcessorJob("Formatter", "format")
    cache_manager = ProcessorJob("CacheManager", "cache")

    jobs:JobsDict = wrap({
            "analyzer2": analyzer2,
            "cache_manager": cache_manager,
            "times": times,
            "transformer": transformer,
            "formatter": formatter,
            "add": add,
            "square": square,
            "aggregator": aggregator,
            "test_context": test_context
        })

    jobs["times"].save_result = True
    jobs["add"].save_result = True
    jobs["square"].save_result = True

    dsl:DSLComponent = (
        p(jobs["analyzer2"], jobs["cache_manager"], jobs["times"]) 
        >> jobs["transformer"] 
        >> jobs["formatter"] 
        >> (jobs["add"] | jobs["square"]) 
        >> jobs["test_context"]
        >> jobs["aggregator"] 
    )
        
    fm = FlowManager()
    # Using add_dsl without a graph_name - it should generate one based on head jobs
    fq_name = fm.add_dsl(dsl)
    print(f"Auto-generated FQ name: {fq_name}")
    
    # Extract the auto-generated graph name from the FQ name
    # FQ name format is typically: graph_name$$$$job_name$$
    auto_graph_name = fq_name.split('$$')[0]
    task = {"times": {"fn.x": 1}, "add": {"fn.x": 2}, "square": {"fn.x": 3}}
    fm.submit_task(task,fq_name)
    success = fm.wait_for_completion()
    assert success, "Timed out waiting for tasks to complete"
    
    # Print results
    print("Task counts:", fm.get_counts())
    results = fm.pop_results()
    
    print("\nCompleted tasks:")
    for job_name, job_results in results["completed"].items():
        for result_data in job_results:
            print(f"- {job_name}: {result_data['result']}")
    
    print("\nErrors:")
    if results["errors"]:
        error_messages = []
        for job_name, job_errors in results["errors"].items():
            for error_data in job_errors:
                error_msg = f"- {job_name}: {error_data['error']}"
                print(error_msg)
                error_messages.append(error_msg)
        
        # Raise exception with all errors
        raise Exception("Errors occurred during job execution:\n" + "\n".join(error_messages))
    
    print("\nResults:")
    print(results["completed"].values())
    result_dict = list(results["completed"].values())[0][0] # [0]= first job
    # Assert using the auto-generated graph name instead of hardcoded name
    expected_result = f"Processor {auto_graph_name}$$$$aggregator$$ of type aggregate"
    assert result_dict["result"] == expected_result
    # Convert Task to dict or extract the dictionary data
    task_pass_through = result_dict["task_pass_through"]
    # Check that task_pass_through contains all the expected keys and values
    assert all(key in task_pass_through and task_pass_through[key] == value for key, value in task.items())
    assert result_dict["SAVED_RESULTS"] == {"times": 2, "add": 5, "square": 9}


def test_execute_with_task_params():
    """
    Tests job graph execution defined via DSL using shorthand parameter notation
    for wrapped callables (e.g., {"job.x": value} or {"job": {"x": value}}).
    """
    # Define simple callables
    times = lambda x: x * 2
    add = lambda x, y=1: x + y # Add a default arg to test args/kwargs override
    square = lambda x: x**2
    join = lambda **kwargs: "_".join(map(str, sorted(kwargs.values()))) # Test kwargs

    jobs: JobsDict = wrap({
        "times": times,
        "add": add,
        "square": square,
        "join": join
    })

    jobs["times"].save_result = True
    jobs["add"].save_result = True
    jobs["square"].save_result = True


    # Define DSL Graph (simple linear)
    dsl: DSLComponent = (
        jobs["times"]
        >> jobs["add"]
        >> jobs["square"]
        >> jobs["join"]
    )

    fm = FlowManager()
    fq_name = fm.add_dsl(dsl, "test_shorthand_params")

    task = {
        "times": {"x": 5},                   # times(x=5) -> 10
        "add": {"args": [100], "y": 5},      # add(100, y=5) -> 105 (overrides default y=1), Note: 'args' implicitly takes precedence over named params for positionals
        "square": {"x": 3},                  # square(x=3) -> 9 # This would be overridden by output of add
        "join": {"kwargs": {"a": 1, "b": 2, "c": 3}} # join(result=square_output, extra="data") # Result from square (e.g., 105^2) will be passed implicitly if not specified
    }

    # Submit task and wait
    fm.submit_task(task, fq_name)
    success = fm.wait_for_completion()
    assert success, "Timed out waiting for tasks to complete"
    
    results = fm.pop_results()
    result_dict = list(results["completed"].values())[0][0] # [0]= first job
    assert result_dict["result"] == "1_2_3"
    # Convert Task to dict or extract the dictionary data
    task_pass_through = result_dict["task_pass_through"]
    # Check that task_pass_through contains all the expected keys and values
    assert all(key in task_pass_through and task_pass_through[key] == value for key, value in task.items())
    assert result_dict["SAVED_RESULTS"] == {"times": 10, "add": 105, "square": 9}

def test_execute_with_shorthand_task_params():
    """
    Tests job graph execution defined via DSL using shorthand parameter notation
    for wrapped callables (e.g., {"job.x": value} or {"job": {"x": value}}).
    """
    # Define simple callables
    times = lambda x: x * 2
    add = lambda x, y=1: x + y # Add a default arg to test args/kwargs override
    square = lambda x: x**2
    join = lambda **kwargs: "_".join(map(str, sorted(kwargs.values()))) # Test kwargs

    jobs: JobsDict = wrap({
        "times": times,
        "add": add,
        "square": square,
        "join": join
    })

    jobs["times"].save_result = True
    jobs["add"].save_result = True
    jobs["square"].save_result = True


    # Define DSL Graph (simple linear)
    dsl: DSLComponent = (
        jobs["times"]
        >> jobs["add"]
        >> jobs["square"]
        >> jobs["join"]
    )

    fm = FlowManager()
    fq_name = fm.add_dsl(dsl, "test_shorthand_params")
    task = {"times.x": 5,"add.args": [100], "add.y": 5, "square.x": 3, "join.kwargs": {"a": 1, "b": 2, "c": 3}} 
    fm.submit_task(task, fq_name)
    success = fm.wait_for_completion()
    assert success, "Timed out waiting for  tasks to complete"
    
    results = fm.pop_results()
    result_dict = list(results["completed"].values())[0][0] # [0]= first job
    assert result_dict["result"] == "1_2_3"
    # Convert Task to dict or extract the dictionary data
    task_pass_through = result_dict["task_pass_through"]
    # Check that task_pass_through contains all the expected keys and values
    assert all(key in task_pass_through and task_pass_through[key] == value for key, value in task.items())
    assert result_dict["SAVED_RESULTS"] == {"times": 10, "add": 105, "square": 9}



================================================
FILE: tests/test_error_conditions.py
================================================
"""
    Test resilience under error conditions:

        - Tests basic error handling and propagation
        - Tests timeout scenarios
        - Tests process termination handling
        - Tests invalid input handling
        - Tests resource cleanup
        - Tests result processing errors
        - Tests memory error handling
        - Tests unpicklable result scenarios
"""

import asyncio

import pytest

from flow4ai.flowmanagerMP import FlowManagerMP
from flow4ai.job import JobABC


class ErrorTestJob(JobABC):
    """Job implementation for testing error conditions"""
    def __init__(self):
        super().__init__(name="ErrorTestJob")
    
    async def run(self, task):
        if task.get('raise_error'):
            raise Exception(task.get('error_message', 'Simulated error'))
        if task.get('timeout'):
            await asyncio.sleep(float(task['timeout']))
            return {'task': task, 'status': 'timeout_completed'}
        if task.get('memory_error'):
            # Explicitly raise MemoryError instead of trying to create a large list
            raise MemoryError("Simulated memory error")
        if task.get('invalid_result'):
            # Return an unpicklable object
            return lambda x: x  # Functions can't be pickled
        return {'task': task, 'status': 'completed'}


def test_basic_error_handling():
    """Test handling of basic exceptions during task execution"""
    results = []
    errors = []
    
    def collect_result(result):
        if isinstance(result, Exception):
            errors.append(result)
        else:
            results.append(result)
    
    flowmanagerMP = FlowManagerMP(ErrorTestJob(), collect_result, serial_processing=True)
    
    # Get the head job's name to use in task submissions
    head_jobs = flowmanagerMP.get_head_jobs()
    assert len(head_jobs) > 0, "No head jobs found in the flow manager"
    job_name = head_jobs[0].name
    
    # Submit mix of successful and failing tasks
    tasks = [
        {'task_id': 1},
        {'task_id': 2, 'raise_error': True, 'error_message': 'Task 2 error'},
        {'task_id': 3},
        {'task_id': 4, 'raise_error': True, 'error_message': 'Task 4 error'}
    ]
    
    for task in tasks:
        flowmanagerMP.submit_task(task)
    
    flowmanagerMP.wait_for_completion()
    
    # Verify successful tasks completed
    assert len(results) == 2
    assert all(r['status'] == 'completed' for r in results)
    
    # Verify errors were captured
    assert len(errors) == 0  # Errors should be logged, not passed to result processor

def test_timeout_handling():
    """Test handling of task timeouts"""
    results = []
    def collect_result(result):
        results.append(result)
    
    flowmanagerMP = FlowManagerMP(ErrorTestJob(), collect_result, serial_processing=True)
    
    # Get the head job's name to use in task submissions
    head_jobs = flowmanagerMP.get_head_jobs()
    assert len(head_jobs) > 0, "No head jobs found in the flow manager"
    job_name = head_jobs[0].name
    
    # Submit tasks with varying timeouts
    tasks = [
        {'task_id': 1, 'timeout': 0.3},
        {'task_id': 2, 'timeout': 0.2},
        {'task_id': 3, 'timeout': 0.1}
    ]
    
    for task in tasks:
        flowmanagerMP.submit_task(task)
    
    flowmanagerMP.wait_for_completion()
    
    # Verify all tasks eventually completed
    assert len(results) == 3
    assert all(r['status'] == 'timeout_completed' for r in results)
    
    # Verify tasks completed in order of timeout
    task_ids = [r['task']['task_id'] for r in results]
    assert task_ids == [3, 2, 1]

def test_process_termination():
    """Test handling of process termination"""
    flowmanagerMP = FlowManagerMP(ErrorTestJob())
    
    # Submit a long-running task
    flowmanagerMP.submit_task({'task_id': 1, 'timeout': 1.0})
    
    # Force terminate the process
    flowmanagerMP.job_executor_process.terminate()
    
    # Verify cleanup handles terminated process
    flowmanagerMP._cleanup()
    assert not flowmanagerMP.job_executor_process.is_alive()

def test_invalid_input():
    """Test handling of invalid input data"""
    flowmanagerMP = FlowManagerMP(ErrorTestJob())
    
    # Test various invalid inputs
    invalid_inputs = [
        None,
        "",
        {},
        {'task_id': None},
        {'task_id': object()},  # Unpicklable object
        []
    ]
    
    for invalid_input in invalid_inputs:
        flowmanagerMP.submit_task(invalid_input)
    
    flowmanagerMP.wait_for_completion()
    # Should complete without raising exceptions

def test_resource_cleanup():
    """Test proper cleanup of resources"""
    flowmanagerMP = FlowManagerMP(ErrorTestJob())
    
    # Submit some tasks
    for i in range(5):
        flowmanagerMP.submit_task({'task_id': i})
    
    # Get queue references
    task_queue = flowmanagerMP._task_queue
    result_queue = flowmanagerMP._result_queue
    
    # Cleanup
    flowmanagerMP._cleanup()
    
    # Verify queues are closed
    assert task_queue._closed
    assert result_queue._closed
    
    # Verify processes are terminated
    assert not flowmanagerMP.job_executor_process.is_alive()
    if flowmanagerMP.result_processor_process:
        assert not flowmanagerMP.result_processor_process.is_alive()

def test_error_in_result_processing():
    """Test handling of errors in result processing function"""
    def failing_processor(result):
        raise Exception("Result processing error")
    
    flowmanagerMP = FlowManagerMP(ErrorTestJob(), failing_processor, serial_processing=True)
    
    # Submit tasks
    for i in range(3):
        flowmanagerMP.submit_task({'task_id': i})
    
    flowmanagerMP.wait_for_completion()
    # Should complete without hanging or crashing

def test_memory_error_handling():
    """Test handling of memory errors"""
    results = []
    def collect_result(result):
        results.append(result)
    
    flowmanagerMP = FlowManagerMP(ErrorTestJob(), collect_result, serial_processing=True)
    
    # Submit task that will cause memory error
    flowmanagerMP.submit_task({'task_id': 1, 'memory_error': True})
    
    flowmanagerMP.wait_for_completion()
    
    # Process should handle the memory error gracefully
    assert len(results) == 0  # No results should be processed

def test_unpicklable_result():
    """Test handling of unpicklable results"""
    results = []
    def collect_result(result):
        results.append(result)
    
    flowmanagerMP = FlowManagerMP(ErrorTestJob(), collect_result, serial_processing=True)
    
    # Submit task that returns unpicklable result
    flowmanagerMP.submit_task({'task_id': 1, 'invalid_result': True})
    
    flowmanagerMP.wait_for_completion()
    
    # Process should handle the pickling error gracefully
    assert len(results) == 0  # No results should be processed

if __name__ == '__main__':
    pytest.main(['-v', 'test_error_conditions.py'])



================================================
FILE: tests/test_flowmanager.py
================================================
import asyncio
import time
from typing import Any, Dict

import pytest

from flow4ai import f4a_logging as logging
from flow4ai.dsl import DSLComponent, JobsDict, p, wrap
from flow4ai.flowmanager import FlowManager
from flow4ai.job import JobABC

# Configure logging
logger = logging.getLogger(__name__)


class ProcessorJob(JobABC):
    """Example component that implements JobABC interface."""
    def __init__(self, name, process_type):
        super().__init__(name)
        self.process_type = process_type
    
    async def run(self, task: Dict[str, Any]) -> Dict[str, Any]:
        return f"Processor {self.name} of type {self.process_type}"


def test_execute_job_graph_from_dsl():
    """
    Test a complex DSL with a mix of JobABC and functions and lambdas.
    """
    times = lambda x: x*2
    add = lambda x: x+3
    square = lambda x: x**2
    
    def test_context(j_ctx):
        task = j_ctx["task"]
        inputs = j_ctx["inputs"]
        return {"task": task, "inputs": inputs}
        
    analyzer2 = ProcessorJob("Analyzer2", "analyze")
    transformer = ProcessorJob("Transformer", "transform")
    aggregator = ProcessorJob("Aggregator", "aggregate")
    formatter = ProcessorJob("Formatter", "format")
    cache_manager = ProcessorJob("CacheManager", "cache")

    jobs:JobsDict = wrap({
            "analyzer2": analyzer2,
            "cache_manager": cache_manager,
            "times": times,
            "transformer": transformer,
            "formatter": formatter,
            "add": add,
            "square": square,
            "aggregator": aggregator,
            "test_context": test_context
        })

    jobs["times"].save_result = True
    jobs["add"].save_result = True
    jobs["square"].save_result = True

    dsl:DSLComponent = (
        p(jobs["analyzer2"], jobs["cache_manager"], jobs["times"]) 
        >> jobs["transformer"] 
        >> jobs["formatter"] 
        >> (jobs["add"] | jobs["square"]) 
        >> jobs["test_context"]
        >> jobs["aggregator"] 
    )
        
    fm = FlowManager()
    # Using add_dsl without a graph_name - it should generate one based on head jobs
    fq_name = fm.add_dsl(dsl)
    print(f"Auto-generated FQ name: {fq_name}")
    
    # Extract the auto-generated graph name from the FQ name
    # FQ name format is typically: graph_name$$$$job_name$$
    auto_graph_name = fq_name.split('$$')[0]
    task = {"times.x": 1, "add.x": 2, "square.x": 3}
    fm.submit_task(task,fq_name)
    success = fm.wait_for_completion()
    assert success, "Timed out waiting for tasks to complete"
    
    # Print results
    print("Task counts:", fm.get_counts())
    results = fm.pop_results()
    
    print("\nCompleted tasks:")
    for job_name, job_results in results["completed"].items():
        for result_data in job_results:
            print(f"- {job_name}: {result_data['result']}")
    
    print("\nErrors:")
    if results["errors"]:
        error_messages = []
        for job_name, job_errors in results["errors"].items():
            for error_data in job_errors:
                error_msg = f"- {job_name}: {error_data['error']}"
                print(error_msg)
                error_messages.append(error_msg)
        
        # Raise exception with all errors
        raise Exception("Errors occurred during job execution:\n" + "\n".join(error_messages))
    
    print("\nResults:")
    print(results["completed"].values())
    result_dict = list(results["completed"].values())[0][0] # [0]= first job
    # Assert using the auto-generated graph name instead of hardcoded name
    expected_result = f"Processor {auto_graph_name}$$$$aggregator$$ of type aggregate"
    assert result_dict["result"] == expected_result
    # Convert Task to dict or extract the dictionary data
    task_pass_through = result_dict["task_pass_through"]
    # Check that task_pass_through contains all the expected keys and values
    assert all(key in task_pass_through and task_pass_through[key] == value for key, value in task.items())
    assert result_dict["SAVED_RESULTS"] == {"times": 2, "add": 5, "square": 9}
    

class DelayedJob(JobABC):
    async def run(self, task):
        short_name = self.parse_job_name(self.name)
        delay = task[short_name]
        logger.debug(f"Executing DelayedJob for {task} with delay {delay}")
        await asyncio.sleep(delay)
        return {"status": f"{self.name} complete"}

def create_tm(graph_name:str):
    dsl = DelayedJob("delayed")
    fm = FlowManager(on_complete=lambda x: logger.debug(f"received {x}"))
    fq_name = fm.add_dsl(dsl, graph_name)
    return fm, fq_name

def execute_tm_with_delay(delay, task_count=10):
    tm, fq_name = create_tm("test_parallel_execution" + str(delay))
    task = {"delayed": delay}
    start_time = time.perf_counter()
    for i in range(task_count):
        tm.submit_task(task, fq_name)
    tm.wait_for_completion()
    end_time = time.perf_counter()
    execution_time = end_time - start_time
    logger.info(f"*** Execution time for {task_count} tasks = {execution_time}")
    return execution_time, tm

def test_parallel_execution():
    execution_time, tm = execute_tm_with_delay(1.0)
    result_count = tm.get_counts()
    assert result_count["errors"] == 0, f"{result_count['errors']} errors occurred during job execution"
    assert execution_time < 1.5

    execution_time, tm = execute_tm_with_delay(2.0)
    result_count = tm.get_counts()
    assert result_count["errors"] == 0, f"{result_count['errors']} errors occurred during job execution"
    assert execution_time < 2.5


def test_parallel_load():
    logger.info("Executing parallel load tasks = 500")
    execution_time, tm = execute_tm_with_delay(1.0, 500)
    result_count = tm.get_counts()
    assert result_count["errors"] == 0, f"{result_count['errors']} errors occurred during job execution"
    assert execution_time < 1.4

    logger.info("Executing parallel load tasks = 1000")
    execution_time, tm = execute_tm_with_delay(1.0, 1000)
    result_count = tm.get_counts()
    assert result_count["errors"] == 0, f"{result_count['errors']} errors occurred during job execution"
    assert execution_time < 1.8

    logger.info("Executing parallel load tasks = 2000")
    execution_time, tm = execute_tm_with_delay(1.0, 2000)
    result_count = tm.get_counts()
    assert result_count["errors"] == 0, f"{result_count['errors']} errors occurred during job execution"
    assert execution_time < 2.5

    logger.info("Executing parallel load tasks = 5000")
    execution_time, tm = execute_tm_with_delay(1.0, 5000)
    result_count = tm.get_counts()
    assert result_count["errors"] == 0, f"{result_count['errors']} errors occurred during job execution"
    assert execution_time < 4.0


def test_flowmanager_execute_method():
    """Test the execute method of TaskManager which simplifies the workflow."""
    # Define functions that properly work together in a graph
    def square(x):
        return x**2
        
    def multiply_with_context(j_ctx):
        inputs = j_ctx["inputs"]
        square_result = inputs["square"]["result"]
        return square_result * 10
    
    def create_square_multiply_dsl():
        """Create a fresh DSL with square and multiply jobs."""
        jobs = wrap({
            "square": square,
            "multiply": multiply_with_context
        })
        
        # Need to save result from square for multiply to access
        jobs["square"].save_result = True
        
        # Create the DSL pipeline
        return jobs["square"] >> jobs["multiply"]
    
    # Create the initial DSL
    dsl = create_square_multiply_dsl()

    logger.debug("DSL structure1:")
    logger.debug(dsl)
    
    # Use execute method directly
    fm = FlowManager()
    task = {"square.x": 4}
    
    # Test with dsl and graph_name
    errors, result_dict = fm.execute(task, dsl=dsl, graph_name="test_execute1")
    
    assert errors == {}, "Errors occurred"
    assert result_dict is not None, "No result returned"
    
    # The result should be (4^2)*10 = 160
    assert result_dict["result"] == 160
    assert "SAVED_RESULTS" in result_dict
    assert "square" in result_dict["SAVED_RESULTS"]
    assert result_dict["SAVED_RESULTS"]["square"] == 16
    
    logger.debug("DSL structure2:")
    logger.debug(dsl)

    # Create a fresh DSL for the second test to avoid DSL mutation issues
    fresh_dsl = create_square_multiply_dsl()
    logger.debug("Fresh DSL structure:")
    logger.debug(fresh_dsl)

    # Test with fresh DSL
    task = {"square.x": 5}
    errors, result_dict = fm.execute(task, dsl=fresh_dsl, graph_name="test_execute2")
    
    assert errors == {}, "Errors occurred"
    assert result_dict is not None, "No result returned"
    
    # The result should be (5^2)*10 = 250
    assert result_dict["result"] == 250
    assert "SAVED_RESULTS" in result_dict
    assert "square" in result_dict["SAVED_RESULTS"]
    assert result_dict["SAVED_RESULTS"]["square"] == 25


def test_flowmanager_run_static_method():
    """Test the static run method of TaskManager for one-line execution."""
    def double(x):
        return x*2
        
    def increment_with_context(j_ctx):
        inputs = j_ctx["inputs"]
        double_result = inputs["double"]["result"]
        return double_result + 1
    
    jobs = wrap({
        "double": double,
        "increment": increment_with_context
    })
    
    # Save result for the context
    jobs["double"].save_result = True
    
    dsl = jobs["double"] >> jobs["increment"]
    task = {"double.x": 3}
    
    # Use the static run method
    errors, result_dict = FlowManager.run(dsl, task, "test_run_method")
    
    assert errors == {}, "Errors occurred"
    assert result_dict is not None, "No result returned"
    
    # The result should be (3*2)+1 = 7
    assert result_dict["result"] == 7
    assert "SAVED_RESULTS" in result_dict
    assert "double" in result_dict["SAVED_RESULTS"]
    assert result_dict["SAVED_RESULTS"]["double"] == 6

@pytest.mark.skip("Skipping test, functionality not used, requirements unclear")
def test_display_results(capsys):
    """Test the display_results method for plain text output."""
    def add(x):
        return x + 5
        
    def subtract_with_context(j_ctx):
        inputs = j_ctx["inputs"]
        add_result = inputs["add"]["result"]
        return add_result - 2
    
    jobs = wrap({
        "add": add,
        "subtract": subtract_with_context
    })
    
    # Save result for context
    jobs["add"].save_result = True
    
    dsl = jobs["add"] >> jobs["subtract"]
    task = {"add.x": 10}
    
    fm = FlowManager()
    errors, result_dict = fm.execute(task, dsl=dsl, graph_name="test_display")
    
    assert errors == {}, "Errors occurred"
    assert result_dict is not None, "No result returned"
    
    # The result should be (10+5)-2 = 13
    assert result_dict["result"] == 13
    assert "SAVED_RESULTS" in result_dict
    assert "add" in result_dict["SAVED_RESULTS"]
    assert result_dict["SAVED_RESULTS"]["add"] == 15
    
    
    # Call display_results
    displayed_results = fm.display_results(result_dict)
    
    # Capture stdout
    captured = capsys.readouterr()
    
    # Verify the output contains expected text
    assert "Completed tasks:" in captured.out
    assert "test_display" in captured.out
    
    # Verify the returned results are the same as input
    assert displayed_results == result_dict
    
    # Verify we can call display_results without providing results
    # Use the same graph name as above
    task = {"add.x": 20}
    fm.submit_task(task, fq_name)
    success = fm.wait_for_completion()
    assert success, "Timed out waiting for tasks to complete"
    
    # Call display_results without providing results
    fm.display_results()
    
    # Capture stdout
    captured = capsys.readouterr()
    
    # Verify the output contains expected text
    assert "Completed tasks:" in captured.out


def test_error_handling_in_execute():
    """Test that the execute method properly handles errors."""
    def failing_job(x):
        raise ValueError("This job intentionally fails")
    
    # Create job dictionary with the failing job
    job_dict = {}
    single_job = wrap({"failing": failing_job})
    
    # Create a DSL with just this single job
    dsl = single_job
    task = {"failing.x": 1}
    
    fm = FlowManager()
    
    # The execute method should raise an exception when a job fails
    try:
        fm.execute(task, dsl=dsl, graph_name="test_error_handling")
        assert False, "execute() did not raise an exception when a job failed"
    except Exception as e:
        assert "This job intentionally fails" in str(e), "Unexpected error message"


def test_timeout_handling_in_execute():
    """Test that the execute method properly handles timeouts."""
    async def slow_job(x):
        await asyncio.sleep(2)  # Sleep for 2 seconds
        return x
    
    # Create the DSL directly
    single_job = wrap({"slow": slow_job})
    
    # For a single job, just use the job itself as the DSL
    dsl = single_job
    task = {"slow.x": 1}
    
    fm = FlowManager()


def test_submit_multiple_tasks():
    """Test submitting multiple Tasks at once using the updated submit method."""
    from flow4ai.job import Task

    # Define a simple job
    processor = ProcessorJob("Processor", "process")
    
    # Create DSL directly with the processor job
    dsl = processor
    
    # Initialize FlowManager and add the DSL
    fm = FlowManager()
    fq_name = fm.add_dsl(dsl, "test_multiple_tasks")
    
    # Create a list of tasks to submit
    tasks = [
        Task({"value": 1}, fq_name=fq_name),
        Task({"value": 2}, fq_name=fq_name),
        Task({"value": 3}, fq_name=fq_name)
    ]
    
    # Submit the list of tasks
    logger.info("Submitting multiple tasks at once")
    fm.submit_task(tasks, fq_name)
    
    # Wait for completion
    success = fm.wait_for_completion()
    assert success, "Timed out waiting for tasks to complete"
    
    # Check results
    results = fm.pop_results()
    
    # Verify no errors
    assert not results["errors"], "Errors occurred during task execution"
    
    # Verify completed task count
    result_count = fm.get_counts()
    assert result_count["completed"] == 3, f"Expected 3 completed tasks, got {result_count['completed']}"
    
    # Log the results for inspection
    logger.info(f"Results from multiple tasks: {results['completed']}")


def test_submit_tasks_with_different_data():
    """Test submitting multiple Tasks with different data using the updated submit method."""
    from flow4ai.job import Task
    
    class DataProcessor(JobABC):
        """Job that processes data and returns a modified version."""
        async def run(self, task):
            # Process based on data type
            if isinstance(task.get("data"), int):
                return {"processed": task.get("data") * 2, "type": "integer"}
            elif isinstance(task.get("data"), str):
                return {"processed": task.get("data").upper(), "type": "string"}
            elif isinstance(task.get("data"), list):
                return {"processed": len(task.get("data")), "type": "list"}
            else:
                return {"processed": None, "type": "unknown"}
    
    # Create DSL with the data processor job
    processor = DataProcessor("DataProcessor")
    dsl = processor
    
    # Initialize FlowManager and add the DSL
    fm = FlowManager()
    fq_name = fm.add_dsl(dsl, "test_different_data")
    
    # Create tasks with different types of data
    tasks = [
        Task({"data": 10}, fq_name=fq_name),            # Integer
        Task({"data": "hello world"}, fq_name=fq_name), # String
        Task({"data": [1, 2, 3, 4]}, fq_name=fq_name),  # List
        Task({"data": None}, fq_name=fq_name)           # None/unknown
    ]
    
    # Submit the list of tasks
    logger.info("Submitting tasks with different data types")
    fm.submit_task(tasks, fq_name)
    
    # Wait for completion
    success = fm.wait_for_completion()
    assert success, "Timed out waiting for tasks to complete"
    
    # Check results
    results = fm.pop_results()
    
    # Verify no errors
    assert not results["errors"], "Errors occurred during task execution"
    
    # Verify completed task count
    result_count = fm.get_counts()
    assert result_count["completed"] == 4, f"Expected 4 completed tasks, got {result_count['completed']}"
    
    # Log the results for inspection
    logger.info(f"Results from different data tasks: {results['completed']}")
    
    # Verify the specific results for each data type
    completed_results = []
    for job_results in results["completed"].values():
        # Add all result objects to our list
        completed_results.extend(job_results)
    
    # Check that we got the expected processed results
    assert any(r["processed"] == 20 and r["type"] == "integer" for r in completed_results), "Integer processing failed"
    assert any(r["processed"] == "HELLO WORLD" and r["type"] == "string" for r in completed_results), "String processing failed"
    assert any(r["processed"] == 4 and r["type"] == "list" for r in completed_results), "List processing failed"
    assert any(r["processed"] is None and r["type"] == "unknown" for r in completed_results), "None processing failed"


def test_submit_without_fqname():
    """Test submitting tasks without specifying fq_name when only one DSL has been added.
    
    When only one job graph has been added to FlowManager, the fq_name parameter in submit()
    can be omitted for convenience.
    """
    from flow4ai.job import Task

    # Define a simple job
    processor = ProcessorJob("Processor", "process")
    
    # Create DSL directly with the processor job
    dsl = processor
    
    # Initialize FlowManager and add the DSL
    fm = FlowManager()
    fq_name = fm.add_dsl(dsl, "test_without_fqname")
    
    # Create a task
    task = Task({"value": 42})
    
    # Submit the task WITHOUT specifying fq_name
    logger.info("Submitting task without specifying fq_name")
    fm.submit_task(task)
    
    # Wait for completion
    success = fm.wait_for_completion()
    assert success, "Timed out waiting for tasks to complete"
    
    # Check results
    results = fm.pop_results()
    
    # Verify no errors
    assert not results["errors"], "Errors occurred during task execution"
    
    # Verify completed task count
    result_count = fm.get_counts()
    assert result_count["completed"] == 1, f"Expected 1 completed task, got {result_count['completed']}"
    
    # Log the results for inspection
    logger.info(f"Results: {results['completed']}")
    
    # Verify the result contains the expected processor output
    job_key = next(iter(results["completed"]))
    result_value = results["completed"][job_key][0]["result"]
    expected = f"Processor test_without_fqname$$$$Processor$$ of type process"
    assert result_value == expected, f"Expected '{expected}', got '{result_value}'"


def test_submit_multiple_tasks_pipeline():
    """Test submitting multiple Tasks that flow through a multi-step pipeline.
    
    This test demonstrates both JobABC subclasses and wrapped functions approaches for handling inputs,
    while following proper FlowManager usage patterns - using a single instance and adding DSL once.
    """
    from flow4ai.job import Task

    # Define a JobABC subclass for number generation
    class NumberGenerator(JobABC):
        """Job that generates a number sequence."""
        async def run(self, task):
            start = task.get("start", 0)
            count = task.get("count", 5)
            return {"numbers": list(range(start, start + count))}
    
    # Define a JobABC subclass that uses get_inputs() method to access previous job results
    class NumberTransformer(JobABC):
        """Job that transforms numbers based on operation using JobABC.get_inputs()."""
        async def run(self, task):
            # Get inputs directly using JobABC method (no j_ctx needed)
            inputs = self.get_inputs()
            
            # Log inputs for debugging with INFO level to see in test output
            self.logger.info(f"Transformer inputs: {inputs}")
            
            # Get the numbers from the generator job's output - direct access instead of through result
            numbers = inputs.get("generator", {}).get("numbers", [])
            
            self.logger.info(f"Extracted numbers from generator: {numbers}")
            
            # Get the operation to perform
            operation = task.get("operation", "square")
            
            # Transform numbers based on the operation
            if operation == "square":
                result = [n * n for n in numbers]
            elif operation == "double":
                result = [n * 2 for n in numbers]
            elif operation == "increment":
                result = [n + 1 for n in numbers]
            else:
                result = numbers
            
            self.logger.info(f"Transformer output: {result} with operation {operation}")
            return {"transformed": result, "operation": operation}
    
    # Define a function (not a JobABC class) that will be wrapped
    # This demonstrates how regular functions need to use j_ctx parameter to access inputs
    def aggregate_results(j_ctx):
        """Function that aggregates results from transformer using j_ctx parameter."""
        # Access inputs through j_ctx parameter
        inputs = j_ctx["inputs"]
        task = j_ctx["task"]
        
        logger.info(f"Aggregator j_ctx inputs: {inputs}")
        
        # Get results from transformer - direct access instead of through result
        transformed = inputs.get("transformer", {}).get("transformed", [])
        operation = inputs.get("transformer", {}).get("operation", "unknown")
        
        logger.info(f"Aggregator extracted transformed: {transformed}, operation: {operation}")
        
        # Calculate aggregate values
        if not transformed:
            logger.warning("Transformed list is empty, returning zeros")
            return {"sum": 0, "count": 0, "avg": 0, "operation": operation}
            
        total_sum = sum(transformed)
        count = len(transformed)
        avg = total_sum / count if count > 0 else 0
        
        logger.info(f"Aggregator results: sum={total_sum}, count={count}, avg={avg}")
        
        return {
            "sum": total_sum,
            "count": count,
            "avg": avg,
            "operation": operation
        }
    
    # Create job instances
    generator = NumberGenerator("generator")
    transformer = NumberTransformer("transformer")
    
    # Create the DSL pipeline with both JobABC classes and a wrapped function
    jobs = wrap({
        "generator": generator,
        "transformer": transformer,
        "aggregator": aggregate_results  # Regular function that will be wrapped
    })
 
    # Save results for context between pipeline steps
    jobs["generator"].save_result = True
    jobs["transformer"].save_result = True
    
    # Build the pipeline: generator -> transformer -> aggregator
    dsl = jobs["generator"] >> jobs["transformer"] >> jobs["aggregator"]
    
    # Initialize a SINGLE FlowManager instance (FlowManager is effectively a singleton)
    fm = FlowManager()
    
    # Add the DSL ONCE to get a fully qualified name
    fq_name = fm.add_dsl(dsl, "test_pipeline")
    
    # Demo approach 1: Submit multiple tasks one-by-one, with wait_for_completion after each
    logger.info("Approach 1: Multiple individual submit() calls with wait_for_completion after each")
    
    # Task 1: Numbers 0-4, squared 
    task1 = Task({
        "start": 0, 
        "count": 5, 
        "operation": "square"
    })
    
    # Submit task 1
    logger.info("Submitting task 1 (square operation)")
    fm.submit_task(task1, fq_name)
    
    # Wait for task 1 to complete
    success = fm.wait_for_completion()
    assert success, "Timed out waiting for task 1 to complete"
    
    # Check results for task 1
    results = fm.pop_results()
    assert not results["errors"], f"Errors occurred during task 1 execution: {results['errors']}"
    logger.info(f"Task 1 completed successfully: {results['completed']}")
    
    # Verify results for task 1 (square operation on numbers 0-4)
    task1_result = results['completed'][fq_name][0]
    saved_results = task1_result.get('SAVED_RESULTS', {})
    
    # Check generator output
    assert saved_results.get('generator', {}).get('numbers') == [0, 1, 2, 3, 4], "Generator should produce [0, 1, 2, 3, 4]"
    
    # Check transformer output
    assert saved_results.get('transformer', {}).get('transformed') == [0, 1, 4, 9, 16], "Transformer with square operation should produce [0, 1, 4, 9, 16]"
    assert saved_results.get('transformer', {}).get('operation') == 'square', "Operation should be 'square'"
    
    # Check aggregator output
    assert task1_result.get('sum') == 30, "Aggregator sum should be 30"
    assert task1_result.get('count') == 5, "Aggregator count should be 5"
    assert task1_result.get('avg') == 6.0, "Aggregator average should be 6.0"
    
    # Verify completed task count after task 1
    result_count = fm.get_counts()
    assert result_count["completed"] == 1, f"Expected 1 completed task, got {result_count['completed']}"
    completed_so_far = result_count["completed"]  # Track completed count
    
    # Task 2: Numbers 5-9, doubled
    task2 = Task({
        "start": 5, 
        "count": 5, 
        "operation": "double"
    })
    
    # Submit task 2
    logger.info("Submitting task 2 (double operation)")
    fm.submit_task(task2) # fq_name is optional when there is only one job graph
    
    # Wait for task 2 to complete
    success = fm.wait_for_completion()
    assert success, "Timed out waiting for task 2 to complete"
    
    # Check results for task 2
    results = fm.pop_results()
    assert not results["errors"], f"Errors occurred during task 2 execution: {results['errors']}"
    logger.info(f"Task 2 completed successfully: {results['completed']}")
    
    # Verify results for task 2 (double operation on numbers 5-9)
    task2_result = results['completed'][fq_name][0]
    saved_results = task2_result.get('SAVED_RESULTS', {})
    
    # Check generator output
    assert saved_results.get('generator', {}).get('numbers') == [5, 6, 7, 8, 9], "Generator should produce [5, 6, 7, 8, 9]"
    
    # Check transformer output
    assert saved_results.get('transformer', {}).get('transformed') == [10, 12, 14, 16, 18], "Transformer with double operation should produce [10, 12, 14, 16, 18]"
    assert saved_results.get('transformer', {}).get('operation') == 'double', "Operation should be 'double'"
    
    # Check aggregator output
    assert task2_result.get('sum') == 70, "Aggregator sum should be 70"
    assert task2_result.get('count') == 5, "Aggregator count should be 5"
    assert task2_result.get('avg') == 14.0, "Aggregator average should be 14.0"
    
    # Verify completed task count after task 2
    result_count = fm.get_counts()
    # FM.get_counts() returns cumulative counts, so we expect 2 total completed tasks now
    assert result_count["completed"] == completed_so_far + 1, f"Expected {completed_so_far + 1} total completed tasks, got {result_count['completed']}"
    completed_so_far = result_count["completed"]  # Update completed count
    
    # Demo approach 2: Submit multiple tasks at once using lists
    logger.info("\nApproach 2: Submit multiple tasks at once using Task[]")
    
    # Create a list of tasks
    tasks = [
        # Task 3: Numbers 10-14, incremented
        Task({
            "start": 10, 
            "count": 5, 
            "operation": "increment"
        }),
        # Task 4: Numbers 15-19, squared
        Task({
            "start": 15, 
            "count": 5, 
            "operation": "square"
        })
    ]
    
    # Submit multiple tasks at once
    logger.info("Submitting tasks 3 & 4 as a batch")
    fm.submit_task(tasks) # fq_name is optional when there is only one job graph
    
    # Wait for all tasks to complete
    success = fm.wait_for_completion()
    assert success, "Timed out waiting for tasks 3 & 4 to complete"
    
    # Check results for tasks 3 & 4
    results = fm.pop_results()
    assert not results["errors"], f"Errors occurred during tasks 3 & 4 execution: {results['errors']}"
    
    # Verify results for task 3 (increment operation on numbers 10-14)
    task3_result = results['completed'][fq_name][0]
    saved_results_task3 = task3_result.get('SAVED_RESULTS', {})
    
    # Check generator output for task 3
    assert saved_results_task3.get('generator', {}).get('numbers') == [10, 11, 12, 13, 14], "Generator should produce [10, 11, 12, 13, 14]"
    
    # Check transformer output for task 3
    assert saved_results_task3.get('transformer', {}).get('transformed') == [11, 12, 13, 14, 15], "Transformer with increment operation should produce [11, 12, 13, 14, 15]"
    assert saved_results_task3.get('transformer', {}).get('operation') == 'increment', "Operation should be 'increment'"
    
    # Check aggregator output for task 3
    assert task3_result.get('sum') == 65, "Aggregator sum should be 65"
    assert task3_result.get('count') == 5, "Aggregator count should be 5"
    assert task3_result.get('avg') == 13.0, "Aggregator average should be 13.0"
    
    # Verify results for task 4 (square operation on numbers 15-19)
    task4_result = results['completed'][fq_name][1]
    saved_results_task4 = task4_result.get('SAVED_RESULTS', {})
    
    # Check generator output for task 4
    assert saved_results_task4.get('generator', {}).get('numbers') == [15, 16, 17, 18, 19], "Generator should produce [15, 16, 17, 18, 19]"
    
    # Check transformer output for task 4
    assert saved_results_task4.get('transformer', {}).get('transformed') == [225, 256, 289, 324, 361], "Transformer with square operation should produce [225, 256, 289, 324, 361]"
    assert saved_results_task4.get('transformer', {}).get('operation') == 'square', "Operation should be 'square'"
    
    # Check aggregator output for task 4
    assert task4_result.get('sum') == 1455, "Aggregator sum should be 1455"
    assert task4_result.get('count') == 5, "Aggregator count should be 5"
    assert task4_result.get('avg') == 291.0, "Aggregator average should be 291.0"
    
    # Verify completed task count after tasks 3 & 4
    result_count = fm.get_counts()
    # We expect 2 more completed tasks in addition to what we had before
    assert result_count["completed"] == completed_so_far + 2, f"Expected {completed_so_far + 2} total completed tasks, got {result_count['completed']}"
    
    # Log the results for inspection
    logger.info(f"Tasks 3 & 4 completed successfully: {results['completed']}")



================================================
FILE: tests/test_flowmanager_details.py
================================================
import pytest

from flow4ai import f4a_logging as logging
from flow4ai.dsl import DSLComponent, JobsDict, wrap
from flow4ai.flowmanager import FlowManager
from flow4ai.job import JobABC, Task

# Configure logging
logger = logging.getLogger(__name__)


class NumberGenerator(JobABC):
    """Generates a sequence of numbers starting from 'start' with length 'count'."""
    def __init__(self, name):
        super().__init__(name)
    
    async def run(self, task):
        start = task.get("start", 0)
        count = task.get("count", 5)
        numbers = list(range(start, start + count))
        logger.info(f"[{self.name}] Generated numbers: {numbers}")
        return {"numbers": numbers}


class MathOperation(JobABC):
    """Performs a mathematical operation on numbers from previous job."""
    def __init__(self, name, operation="square"):
        super().__init__(name)
        self.operation = operation
    
    async def run(self, task):
        # Get inputs from previous job using the get_inputs method
        inputs = self.get_inputs()
        logger.info(f"[{self.name}] Received inputs: {inputs}")
        
        # Get the numbers from the input job (assuming it's the first input)
        input_job_name = list(inputs.keys())[0] if inputs else None
        numbers = inputs.get(input_job_name, {}).get("numbers", [])
        
        # Override operation if specified in task
        operation = task.get("operation", self.operation)
        
        if not numbers:
            logger.warning(f"[{self.name}] No numbers found in inputs")
            return {"result": [], "operation": operation}
            
        logger.info(f"[{self.name}] Performing {operation} on {numbers}")
        
        # Perform the requested operation
        if operation == "square":
            result = [n * n for n in numbers]
        elif operation == "double":
            result = [n * 2 for n in numbers]
        elif operation == "increment":
            result = [n + 1 for n in numbers]
        elif operation == "sum":
            result = [sum(numbers)]
        elif operation == "multiply":
            import operator
            from functools import reduce
            result = [reduce(operator.mul, numbers, 1)]
        else:
            result = numbers
            
        logger.info(f"[{self.name}] Result: {result}")
        return {"numbers": result, "operation": operation}


class Aggregator(JobABC):
    """Aggregates results from previous math operations."""
    def __init__(self, name):
        super().__init__(name)
    
    async def run(self, task):
        # Get inputs from all previous jobs
        inputs = self.get_inputs()
        logger.info(f"[{self.name}] Aggregator inputs: {inputs}")
        
        all_results = {}
        
        # Collect all numbers from inputs
        for job_name, job_result in inputs.items():
            if "numbers" in job_result:
                all_results[job_name] = {
                    "numbers": job_result.get("numbers", []),
                    "operation": job_result.get("operation", "unknown")  
                }
        
        # Calculate statistics on all collected numbers
        all_numbers = []
        for job_info in all_results.values():
            all_numbers.extend(job_info.get("numbers", []))
            
        if not all_numbers:
            logger.warning(f"[{self.name}] No numbers found to aggregate")
            return {"sum": 0, "avg": 0, "min": 0, "max": 0, "count": 0}
        
        stats = {
            "sum": sum(all_numbers),
            "avg": sum(all_numbers) / len(all_numbers),
            "min": min(all_numbers),
            "max": max(all_numbers),
            "count": len(all_numbers)
        }
        
        logger.info(f"[{self.name}] Aggregated stats: {stats}")
        return stats


class SimpleJob(JobABC):
    """Example component that implements JobABC interface for testing."""
    def __init__(self, name):
        super().__init__(name)
    
    async def run(self, task):
        logger.info(f"Processing task in {self.name}")
        return {"result": f"Job {self.name} processed task"}


def test_add_dsl_resubmission():
    """Test that resubmitting the same DSL returns the original FQ name.
    
    This verifies our enhancement to the add_dsl method that tracks previously added DSLs
    and returns their FQ name without reprocessing them.
    """
    # Create job instances
    job1 = SimpleJob("job1")
    job2 = SimpleJob("job2")
    
    # Wrap jobs in a dictionary
    jobs = wrap({
        "job1": job1,
        "job2": job2
    })
    
    # Create a simple pipeline: job1 >> job2
    dsl = jobs["job1"] >> jobs["job2"]
    
    # Initialize FlowManager
    fm = FlowManager()
    
    # First submission - should process normally
    logger.info("First DSL submission")
    fq_name1 = fm.add_dsl(dsl, "test_graph", "test")
    logger.info(f"Received FQ name: {fq_name1}")
    
    # Verify DSL is marked as already added
    assert hasattr(dsl, "_f4a_already_added")
    assert dsl._f4a_already_added is True
    
    # Verify head job has reference to source DSL
    head_job = fm.job_graph_map[fq_name1]
    assert hasattr(head_job, "_f4a_source_dsl")
    assert head_job._f4a_source_dsl is dsl
    
    # Second submission with same DSL - should return original FQ name
    logger.info("Second DSL submission (same object)")
    fq_name2 = fm.add_dsl(dsl, "test_graph", "test")
    logger.info(f"Received FQ name: {fq_name2}")
    
    # Should be the same FQ name
    assert fq_name1 == fq_name2, f"Expected same FQ name: {fq_name1} != {fq_name2}"
    
    # Third submission with different graph name and variant
    # Since it's the same DSL object, should still return original FQ name
    logger.info("Third DSL submission (same object, different graph name)")
    fq_name3 = fm.add_dsl(dsl, "different_graph", "different")
    logger.info(f"Received FQ name: {fq_name3}")
    
    # Still should be the same FQ name
    assert fq_name1 == fq_name3, f"Expected same FQ name: {fq_name1} != {fq_name3}"


def test_dsl_submission_with_tasks():
    """Test submitting tasks to a reused DSL FQ name."""
    # Create job instances
    processor1 = SimpleJob("processor1")
    processor2 = SimpleJob("processor2")
    
    # Wrap jobs
    jobs = wrap({
        "processor1": processor1,
        "processor2": processor2
    })
    
    # Enable result saving
    jobs["processor1"].save_result = True
    jobs["processor2"].save_result = True
    
    # Create pipeline
    dsl = jobs["processor1"] >> jobs["processor2"]
    
    # Initialize FlowManager
    fm = FlowManager()
    
    # First submission
    fq_name = fm.add_dsl(dsl, "test_pipeline")
    
    # Create and submit a task
    task = Task({"input": "test data"})
    fm.submit_task(task, fq_name)
    
    # Wait for completion
    success = fm.wait_for_completion()
    assert success, "Task did not complete successfully"
    
    # Get results
    results = fm.pop_results()
    assert "completed" in results
    assert len(results["completed"]) > 0, "No completed results found"
    
    # Submit the same DSL again - should get same FQ name
    same_fq_name = fm.add_dsl(dsl, "different_name")
    assert fq_name == same_fq_name, "Did not get the same FQ name for resubmitted DSL"
    
    # Submit another task using the returned FQ name
    task2 = Task({"input": "more test data"})
    fm.submit_task(task2, same_fq_name)
    
    # Wait for completion
    success = fm.wait_for_completion()
    assert success, "Second task did not complete successfully"
    
    # Get results
    results2 = fm.pop_results()
    assert "completed" in results2
    assert len(results2["completed"]) > 0, "No completed results found for second task"


def test_new_dsl_with_same_structure():
    """Test that two DSLs with the same structure get unique FQ names.
    
    This test confirms the updated system behavior: Different DSL objects get unique FQ names
    even if they have the same structure, to prevent collisions and ensure correct input handling.
    """
    # Create a FlowManager
    fm = FlowManager()
    
    # Create first DSL
    job1 = SimpleJob("job1")
    job2 = SimpleJob("job2")
    jobs1 = wrap({
        "job1": job1,
        "job2": job2
    })
    dsl1 = jobs1["job1"] >> jobs1["job2"]
    
    # Add first DSL
    logger.info("Adding first DSL")
    fq_name1 = fm.add_dsl(dsl1)
    logger.info(f"First DSL FQ name: {fq_name1}")
    
    # Extract the auto-generated graph name
    graph_name1 = fq_name1.split('$$')[0]
    
    # Create second DSL with same structure but different objects
    job3 = SimpleJob("job1")  # Same name but different object
    job4 = SimpleJob("job2")  # Same name but different object
    jobs2 = wrap({
        "job1": job3,
        "job2": job4
    })
    dsl2 = jobs2["job1"] >> jobs2["job2"]
    
    # Add second DSL
    logger.info("Adding second DSL with same structure but different objects")
    fq_name2 = fm.add_dsl(dsl2)
    logger.info(f"Second DSL FQ name: {fq_name2}")
    
    # Extract the auto-generated graph name
    graph_name2 = fq_name2.split('$$')[0]
    
    # New behavior: Different DSLs should get unique FQ names even with the same structure
    assert fq_name1 != fq_name2, "Different DSL objects with same structure should get unique FQ names"
    
    # Check the _f4a_source_dsl reference for the first DSL's head job
    head_job1 = fm.job_graph_map[fq_name1]
    assert hasattr(head_job1, "_f4a_source_dsl")
    assert head_job1._f4a_source_dsl is dsl1, "First head job should reference first DSL"
    
    # Check the _f4a_source_dsl reference for the second DSL's head job
    head_job2 = fm.job_graph_map[fq_name2]
    assert hasattr(head_job2, "_f4a_source_dsl")
    assert head_job2._f4a_source_dsl is dsl2, "Second head job should reference second DSL"
    
    # Create a third DSL with different job name structure
    job5 = SimpleJob("processorA")
    job6 = SimpleJob("processorB")
    jobs3 = wrap({
        "processorA": job5,
        "processorB": job6
    })
    dsl3 = jobs3["processorA"] >> jobs3["processorB"]
    
    # Add third DSL
    logger.info("Adding third DSL with different job names")
    fq_name3 = fm.add_dsl(dsl3)
    logger.info(f"Third DSL FQ name: {fq_name3}")
    
    # Extract the auto-generated graph name
    graph_name3 = fq_name3.split('$$')[0]
    
    # FQ name should be different with different job names
    assert fq_name1 != fq_name3, "DSLs with different job names should get different FQ names"


def test_add_dsl_dict_single_graph_no_variants():
    """Test add_dsl_dict with a single graph without variants.
    
    This tests the simplified DSL dictionary format:
    { "graph_name": dsl }
    
    Uses math operations to demonstrate data flow between jobs.
    """
    # Create job instances for a math pipeline
    generator = NumberGenerator("generator")
    squarer = MathOperation("squarer", operation="square")
    aggregator = Aggregator("aggregator")
    
    # Wrap jobs in a dictionary
    jobs = wrap({
        "generator": generator,
        "squarer": squarer,
        "aggregator": aggregator
    })
    
    # Create a math pipeline: generator -> squarer -> aggregator
    dsl = jobs["generator"] >> jobs["squarer"] >> jobs["aggregator"]
    
    # Save results for context between pipeline steps
    jobs["generator"].save_result = True
    jobs["squarer"].save_result = True
    
    # Create DSL dict with a single graph, no variants
    dsl_dict = {
        "math_graph": dsl
    }
    
    # Initialize FlowManager
    fm = FlowManager()
    
    # Add the DSL dict
    logger.info("Adding DSL dict with single graph, no variants")
    fq_names = fm.add_dsl_dict(dsl_dict)
    
    # Verify a single FQ name was returned
    assert len(fq_names) == 1, "Should return exactly one FQ name"
    fq_name = fq_names[0]
    logger.info(f"Received FQ name: {fq_name}")
    
    # Verify the head job exists in the job map
    assert fq_name in fm.job_graph_map, f"FQ name {fq_name} should be in job_map"
    
    # Submit a task with numbers 1-5 to be squared
    task = Task({
        "start": 1,
        "count": 5,
        "operation": "square"
    })
    logger.info("Submitting task to the graph")
    fm.submit_task(task)  # No need to specify fq_name when only one graph exists
    
    success = fm.wait_for_completion()
    assert success, "Timed out waiting for task to complete"
    
    # Check results
    results = fm.pop_results()
    logger.info(f"Results: {results}")
    assert not results["errors"], f"Errors occurred during task execution: {results['errors']}"
    assert len(results["completed"]) > 0, "No completed tasks found"
    
    # Get the task result
    task_result = results['completed'][fq_name][0]
    logger.info(f"Task result: {task_result}")
    
    # Verify specific result values from the aggregator
    # For numbers 1,2,3,4,5 squared to 1,4,9,16,25, the sum is 55
    assert 'sum' in task_result, "Missing 'sum' in aggregator result"
    assert task_result['sum'] == 55, f"Expected sum=55, got {task_result['sum']}"
    
    # The average should be 55/5 = 11
    assert 'avg' in task_result, "Missing 'avg' in aggregator result"
    assert task_result['avg'] == 11.0, f"Expected avg=11.0, got {task_result['avg']}"
    
    # Min should be 1, max should be 25
    assert 'min' in task_result, "Missing 'min' in aggregator result"
    assert task_result['min'] == 1, f"Expected min=1, got {task_result['min']}"
    
    assert 'max' in task_result, "Missing 'max' in aggregator result"
    assert task_result['max'] == 25, f"Expected max=25, got {task_result['max']}"
    
    # Count should be 5
    assert 'count' in task_result, "Missing 'count' in aggregator result" 
    assert task_result['count'] == 5, f"Expected count=5, got {task_result['count']}"
    
    # Check saved results from each stage
    if "SAVED_RESULTS" in task_result:
        saved_results = task_result.get("SAVED_RESULTS", {})
        logger.info(f"Saved results: {saved_results}")
        
        # Verify generator results (numbers 1-5)
        assert "generator" in saved_results, "Missing generator in saved results"
        assert "numbers" in saved_results["generator"], "Missing 'numbers' in generator result"
        assert saved_results["generator"]["numbers"] == [1, 2, 3, 4, 5], \
            f"Expected [1,2,3,4,5], got {saved_results['generator']['numbers']}"
        
        # Verify squarer results (numbers squared: 1,4,9,16,25)
        assert "squarer" in saved_results, "Missing squarer in saved results"
        assert "numbers" in saved_results["squarer"], "Missing 'numbers' in squarer result"
        assert saved_results["squarer"]["numbers"] == [1, 4, 9, 16, 25], \
            f"Expected [1,4,9,16,25], got {saved_results['squarer']['numbers']}"
        assert saved_results["squarer"]["operation"] == "square", \
            f"Expected operation='square', got {saved_results['squarer']['operation']}"


def test_add_dsl_dict_single_graph_with_variants():
    """Test add_dsl_dict with a single graph with variants.
    
    This tests the DSL dictionary format with variants:
    { "graph_name": { "variant1": dsl1, "variant2": dsl2 } }
    
    Uses math operations with different configurations for each variant.
    """
    # Create a helper function to build a DSL pipeline with specified operation
    def create_math_pipeline(prefix: str, operation: str):
        generator = NumberGenerator(f"{prefix}_generator")
        math_op = MathOperation(f"{prefix}_operation", operation=operation)
        aggregator = Aggregator(f"{prefix}_aggregator")
        
        jobs = wrap({
            "generator": generator,
            "operation": math_op,
            "aggregator": aggregator
        })
        
        # Save results for all jobs to verify later
        jobs["generator"].save_result = True
        jobs["operation"].save_result = True
        
        return jobs["generator"] >> jobs["operation"] >> jobs["aggregator"]
    
    # Create pipelines for each variant with different operations
    dev_dsl = create_math_pipeline("dev", "square")   # Square operation for dev
    prod_dsl = create_math_pipeline("prod", "double")  # Double operation for prod
    
    # Create DSL dict with a single graph with variants
    dsl_dict = {
        "math_graph": {
            "dev": dev_dsl,
            "prod": prod_dsl
        }
    }
    
    # Initialize FlowManager
    fm = FlowManager()
    
    # Add the DSL dict
    logger.info("Adding DSL dict with single graph, with variants")
    fq_names = fm.add_dsl_dict(dsl_dict)
    
    # Verify two FQ names were returned (one for each variant)
    assert len(fq_names) == 2, "Should return exactly two FQ names (one for each variant)"
    logger.info(f"Received FQ names: {fq_names}")
    
    # Verify the head jobs exist in the job map
    for fq_name in fq_names:
        assert fq_name in fm.job_graph_map, f"FQ name {fq_name} should be in job_map"
    
    # Test the dev variant (square operation)
    dev_fq_name = [name for name in fq_names if "dev" in name][0]
    logger.info(f"Testing dev variant: {dev_fq_name}")
    
    # Submit a task with numbers 1-5 to be squared
    dev_task = Task({
        "start": 1,
        "count": 5
    })
    
    # Submit to the dev variant
    fm.submit_task(dev_task, dev_fq_name)
    success = fm.wait_for_completion()
    assert success, "Timed out waiting for dev task to complete"
    
    # Check results for dev variant
    dev_results = fm.pop_results()
    assert not dev_results["errors"], f"Errors occurred during dev task execution: {dev_results['errors']}"
    assert len(dev_results["completed"]) > 0, "No completed tasks found for dev variant"
    
    # Get the dev task result
    dev_task_result = dev_results['completed'][dev_fq_name][0]
    logger.info(f"Dev task result: {dev_task_result}")
    
    # Verify dev variant results (square operation)
    # For numbers 1,2,3,4,5 squared to 1,4,9,16,25, the sum is 55
    assert dev_task_result['sum'] == 55, f"Expected sum=55 for dev variant, got {dev_task_result['sum']}"
    assert dev_task_result['avg'] == 11.0, f"Expected avg=11.0 for dev variant, got {dev_task_result['avg']}"
    
    # Check saved results to verify operation type and input/output data
    if "SAVED_RESULTS" in dev_task_result:
        saved_results = dev_task_result.get("SAVED_RESULTS", {})
        logger.info(f"Dev variant saved results: {saved_results}")
        
        # Find the operation job results
        op_key = [key for key in saved_results.keys() if "operation" in key][0]
        assert saved_results[op_key]["operation"] == "square", \
            f"Expected operation='square' for dev variant, got {saved_results[op_key]['operation']}"
        assert saved_results[op_key]["numbers"] == [1, 4, 9, 16, 25], \
            f"Expected [1,4,9,16,25] for dev variant, got {saved_results[op_key]['numbers']}"
    
    # Test the prod variant (double operation)
    prod_fq_name = [name for name in fq_names if "prod" in name][0]
    logger.info(f"Testing prod variant: {prod_fq_name}")
    
    # Submit a task with numbers 1-5 to be doubled
    prod_task = Task({
        "start": 1,
        "count": 5
    })
    
    # Submit to the prod variant
    fm.submit_task(prod_task, prod_fq_name)
    success = fm.wait_for_completion()
    assert success, "Timed out waiting for prod task to complete"
    
    # Check results for prod variant
    prod_results = fm.pop_results()
    assert not prod_results["errors"], f"Errors occurred during prod task execution: {prod_results['errors']}"
    assert len(prod_results["completed"]) > 0, "No completed tasks found for prod variant"
    
    # Get the prod task result
    prod_task_result = prod_results['completed'][prod_fq_name][0]
    logger.info(f"Prod task result: {prod_task_result}")
    
    # Verify prod variant results (double operation)
    # For numbers 1,2,3,4,5 doubled to 2,4,6,8,10, the sum is 30
    assert prod_task_result['sum'] == 30, f"Expected sum=30 for prod variant, got {prod_task_result['sum']}"
    assert prod_task_result['avg'] == 6.0, f"Expected avg=6.0 for prod variant, got {prod_task_result['avg']}"
    
    # Check saved results to verify operation type and input/output data
    if "SAVED_RESULTS" in prod_task_result:
        saved_results = prod_task_result.get("SAVED_RESULTS", {})
        logger.info(f"Prod variant saved results: {saved_results}")
        
        # Find the operation job results
        op_key = [key for key in saved_results.keys() if "operation" in key][0]
        assert saved_results[op_key]["operation"] == "double", \
            f"Expected operation='double' for prod variant, got {saved_results[op_key]['operation']}"
        assert saved_results[op_key]["numbers"] == [2, 4, 6, 8, 10], \
            f"Expected [2,4,6,8,10] for prod variant, got {saved_results[op_key]['numbers']}"


def test_add_dsl_dict_multiple_graphs_no_variants():
    """Test add_dsl_dict with multiple graphs without variants.
    
    This tests the simplified DSL dictionary format with multiple graphs:
    { "graph1": dsl1, "graph2": dsl2 }
    
    Uses different math operations for each graph to demonstrate data flow.
    """
    # Create job instances for first graph - sum operation
    sum_generator = NumberGenerator("sum_generator")
    sum_operation = MathOperation("sum_operation", operation="sum")
    sum_aggregator = Aggregator("sum_aggregator")
    
    # Create job instances for second graph - multiply operation
    multiply_generator = NumberGenerator("multiply_generator")
    multiply_operation = MathOperation("multiply_operation", operation="multiply")
    multiply_aggregator = Aggregator("multiply_aggregator")
    
    # Wrap jobs for first graph (sum)
    sum_jobs = wrap({
        "generator": sum_generator,
        "operation": sum_operation,
        "aggregator": sum_aggregator
    })
    
    # Wrap jobs for second graph (multiply)
    multiply_jobs = wrap({
        "generator": multiply_generator,
        "operation": multiply_operation,
        "aggregator": multiply_aggregator
    })
    
    # Save results for context between pipeline steps
    sum_jobs["generator"].save_result = True
    sum_jobs["operation"].save_result = True
    multiply_jobs["generator"].save_result = True
    multiply_jobs["operation"].save_result = True
    
    # Create pipelines
    sum_dsl = sum_jobs["generator"] >> sum_jobs["operation"] >> sum_jobs["aggregator"]
    multiply_dsl = multiply_jobs["generator"] >> multiply_jobs["operation"] >> multiply_jobs["aggregator"]
    
    # Create DSL dict with multiple graphs, no variants
    dsl_dict = {
        "sum_graph": sum_dsl,
        "multiply_graph": multiply_dsl
    }
    
    # Initialize FlowManager
    fm = FlowManager()
    
    # Add the DSL dict
    logger.info("Adding DSL dict with multiple graphs, no variants")
    fq_names = fm.add_dsl_dict(dsl_dict)
    
    # Verify two FQ names were returned
    assert len(fq_names) == 2, "Should return exactly two FQ names"
    logger.info(f"Received FQ names: {fq_names}")
    
    # Verify the head jobs exist in the job map
    for fq_name in fq_names:
        assert fq_name in fm.job_graph_map, f"FQ name {fq_name} should be in job_map"
    
    # Test data for both graphs - numbers 1 through 5
    task_data = {
        "start": 1,
        "count": 5
    }
    
    # Find each graph's FQ name
    sum_fq_name = [name for name in fq_names if "sum" in name][0]
    multiply_fq_name = [name for name in fq_names if "multiply" in name][0]
    
    # Submit tasks to each graph
    logger.info(f"Submitting task to sum graph: {sum_fq_name}")
    fm.submit_task(Task(task_data), sum_fq_name)  # Must specify fq_name with multiple graphs
    
    logger.info(f"Submitting task to multiply graph: {multiply_fq_name}")
    fm.submit_task(Task(task_data), multiply_fq_name)  # Must specify fq_name with multiple graphs
    
    success = fm.wait_for_completion()
    assert success, "Timed out waiting for tasks to complete"
    
    # Check results
    results = fm.pop_results()
    logger.info(f"Results: {results}")
    assert not results["errors"], f"Errors occurred during task execution: {results['errors']}"
    assert len(results["completed"]) == 2, "Expected 2 completed tasks"
    
    # Verify sum graph results
    sum_result = results["completed"][sum_fq_name][0]
    logger.info(f"Sum graph result: {sum_result}")
    
    # For numbers 1,2,3,4,5, the sum operation gives [15], which should produce these statistics
    assert sum_result["sum"] == 15, f"Expected sum=15 for sum graph, got {sum_result['sum']}"
    assert sum_result["avg"] == 15.0, f"Expected avg=15.0 for sum graph, got {sum_result['avg']}"
    assert sum_result["count"] == 1, f"Expected count=1 for sum graph, got {sum_result['count']}"
    
    # Check saved results to verify operation and data flow
    if "SAVED_RESULTS" in sum_result:
        saved_results = sum_result.get("SAVED_RESULTS", {})
        logger.info(f"Sum graph saved results: {saved_results}")
        
        # Verify the operation performed
        op_key = [key for key in saved_results.keys() if "operation" in key][0]
        assert saved_results[op_key]["operation"] == "sum", \
            f"Expected operation='sum', got {saved_results[op_key]['operation']}"
        assert saved_results[op_key]["numbers"] == [15], \
            f"Expected [15], got {saved_results[op_key]['numbers']}"
    
    # Verify multiply graph results
    multiply_result = results["completed"][multiply_fq_name][0]
    logger.info(f"Multiply graph result: {multiply_result}")
    
    # For numbers 1,2,3,4,5, the multiply operation gives [120], which should produce these statistics
    assert multiply_result["sum"] == 120, f"Expected sum=120 for multiply graph, got {multiply_result['sum']}"
    assert multiply_result["avg"] == 120.0, f"Expected avg=120.0 for multiply graph, got {multiply_result['avg']}"
    assert multiply_result["count"] == 1, f"Expected count=1 for multiply graph, got {multiply_result['count']}"
    
    # Check saved results to verify operation and data flow
    if "SAVED_RESULTS" in multiply_result:
        saved_results = multiply_result.get("SAVED_RESULTS", {})
        logger.info(f"Multiply graph saved results: {saved_results}")
        
        # Verify the operation performed
        op_key = [key for key in saved_results.keys() if "operation" in key][0]
        assert saved_results[op_key]["operation"] == "multiply", \
            f"Expected operation='multiply', got {saved_results[op_key]['operation']}"
        assert saved_results[op_key]["numbers"] == [120], \
            f"Expected [120], got {saved_results[op_key]['numbers']}"


def test_add_dsl_dict_multiple_graphs_with_variants():
    """Test add_dsl_dict with multiple graphs with variants.
    
    This tests the DSL dictionary format with multiple graphs and variants:
    {
        "graph1": { "dev": dsl1d, "prod": dsl1p },
        "graph2": { "dev": dsl2d, "prod": dsl2p }
    }
    
    Uses different math operations for each graph-variant combination to demonstrate data flow.
    """
    # Create a helper function to build a DSL pipeline with specified operation
    def create_math_pipeline(prefix: str, operation: str):
        generator = NumberGenerator(f"{prefix}_generator")
        math_op = MathOperation(f"{prefix}_operation", operation=operation)
        aggregator = Aggregator(f"{prefix}_aggregator")
        
        jobs = wrap({
            "generator": generator,
            "operation": math_op,
            "aggregator": aggregator
        })
        
        # Save results for all jobs to verify later
        jobs["generator"].save_result = True
        jobs["operation"].save_result = True
        
        return jobs["generator"] >> jobs["operation"] >> jobs["aggregator"]
    
    # Create DSL dict with different operations for each graph and variant
    dsl_dict = {
        "first": {
            "dev": create_math_pipeline("first_dev", "square"),    # Square operation
            "prod": create_math_pipeline("first_prod", "double")   # Double operation
        },
        "second": {
            "dev": create_math_pipeline("second_dev", "sum"),      # Sum operation
            "prod": create_math_pipeline("second_prod", "multiply") # Multiply operation
        }
    }
    
    # Initialize FlowManager
    fm = FlowManager()
    
    # Add the DSL dict
    logger.info("Adding DSL dict with multiple graphs, with variants")
    fq_names = fm.add_dsl_dict(dsl_dict)
    
    # Verify four FQ names were returned (one for each graph-variant combination)
    assert len(fq_names) == 4, "Should return exactly four FQ names"
    logger.info(f"Received FQ names: {fq_names}")
    
    # Verify the head jobs exist in the job map
    for fq_name in fq_names:
        assert fq_name in fm.job_graph_map, f"FQ name {fq_name} should be in job_map"
    
    # Function to find a specific graph-variant combination from fq_names
    def find_fq_name(graph, variant):
        return [name for name in fq_names if graph in name and variant in name][0]
    
    # Define test data - numbers 1 through 5
    task_data = {
        "start": 1,
        "count": 5
    }
    
    # Test all four graph-variant combinations with the same input data
    combinations = [
        {"graph": "first", "variant": "dev", "operation": "square", "expected": [1, 4, 9, 16, 25]},
        {"graph": "first", "variant": "prod", "operation": "double", "expected": [2, 4, 6, 8, 10]},
        {"graph": "second", "variant": "dev", "operation": "sum", "expected": [15]},
        {"graph": "second", "variant": "prod", "operation": "multiply", "expected": [120]}
    ]
    
    for combo in combinations:
        # Find the correct FQ name
        fq_name = find_fq_name(combo["graph"], combo["variant"])
        logger.info(f"Testing {combo['graph']}-{combo['variant']} with {combo['operation']} operation: {fq_name}")
        
        # Submit the task
        fm.submit_task(Task(task_data), fq_name)
        
        # Wait for completion
        success = fm.wait_for_completion()
        assert success, f"Timed out waiting for {combo['graph']}-{combo['variant']} task to complete"
        
        # Check results
        results = fm.pop_results()
        assert not results["errors"], f"Errors occurred during {combo['graph']}-{combo['variant']} execution: {results['errors']}"
        assert len(results["completed"]) > 0, f"No completed tasks found for {combo['graph']}-{combo['variant']}"
        
        # Get the result
        result = results["completed"][fq_name][0]
        logger.info(f"{combo['graph']}-{combo['variant']} result: {result}")
        
        # Check saved results to verify operation and data flow
        if "SAVED_RESULTS" in result:
            saved_results = result.get("SAVED_RESULTS", {})
            logger.info(f"{combo['graph']}-{combo['variant']} saved results: {saved_results}")
            
            # Find the operation job results
            op_key = [key for key in saved_results.keys() if "operation" in key][0]
            
            # Verify the operation performed
            assert saved_results[op_key]["operation"] == combo["operation"], \
                f"Expected operation='{combo['operation']}', got {saved_results[op_key]['operation']}"
            
            # Verify the expected result of the operation
            assert saved_results[op_key]["numbers"] == combo["expected"], \
                f"Expected {combo['expected']}, got {saved_results[op_key]['numbers']}"
            
            # For aggregate operations (sum, multiply) that return a single value
            if combo["operation"] in ["sum", "multiply"]:
                # Check the aggregate value
                single_value = combo["expected"][0]
                assert result["sum"] == single_value, \
                    f"Expected sum={single_value}, got {result['sum']}"
                assert result["avg"] == single_value, \
                    f"Expected avg={single_value}, got {result['avg']}"
                assert result["count"] == 1, \
                    f"Expected count=1, got {result['count']}"
                
            # For square operation on numbers 1-5, sum should be 55, avg 11.0
            elif combo["operation"] == "square":
                assert result["sum"] == 55, f"Expected sum=55, got {result['sum']}"
                assert result["avg"] == 11.0, f"Expected avg=11.0, got {result['avg']}"
                
            # For double operation on numbers 1-5, sum should be 30, avg 6.0
            elif combo["operation"] == "double":
                assert result["sum"] == 30, f"Expected sum=30, got {result['sum']}"
                assert result["avg"] == 6.0, f"Expected avg=6.0, got {result['avg']}"
            
        # Verify generator inputs are consistent
        if "SAVED_RESULTS" in result:
            gen_key = [key for key in result["SAVED_RESULTS"].keys() if "generator" in key][0]
            assert result["SAVED_RESULTS"][gen_key]["numbers"] == [1, 2, 3, 4, 5], \
                f"Expected generator numbers [1,2,3,4,5], got {result['SAVED_RESULTS'][gen_key]['numbers']}"



================================================
FILE: tests/test_flowmanager_init_params.py
================================================
"""Tests for enhanced FlowManager API with convenience methods."""

from typing import Dict, List

import pytest

from flow4ai import f4a_logging as logging
from flow4ai.dsl import DSLComponent, JobsDict, wrap
from flow4ai.flowmanager import FlowManager
from flow4ai.job import (SPLIT_STR, JobABC,  # Import the separator constant
                         Task)

# Configure logging
logger = logging.getLogger(__name__)


class NumberGenerator(JobABC):
    """Job that generates a range of numbers based on start and count."""
    
    def __init__(self, name):
        super().__init__(name)
        
    async def run(self, task):
        start = task.get("start", 1)
        count = task.get("count", 5)
        numbers = list(range(start, start + count))
        logger.info(f"Generator {self.name} producing numbers: {numbers}")
        return {"numbers": numbers}


class MathOperation(JobABC):
    """Job that performs a math operation on a list of numbers."""
    
    def __init__(self, name, operation="square"):
        super().__init__(name)
        self._operation = operation
    
    @property
    def operation(self):
        """The operation to perform on the numbers."""
        return self._operation
        
    async def run(self, task):
        # Get inputs from predecessor jobs
        inputs = self.get_inputs()
        
        # Get numbers from producer
        numbers = inputs.get("generator", {}).get("numbers", [])
        logger.info(f"Operation {self.name} received numbers: {numbers}")
        
        # Apply the operation
        if self.operation == "square":
            result = [n * n for n in numbers]
        elif self.operation == "double":
            result = [n * 2 for n in numbers]
        elif self.operation == "sum":
            result = [sum(numbers)]
        elif self.operation == "multiply":
            product = 1
            for n in numbers:
                product *= n
            result = [product]
        else:
            result = numbers  # Default: identity operation
            
        logger.info(f"Operation {self.name} applied '{self.operation}', result: {result}")
        return {"numbers": result, "operation": self.operation}


class Aggregator(JobABC):
    """Job that aggregates numbers and computes statistics."""
    
    def __init__(self, name):
        super().__init__(name)
        
    async def run(self, task):
        # Get inputs from predecessor jobs
        inputs = self.get_inputs()
        
        # Get numbers from operation
        numbers = inputs.get("operation", {}).get("numbers", [])
        operation = inputs.get("operation", {}).get("operation", "unknown")
        
        # Compute statistics
        total = sum(numbers)
        avg = total / len(numbers) if numbers else 0
        
        logger.info(f"Aggregator {self.name} computed stats for operation '{operation}': sum={total}, avg={avg}")
        return {
            "numbers": numbers,
            "sum": total,
            "avg": avg,
            "operation": operation
        }


def create_math_pipeline(prefix: str, operation: str) -> DSLComponent:
    """Helper function to create a math pipeline with specified operation."""
    generator = NumberGenerator(f"{prefix}_generator")
    math_op = MathOperation(f"{prefix}_operation", operation=operation)
    aggregator = Aggregator(f"{prefix}_aggregator")
    
    jobs = wrap({
        "generator": generator,
        "operation": math_op,
        "aggregator": aggregator
    })
    
    # Save results for all jobs to verify later
    jobs["generator"].save_result = True
    jobs["operation"].save_result = True
    
    return jobs["generator"] >> jobs["operation"] >> jobs["aggregator"]


def test_flowmanager_constructor_with_dsl_dict():
    """Test initializing FlowManager with DSL dictionary in constructor."""
    # Create DSL dict with different operations for each graph and variant
    dsl_dict = {
        "first": {
            "dev": create_math_pipeline("first_dev", "square"),    # Square operation
            "prod": create_math_pipeline("first_prod", "double")   # Double operation
        },
        "second": {
            "dev": create_math_pipeline("second_dev", "sum"),      # Sum operation
            "prod": create_math_pipeline("second_prod", "multiply") # Multiply operation
        }
    }
    
    # Initialize FlowManager with DSL dict
    logger.info("Initializing FlowManager with DSL dict")
    fm = FlowManager(dsl=dsl_dict)
    
    # Verify all graphs are properly loaded
    # We should have 4 different FQ names in job_map
    assert len(fm.job_graph_map) == 4, f"Expected 4 graphs, got {len(fm.job_graph_map)}"
    
    # All expected head jobs should be present
    first_dev_jobs = fm.get_fq_names_by_graph("first", "dev")
    first_prod_jobs = fm.get_fq_names_by_graph("first", "prod")
    second_dev_jobs = fm.get_fq_names_by_graph("second", "dev")
    second_prod_jobs = fm.get_fq_names_by_graph("second", "prod")
    
    assert len(first_dev_jobs) == 1, f"Expected 1 first-dev job, got {len(first_dev_jobs)}"
    assert len(first_prod_jobs) == 1, f"Expected 1 first-prod job, got {len(first_prod_jobs)}"
    assert len(second_dev_jobs) == 1, f"Expected 1 second-dev job, got {len(second_dev_jobs)}"
    assert len(second_prod_jobs) == 1, f"Expected 1 second-prod job, got {len(second_prod_jobs)}"
    
    # Ensure the jobs have the expected FQ name structure
    for fq_name in fm.job_graph_map.keys():
        logger.info(f"Found FQ name: {fq_name}")
        parts = fq_name.split(SPLIT_STR)
        assert len(parts) >= 3, f"FQ name should have at least 3 parts: {fq_name}"
        
        graph_name = parts[0]
        variant = parts[1]
        
        assert graph_name in ["first", "second"], f"Unexpected graph name: {graph_name}"
        assert variant in ["dev", "prod"] or variant.startswith(("dev_", "prod_")), \
            f"Unexpected variant: {variant}"


def test_get_fq_names_by_graph():
    """Test getting FQ names by graph name and variant."""
    # Create a FlowManager
    fm = FlowManager()
    
    # Add DSL dictionaries for testing
    dsl_dict = {
        "math": {
            "basic": create_math_pipeline("math_basic", "square"),
            "advanced": create_math_pipeline("math_advanced", "double")
        }
    }
    
    # Add the DSL dict
    fm.add_dsl_dict(dsl_dict)
    
    # Test getting FQ names by graph and variant
    basic_fq_names = fm.get_fq_names_by_graph("math", "basic")
    advanced_fq_names = fm.get_fq_names_by_graph("math", "advanced")
    
    assert len(basic_fq_names) == 1, f"Expected 1 basic FQ name, got {len(basic_fq_names)}"
    assert len(advanced_fq_names) == 1, f"Expected 1 advanced FQ name, got {len(advanced_fq_names)}"
    
    # Test with non-existent graph
    non_existent = fm.get_fq_names_by_graph("nonexistent", "variant")
    assert len(non_existent) == 0, f"Expected 0 matches for non-existent graph, got {len(non_existent)}"
    
    # Add another DSL with same structure (should get different FQ name with suffix)
    dsl_basic2 = create_math_pipeline("math_basic2", "sum")
    fm.add_dsl(dsl_basic2, "math", "basic")
    
    # Now we should have two "math-basic" FQ names
    basic_fq_names_updated = fm.get_fq_names_by_graph("math", "basic")
    assert len(basic_fq_names_updated) == 2, \
        f"Expected 2 basic FQ names after adding same structure, got {len(basic_fq_names_updated)}"


def test_submit_by_graph():
    """Test submitting tasks using the graph name and variant."""
    # Create DSL dict with different operations
    dsl_dict = {
        "calculator": {
            "square": create_math_pipeline("calc_square", "square"),
            "double": create_math_pipeline("calc_double", "double")
        }
    }
    
    # Initialize FlowManager with DSL dict
    fm = FlowManager(dsl=dsl_dict)
    
    # Create test task - numbers 1 through 5
    task_data = {
        "start": 1,
        "count": 5
    }
    
    # Test submit_by_graph with square variant
    logger.info("Submitting task to calculator-square graph")
    fm.submit_by_graph(Task(task_data), "calculator", "square")
    
    # Wait for completion
    success = fm.wait_for_completion()
    assert success, "Timed out waiting for calculator-square task to complete"
    
    # Check results
    results = fm.pop_results()
    assert not results["errors"], f"Errors occurred during execution: {results['errors']}"
    assert len(results["completed"]) > 0, "No completed tasks found"
    
    # Get the result - should have one key matching calculator-square FQ name
    square_fq_name = fm.get_fq_names_by_graph("calculator", "square")[0]
    result = results["completed"][square_fq_name][0]
    logger.info(f"calculator-square result: {result}")
    
    # Verify the result
    assert result["operation"] == "square", f"Expected operation='square', got {result['operation']}"
    assert result["sum"] == 55, f"Expected sum=55, got {result['sum']}"
    assert result["avg"] == 11.0, f"Expected avg=11.0, got {result['avg']}"
    
    # Check saved results to verify operation
    if "SAVED_RESULTS" in result:
        saved_results = result.get("SAVED_RESULTS", {})
        
        # Find the operation job results
        op_key = [key for key in saved_results.keys() if "operation" in key][0]
        
        # Verify the operation performed
        assert saved_results[op_key]["operation"] == "square", \
            f"Expected operation='square', got {saved_results[op_key]['operation']}"
        
        # Verify the expected result of the operation
        assert saved_results[op_key]["numbers"] == [1, 4, 9, 16, 25], \
            f"Expected [1, 4, 9, 16, 25], got {saved_results[op_key]['numbers']}"


def test_submit_by_graph_with_collision():
    """Test submitting when multiple graphs with same name/variant exist."""
    # Create a FlowManager
    fm = FlowManager()
    
    # Create and add two DSLs with same graph name and variant
    dsl1 = create_math_pipeline("collision_test1", "square")
    dsl2 = create_math_pipeline("collision_test2", "double")
    
    # Add both with same graph name and variant
    fq_name1 = fm.add_dsl(dsl1, "calc", "test")
    fq_name2 = fm.add_dsl(dsl2, "calc", "test")
    
    # Verify we got different FQ names
    assert fq_name1 != fq_name2, "Expected different FQ names for different DSLs"
    
    # Verify we can get both by graph name
    calc_test_fq_names = fm.get_fq_names_by_graph("calc", "test")
    assert len(calc_test_fq_names) == 2, f"Expected 2 FQ names, got {len(calc_test_fq_names)}"
    
    # Test that submit_by_graph raises an error for ambiguous graph/variant
    with pytest.raises(ValueError) as excinfo:
        fm.submit_by_graph(Task({"start": 1, "count": 3}), "calc", "test")
    
    # Check that error message contains the FQ names
    error_message = str(excinfo.value)
    assert "Multiple matching graphs found" in error_message
    assert fq_name1 in error_message
    assert fq_name2 in error_message


def test_combined_workflow():
    """Test a complete workflow using the enhanced FlowManager API."""
    # Create DSL dict
    dsl_dict = {
        "workflow": {
            "simple": create_math_pipeline("workflow_simple", "double")
        }
    }
    
    # Initialize FlowManager with DSL dict
    fm = FlowManager(dsl=dsl_dict)
    
    # Get FQ name for verification
    simple_fq_names = fm.get_fq_names_by_graph("workflow", "simple")
    assert len(simple_fq_names) == 1, "Expected one FQ name for workflow-simple"
    simple_fq_name = simple_fq_names[0]
    
    # Task data
    task_data = {"start": 2, "count": 3}  # Will generate [2, 3, 4]
    
    # Submit task using graph and variant
    fm.submit_by_graph(Task(task_data), "workflow", "simple")
    
    # Wait for completion
    success = fm.wait_for_completion()
    assert success, "Task did not complete successfully"
    
    # Get results
    results = fm.pop_results()
    assert not results["errors"], f"Errors in execution: {results['errors']}"
    assert simple_fq_name in results["completed"], f"No results for FQ name {simple_fq_name}"
    
    # Verify results
    result = results["completed"][simple_fq_name][0]
    
    # For double operation on [2, 3, 4], result should be [4, 6, 8]
    # Sum should be 18, avg 6.0
    assert result["sum"] == 18, f"Expected sum=18, got {result['sum']}"
    assert result["avg"] == 6.0, f"Expected avg=6.0, got {result['avg']}"
    
    # Verify operation and generator data
    if "SAVED_RESULTS" in result:
        saved_results = result.get("SAVED_RESULTS", {})
        
        # Check generator output
        gen_key = [key for key in saved_results.keys() if "generator" in key][0]
        assert saved_results[gen_key]["numbers"] == [2, 3, 4], \
            f"Expected generator numbers [2, 3, 4], got {saved_results[gen_key]['numbers']}"
        
        # Check operation output
        op_key = [key for key in saved_results.keys() if "operation" in key][0]
        assert saved_results[op_key]["operation"] == "double", \
            f"Expected operation='double', got {saved_results[op_key]['operation']}"
        assert saved_results[op_key]["numbers"] == [4, 6, 8], \
            f"Expected [4, 6, 8], got {saved_results[op_key]['numbers']}"

def test_completion_callback():
    once = lambda x: x + "upon a time"
    ina = lambda x: x + "galaxy far far away"

    async def collate(j_ctx):
        await asyncio.sleep(2)
        inputs = j_ctx["inputs"]
        output = f"{inputs['once']['result']} {inputs['ina']['result']}"
        return output

    def post_processor(result):
        assert result["result"] == "once upon a time in a galaxy far far away"
            
    jobs = wrap({
        "once": once,
        "ina": ina,
        "collate": collate
    })

    dsl =(jobs["once"] | jobs["ina"] ) >> jobs["collate"]
        
    fm = FlowManager(on_complete=post_processor)
    fq_name =fm.add_dsl(dsl, "test_completion_callback")
    print(fq_name)
    task = {"once.x": "once ", "ina.x": "in a "}
    fm.submit_task(task,fq_name)
    fm.wait_for_completion()


================================================
FILE: tests/test_fmmp_async_functionality.py
================================================
"""
    Simple Tests to confirm the basics of async functionality are working:
    
            - Tests concurrent task execution and verification
            - Tests async task cancellation and cleanup
            - Tests event loop handling and lifecycle
            - Tests async exception propagation
            - Tests parallel task limits and scaling  
"""

import asyncio

import pytest

from flow4ai.flowmanagerMP import FlowManagerMP
from flow4ai.job import JobABC


class AsyncTestJob(JobABC):
    """Job to confirm the basics of async functionality are working: """
    def __init__(self):
        super().__init__(name="AsyncTestJob")
    
    async def run(self, task):
        if isinstance(task, dict) and task.get('fail'):
            raise ValueError("Simulated task failure")
        if isinstance(task, dict) and task.get('delay'):
            await asyncio.sleep(task['delay'])
        return {'task': task, 'completed': True}


@pytest.mark.asyncio
async def test_concurrent_task_execution():
    """Test that tasks are truly executed concurrently"""
    results = []
    
    def collect_result(result):
        results.append(result)
    
    flowmanagerMP = FlowManagerMP(AsyncTestJob(), collect_result, serial_processing=True)
    
    # Get the head job's name to use in task submissions
    head_jobs = flowmanagerMP.get_head_jobs()
    assert len(head_jobs) > 0, "No head jobs found in the flow manager"
    job_name = head_jobs[0].name
    
    # Submit tasks with different delays using the head job's name
    tasks = [
        {'task_id': 1, 'delay': 0.2},
        {'task_id': 2, 'delay': 0.1},
        {'task_id': 3, 'delay': 0.3}
    ]
    
    for task in tasks:
        flowmanagerMP.submit_task(task)
    
    flowmanagerMP.wait_for_completion()
    
    # Verify all tasks completed
    assert len(results) == 3
    # Verify task completion order (should still be in delay order due to async execution)
    task_ids = [r['task']['task_id'] for r in results]
    assert 2 in task_ids  # Task 2 should be completed (not necessarily first in serial mode)
    assert 1 in task_ids
    assert 3 in task_ids

@pytest.mark.asyncio
async def test_async_task_cancellation():
    """Test proper cleanup of cancelled async tasks"""
    flowmanagerMP = FlowManagerMP(AsyncTestJob())
    
    # Submit a long-running task
    flowmanagerMP.submit_task({'delay': 1.0})
    
    # Force cleanup before completion
    flowmanagerMP._cleanup()
    
    # Verify process termination
    assert not flowmanagerMP.job_executor_process.is_alive()
    assert flowmanagerMP._task_queue._closed

@pytest.mark.asyncio
async def test_event_loop_handling():
    """Test proper event loop creation and cleanup"""
    flowmanagerMP = FlowManagerMP(AsyncTestJob())
    
    # Submit a simple task
    flowmanagerMP.submit_task({'task_id': 1})
    flowmanagerMP.wait_for_completion()
    
    # Verify process cleanup
    assert not flowmanagerMP.job_executor_process.is_alive()

@pytest.mark.asyncio
async def test_async_exception_handling():
    """Test that async exceptions are properly caught and handled"""
    results = []
    
    def collect_result(result):
        results.append(result)
    
    flowmanagerMP = FlowManagerMP(AsyncTestJob(), collect_result, serial_processing=True)
    
    # Get the head job's name to use in task submissions
    head_jobs = flowmanagerMP.get_head_jobs()
    assert len(head_jobs) > 0, "No head jobs found in the flow manager"
    job_name = head_jobs[0].name
    
    # Submit mix of successful and failing tasks
    tasks = [
        {'task_id': 1},
        {'task_id': 2, 'fail': True},
        {'task_id': 3}
    ]
    
    for task in tasks:
        flowmanagerMP.submit_task(task)
    
    flowmanagerMP.wait_for_completion()
    
    # Only successful tasks should have results
    assert len(results) == 2
    assert all(r['completed'] for r in results)

@pytest.mark.asyncio
async def test_parallel_task_limit():
    """Test handling of many concurrent tasks"""
    results = []
    
    def collect_result(result):
        results.append(result)
    
    flowmanagerMP = FlowManagerMP(AsyncTestJob(), collect_result, serial_processing=True)
    
    # Get the head job's name to use in task submissions
    head_jobs = flowmanagerMP.get_head_jobs()
    assert len(head_jobs) > 0, "No head jobs found in the flow manager"
    job_name = head_jobs[0].name
    
    # Submit many quick tasks
    num_tasks = 100
    for i in range(num_tasks):
        flowmanagerMP.submit_task({'task_id': i, 'delay': 0.01})
    
    flowmanagerMP.wait_for_completion()
    
    # All tasks should complete
    assert len(results) == num_tasks
    # Results should maintain task integrity
    task_ids = {r['task']['task_id'] for r in results}
    assert len(task_ids) == num_tasks

if __name__ == '__main__':
    pytest.main(['-v', 'test_async_functionality.py'])



================================================
FILE: tests/test_fmmp_factory.py
================================================
"""
Tests for FlowManagerMPFactory class functionality.

This module contains tests that verify the FlowManagerMPFactory wrapper over FlowManagerMP,
focusing on key functionality from other test modules.
"""

import asyncio
import os
import time
from typing import List

import pytest

from flow4ai import f4a_logging as logging
from flow4ai.flowmanagerMP import FlowManagerMPFactory
from flow4ai.job import JobABC
from flow4ai.job_loader import ConfigLoader

# Global results list for picklable result processing
RESULTS: List[dict] = []


def picklable_result_processor(result):
    """A picklable result processor function"""
    RESULTS.append(result)
    logging.info(f"Processing result: {result}")


class AsyncTestJob(JobABC):
    """Job to test async functionality with FlowManagerMPFactory"""
    def __init__(self):
        super().__init__(name="AsyncTestJob")
    
    async def run(self, task):
        if isinstance(task, dict) and task.get('delay'):
            await asyncio.sleep(task['delay'])
        return {'task': task, 'completed': True}


class BasicTestJob(JobABC):
    """Simple job for testing basic functionality"""
    def __init__(self, name="BasicTestJob"):
        super().__init__(name=name)
    
    async def run(self, task):
        return {self.name: "completed"}


class DelayedJob(JobABC):
    """Job that introduces a configurable delay"""
    def __init__(self, delay: float):
        super().__init__(name="DelayedJob")
        self.delay = delay
    
    async def run(self, task):
        logging.info(f"Executing DelayedJob for {task} with delay {self.delay}")
        await asyncio.sleep(self.delay)
        return {"task": task, "status": "complete"}


class ResultTimingJob(JobABC):
    """Job that records execution timing"""
    def __init__(self):
        super().__init__("ResultTimingJob")
        self.executed_tasks = set()
    
    async def run(self, task):
        # Extract the actual task
        actual_task = task.get(self.name, task)
        # Record task execution
        task_str = str(actual_task)
        self.executed_tasks.add(task_str)
        # Simulate work
        await asyncio.sleep(0.1)
        current_time = time.time()
        logging.info(f"Executing task {actual_task} at {current_time}")
        return {"task": actual_task, "timestamp": current_time}


class UnpicklableState:
    """Class with unpicklable state (file handle)"""
    def __init__(self):
        self.log_file = open('temp.log', 'w')
    
    def __del__(self):
        try:
            self.log_file.close()
            if os.path.exists('temp.log'):
                os.remove('temp.log')
        except:
            pass


@pytest.fixture(autouse=True)
def reset_factory():
    """Reset FlowManagerMPFactory between tests"""
    FlowManagerMPFactory._instance = None
    FlowManagerMPFactory._flowmanagerMP = None
    RESULTS.clear()  # Clear global results
    # Clean up temp file if exists
    if os.path.exists('temp.log'):
        os.remove('temp.log')
    yield
    # Clean up after test
    if os.path.exists('temp.log'):
        os.remove('temp.log')


def test_empty_initialization():
    """Test that FlowManagerMPFactory is initialized correctly"""
    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config")])
    
    # Create FlowManagerMP with serial processing to ensure deterministic results
    FlowManagerMPFactory()
    
    # Get head jobs from config to know their names
    head_jobs = sorted(FlowManagerMPFactory.get_instance().get_fq_names())
    
    # Verify head jobs are loaded - these are the entry point jobs from all graphs and parameter sets
    expected_jobs = sorted([
        'four_stage_parameterized$$params1$$read_file$$',
        'four_stage_parameterized$$params2$$read_file$$',
        'three_stage$$params1$$ask_llm_mini$$',
        'three_stage_reasoning$$$$ask_llm_reasoning$$'
    ])
    
    assert head_jobs == expected_jobs, "FlowManagerMP config not loaded correctly"

    FlowManagerMPFactory.get_instance().wait_for_completion()


@pytest.mark.asyncio
async def test_concurrent_task_execution():
    """Test that tasks are truly executed concurrently using FlowManagerMPFactory"""
    results = []
    
    def collect_result(result):
        results.append(result)
    
    # Initialize factory with a new FlowManagerMP
    FlowManagerMPFactory(AsyncTestJob(), collect_result, serial_processing=True)
    flowmanagerMP = FlowManagerMPFactory.get_instance()
    
    # Get the head job's name to use in task submissions
    head_jobs = flowmanagerMP.get_head_jobs()
    assert len(head_jobs) > 0, "No head jobs found in the flow manager"
    job_name = head_jobs[0].name
    
    # Submit tasks with different delays
    tasks = [
        {'task_id': 1, 'delay': 0.2},
        {'task_id': 2, 'delay': 0.1},
        {'task_id': 3, 'delay': 0.3}
    ]
    
    for task in tasks:
        flowmanagerMP.submit_task(task)
    
    flowmanagerMP.wait_for_completion()
    
    # Verify all tasks completed
    assert len(results) == 3
    
    # Verify task completion order
    task_ids = [r['task']['task_id'] for r in results]
    assert 2 in task_ids  # Task 2 should be completed
    assert 1 in task_ids
    assert 3 in task_ids


@pytest.mark.asyncio
async def test_job_instantiation_and_execution():
    """Test basic job creation and execution using FlowManagerMPFactory"""
    results = []
    
    def collect_result(result):
        results.append(result)
    
    # Create a FlowManagerMP through the factory
    FlowManagerMPFactory(BasicTestJob(), collect_result, serial_processing=True)
    flowmanagerMP = FlowManagerMPFactory.get_instance()
    
    # Get the head job's name to use in task submissions
    head_jobs = flowmanagerMP.get_head_jobs()
    assert len(head_jobs) > 0, "No head jobs found in the flow manager"
    job_name = head_jobs[0].name
    
    # Submit a simple task
    flowmanagerMP.submit_task({})
    flowmanagerMP.wait_for_completion()
    
    # Verify job execution
    assert len(results) == 1
    assert results[0][job_name] == "completed"
    
    # Verify we can get the same instance again
    same_flowmanagerMP = FlowManagerMPFactory.get_instance()
    assert same_flowmanagerMP is flowmanagerMP


def test_parallel_execution():
    """Test true parallel execution performance using FlowManagerMPFactory"""
    async def run_flowmanagerMP(delay: float) -> float:
        """Run FlowManagerMP with specified delay and return execution time"""
        start_time = time.perf_counter()
        
        # Create FlowManagerMP through factory with picklable result processor
        FlowManagerMPFactory(DelayedJob(delay), picklable_result_processor)
        flowmanagerMP = FlowManagerMPFactory.get_instance()
        
        # Feed 10 tasks with a delay between each to simulate data gathering
        for i in range(10):
            flowmanagerMP.submit_task(f"Task {i}")
            await asyncio.sleep(0.2)  # Simulate time taken to gather data
            
        flowmanagerMP.wait_for_completion()
        
        execution_time = time.perf_counter() - start_time
        logging.info(f"Execution time for delay {delay}s: {execution_time:.2f}s")
        return execution_time
    
    # Test with 1 second delay
    time_1s = asyncio.run(run_flowmanagerMP(1.0))
    
    # Reset factory for second test
    FlowManagerMPFactory._instance = None
    FlowManagerMPFactory._flowmanagerMP = None
    RESULTS.clear()
    
    # Test with 2 second delay
    time_2s = asyncio.run(run_flowmanagerMP(2.0))
    
    # Calculate the ratio of execution times
    time_ratio = time_2s / time_1s
    logging.info(f"Time with 1s delay: {time_1s:.2f}s")
    logging.info(f"Time with 2s delay: {time_2s:.2f}s")
    logging.info(f"Ratio: {time_ratio:.2f}x")
    
    # Verify parallel execution
    assert time_1s <= 4.1, (
        f"Expected tasks to complete in ~4.1s (including data gathering + overhead), took {time_1s:.2f}s. "
        "This suggests tasks are running sequentially"
    )
    
    assert time_2s <= 5.1, (
        f"Expected tasks to complete in ~5.1s (including data gathering + overhead), took {time_2s:.2f}s. "
        "This suggests tasks are running sequentially"
    )
    
    assert time_ratio <= 1.5, (
        f"Expected time ratio <= 1.5, got {time_ratio:.2f}. "
        "This suggests tasks are running sequentially instead of in parallel"
    )


def test_serial_result_processor_with_unpicklable():
    """Test serial processing with unpicklable state using FlowManagerMPFactory"""
    # Create unpicklable state
    unpicklable = UnpicklableState()
    
    def unpicklable_processor(result):
        """A result processor that uses unpicklable state"""
        unpicklable.log_file.write(f"Processing: {result}\n")
        unpicklable.log_file.flush()
        logging.info(f"Logged result: {result}")
    
    # Test parallel mode (should fail)
    with pytest.raises(TypeError) as exc_info:
        FlowManagerMPFactory(ResultTimingJob(), unpicklable_processor, serial_processing=False)
        flowmanagerMP = FlowManagerMPFactory.get_instance()
        flowmanagerMP.submit_task("Task 1")
        flowmanagerMP.wait_for_completion()
    assert "pickle" in str(exc_info.value).lower()
    
    # Reset factory for serial mode test
    FlowManagerMPFactory._instance = None
    FlowManagerMPFactory._flowmanagerMP = None
    
    # Test serial mode (should work)
    FlowManagerMPFactory(ResultTimingJob(), unpicklable_processor, serial_processing=True)
    flowmanagerMP = FlowManagerMPFactory.get_instance()
    
    # Submit tasks
    for i in range(3):
        flowmanagerMP.submit_task({"ResultTimingJob": f"Task {i}"})
        time.sleep(0.1)
    
    flowmanagerMP.wait_for_completion()
    
    # Verify results were logged
    with open('temp.log', 'r') as f:
        log_contents = f.read()
    
    # Check that all tasks were processed
    assert "Task 0" in log_contents
    assert "Task 1" in log_contents
    assert "Task 2" in log_contents



================================================
FILE: tests/test_fmmp_parallel_execution.py
================================================
"""
    Tests parallel workloads with some meaningful load in parallel execution mode:
        - test_parallel_execution: makes sure long running tasks are executed in 
            parallel with short running tasks, and that task results are processed in 
            parallel.
        - run_batch_flowmanagerMP: simulates running a Job on several batches of tasks.
        - test_parallel_execution_in_batches: runs batches in parallel
"""
import asyncio
import json
import os
import time

import yaml

from flow4ai import f4a_logging as logging
from flow4ai.flowmanagerMP import FlowManagerMP
from flow4ai.job import JobABC
from flow4ai.utils.otel_wrapper import TracerFactory
from tests.test_utils.simple_job import SimpleJobFactory

# Configure logging
logging.basicConfig(level=logging.INFO,
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

class DelayedJob(JobABC):
    def __init__(self, name: str, time_delay: float):
        super().__init__(name)
        self.time_delay = time_delay

    async def run(self, task) -> dict:
        """Execute a delayed job with tracing."""
        logger.info(f"Executing DelayedJob for {task} with delay {self.time_delay}")
        await asyncio.sleep(self.time_delay)  # Use specified delay
        return {"task": task, "status": "complete"}

def create_delayed_job(params: dict) -> JobABC:
    time_delay = params.get('time_delay', 1.0)
    return DelayedJob("Test Job", time_delay)

# Store original load_from_file function
original_load_from_file = SimpleJobFactory._load_from_file

def setup_module(module):
    """Set up test environment"""
    SimpleJobFactory._load_from_file = create_delayed_job

def teardown_module(module):
    """Restore original implementation"""
    SimpleJobFactory._load_from_file = original_load_from_file

def dummy_result_processor(result):
    """Dummy function for processing results in tests"""
    logger.info(f"Processing result: {result}")

async def run_flowmanagerMP(time_delay: float, use_direct_job: bool = False) -> float:
    """Run FlowManagerMP with specified delay and return execution time"""
    start_time = time.perf_counter()
    
    if use_direct_job:
        # Create and pass Job instance directly
        job = DelayedJob("Test Job", time_delay)
        flowmanagerMP = FlowManagerMP(job, dummy_result_processor)
    else:
        # Use traditional dictionary initialization
        flowmanagerMP_context = {
                "type": "file",
                "params": {"time_delay": time_delay}
            }
        loaded_job = SimpleJobFactory.load_job(flowmanagerMP_context)
        flowmanagerMP = FlowManagerMP(loaded_job, dummy_result_processor)

    # Feed 10 tasks with a delay between each to simulate data gathering
    for i in range(10):
        flowmanagerMP.submit_task(f"Task {i}")
        await asyncio.sleep(0.2)  # Simulate time taken to gather data
    # Indicate there is no more input data to process to initiate shutdown
    flowmanagerMP.wait_for_completion()

    execution_time = time.perf_counter() - start_time
    logger.info(f"Execution time for delay {time_delay}s: {execution_time:.2f}s")
    return execution_time

def test_parallel_execution():
    # Test with 1 second delay
    time_1s = asyncio.run(run_flowmanagerMP(1.0))
    
    # Test with 2 second delay
    time_2s = asyncio.run(run_flowmanagerMP(2.0))
    
    # Calculate the ratio of execution times
    time_ratio = time_2s / time_1s
    logger.info(f"\nTime with 1s delay: {time_1s:.2f}s")
    logger.info(f"Time with 2s delay: {time_2s:.2f}s")
    logger.info(f"Ratio: {time_ratio:.2f}x")
    
    assert time_1s <= 4.1, (
        f"Expected tasks to complete in ~4.1s (including data gathering + overhead), took {time_1s:.2f}s. "
        "This suggests tasks are running sequentially"
    )
    
    assert time_2s <= 5.1, (
        f"Expected tasks to complete in ~5.1s (including data gathering + overhead), took {time_2s:.2f}s. "
        "This suggests tasks are running sequentially"
    )
    
    assert time_ratio <= 1.5, (
        f"Expected time ratio <= 1.5, got {time_ratio:.2f}. "
        "This suggests tasks are running sequentially instead of in parallel"
    )

def test_direct_job_initialization():
    """Test that direct Job instance initialization works equivalently"""
    # Run with dictionary initialization
    time_dict = asyncio.run(run_flowmanagerMP(1.0, use_direct_job=False))
    
    # Run with direct Job instance
    time_direct = asyncio.run(run_flowmanagerMP(1.0, use_direct_job=True))
    
    # Calculate the ratio of execution times
    time_ratio = abs(time_direct - time_dict) / time_dict
    logger.info(f"\nTime with dict initialization: {time_dict:.2f}s")
    logger.info(f"Time with direct Job instance: {time_direct:.2f}s")
    logger.info(f"Difference ratio: {time_ratio:.2f}")
    
    # The execution times should be very similar (within 10% of each other)
    assert time_ratio <= 0.1, (
        f"Expected similar execution times, but difference ratio was {time_ratio:.2f}. "
        "This suggests the two initialization methods are not equivalent"
    )

async def run_batch_flowmanagerMP() -> float:
    """Run FlowManagerMP with batches of website analysis jobs"""
    start_time = time.perf_counter()
    
    flowmanagerMP_context = {
        "type": "file",
        "params": {"time_delay": 0.70}
    }
    loaded_job = SimpleJobFactory.load_job(flowmanagerMP_context)
    flowmanagerMP = FlowManagerMP(loaded_job, dummy_result_processor)

    # Process 4 batches of 25 links each
    for batch in range(4):
        # Simulate scraping 25 links, 1 second per link
        for link in range(25):
            flowmanagerMP.submit_task(f"Batch{batch}_Link{link}")
            await asyncio.sleep(0.10)  # Simulate time to scrape each link
    # Indicate there is no more input data to process to initiate shutdown
    flowmanagerMP.wait_for_completion()

    execution_time = time.perf_counter() - start_time
    logger.info(f"\nTotal execution time: {execution_time:.2f}s")
    return execution_time

def test_parallel_execution_in_batches():
    """Test parallel execution of website analysis in batches while scraping continues"""
    execution_time = asyncio.run(run_batch_flowmanagerMP())
    
    assert execution_time <= 12.4, (
        f"Expected execution to complete in ~12.4s, took {execution_time:.2f}s. "
        "This suggests analysis jobs are not running in parallel with scraping"
    )
    
    assert execution_time >= 9.5, (
        f"Execution completed too quickly in {execution_time:.2f}s. "
        "Expected ~10s for scraping all links"
    )

async def run_flowmanagerMP_without_result_processor() -> bool:
    """Run FlowManagerMP without a result processing function"""
    try:
        job = DelayedJob("Test Job",  0.1)
        flowmanagerMP = FlowManagerMP(job)  # Pass no result_processing_function

        # Submit a few tasks
        for i in range(3):
            flowmanagerMP.submit_task(f"Task {i}")
        # Indicate there is no more input data to process to initiate shutdown
        flowmanagerMP.wait_for_completion()
        return True
    except Exception as e:
        logger.error(f"Error occurred: {e}")
        return False

def test_no_result_processor():
    """Test that FlowManagerMP works without setting result_processing_function"""
    success = asyncio.run(run_flowmanagerMP_without_result_processor())
    assert success, "FlowManagerMP should execute successfully without result_processing_function"

async def run_traced_flowmanagerMP(time_delay: float) -> float:
    """Run FlowManagerMP with specified delay and return execution time"""
    start_time = time.perf_counter()
    
    # Use traditional dictionary initialization
    flowmanagerMP_context = {
        "type": "file",
        "params": {"time_delay": time_delay}
    }
    # Load job from context
    loaded_job = SimpleJobFactory.load_job(flowmanagerMP_context)
    flowmanagerMP = FlowManagerMP(loaded_job, dummy_result_processor)

    # Feed 10 tasks with a delay between each to simulate data gathering
    for i in range(10):
        flowmanagerMP.submit_task(f"Task {i}")
        await asyncio.sleep(0.2)  # Simulate time taken to gather data
    # Indicate there is no more input data to process to initiate shutdown
    flowmanagerMP.wait_for_completion()

    execution_time = time.perf_counter() - start_time
    logger.info(f"Execution time for delay {time_delay}s: {execution_time:.2f}s")
    return execution_time

def test_parallel_execution_with_tracing(tmp_path):
    """Test parallel execution with OpenTelemetry tracing enabled"""
    logger.info("Starting parallel execution with tracing test")
    
    # Set up temporary trace file
    trace_file = str(tmp_path / "temp_otel_trace.json")
    logger.info(f"Setting up trace file at: {trace_file}")
    with open(trace_file, 'w') as f:
        json.dump([], f)

    # Set up temporary config file
    config_path = str(tmp_path / "otel_config.yaml")
    config = {
        "exporter": "file",
        "service_name": "FlowManagerMPTest",
        "batch_processor": {
            "max_queue_size": 1000,
            "schedule_delay_millis": 1000
        },
        "file_exporter": {
            "path": trace_file
        }
    }
    
    logger.info(f"Writing config to: {config_path}")
    logger.info(f"Config content: {config}")
    with open(config_path, 'w') as f:
        yaml.dump(config, f)

    try:
        # Set environment variable before creating processes
        os.environ['FLOW4AI_OT_CONFIG'] = config_path
        logger.info(f"Set FLOW4AI_OT_CONFIG to: {os.environ.get('FLOW4AI_OT_CONFIG')}")

        # Reset TracerFactory state
        TracerFactory._instance = None
        TracerFactory._config = None

        # Run the FlowManagerMP
        logger.info("Starting FlowManagerMP execution")
        execution_time = asyncio.run(run_traced_flowmanagerMP(1.0))
        logger.info("FlowManagerMP execution completed")

        # Verify execution time
        assert execution_time <= 4.2, (
            f"Expected tasks to complete in ~4.2s (including data gathering + overhead), took {execution_time:.2f}s. "
            "This suggests tasks are running sequentially"
        )

        # Wait for traces to be exported
        logger.info("Waiting for traces to be exported...")
        time.sleep(2.0)

        # Verify traces
        logger.info(f"Reading trace file: {trace_file}")
        with open(trace_file, 'r') as f:
            trace_data = json.load(f)
        logger.info(f"Raw trace data: {json.dumps(trace_data, indent=2)}")
            
        # Count execute method traces
        execute_traces = [
            span for span in trace_data 
            if span['name'].endswith('._execute')
        ]
        logger.info(f"Found {len(execute_traces)} execute traces")
        if len(execute_traces) > 0:
            logger.info("Sample trace names:")
            for trace in execute_traces[:3]:  # Show first 3 traces as sample
                logger.info(f"  - {trace['name']}")
        
        assert len(execute_traces) == 10, (
            f"Expected 10 execute traces (one per task), but found {len(execute_traces)}. "
            "This suggests not all tasks were traced properly."
        )

    finally:
        # Cleanup
        logger.info("Cleaning up test resources")
        if os.path.exists(trace_file):
            os.unlink(trace_file)
        if os.path.exists(config_path):
            os.unlink(config_path)
        if 'FLOW4AI_OT_CONFIG' in os.environ:
            del os.environ['FLOW4AI_OT_CONFIG']
        TracerFactory._instance = None
        TracerFactory._config = None
class DelayedMockJob(JobABC):
    """A mock job that introduces a configurable delay in processing."""
    def __init__(self, name: str, delay: float):
        super().__init__(name=name)
        self.delay = delay

    async def run(self, task):
        await asyncio.sleep(self.delay)
        return {'input': task, 'output': f'processed by {self.name}'}

def test_parallel_execution_multiple_jobs():
    """Test parallel execution with multiple jobs in a job graph."""
    # Test with both 1s and 2s delays
    for delay in [1.0, 2.0]:
        # Create 5 jobs with the same delay
        jobs = [DelayedMockJob(f'job_{i}', delay) for i in range(5)]
        
        # Time the execution
        start_time = time.time()
        
        # Run the FlowManagerMP with all jobs and collect results
        results = []
        def result_collector(result):
            results.append(result)
        flowmanagerMP = FlowManagerMP(jobs, result_collector, serial_processing=True)
        
        # Get the actual fq_names from the head_jobs
        head_jobs = flowmanagerMP.get_head_jobs()
        assert len(head_jobs) == 5, "Expected 5 head jobs"
        fq_names = [job.name for job in head_jobs]
        
        # Create tasks for each job using the fq_name of each job
        tasks = []
        for i in range(5):
            fq_name = fq_names[i]
            for j in range(4):  # 4 tasks per job = 20 total tasks
                tasks.append({'task': f'task_{i}_{j}', 'fq_name': fq_name})
        for task in tasks:
            flowmanagerMP.submit_task(task)
        flowmanagerMP.wait_for_completion()
        
        end_time = time.time()
        execution_time = end_time - start_time

        # Verify results
        assert len(results) == 20  # Should have 20 results
        
        # Check that each task was processed by the correct job
        for result in results:
            input_task = result['input']
            fq_name = input_task['fq_name']
            assert result['output'] == f'processed by {fq_name}'
        
        # Verify parallel execution - should take ~4 * delay seconds (4 batches of tasks)
        # Add some buffer time for overhead
        assert execution_time < (4 * delay + 1), f"Execution took {execution_time} seconds, expected less than {4 * delay + 1} seconds"



================================================
FILE: tests/test_fmmp_parallel_load.py
================================================
import asyncio
import json
import os
import time

import pytest
import yaml

from flow4ai import f4a_logging as logging
from flow4ai.flowmanagerMP import FlowManagerMP
from flow4ai.job import JobABC
from flow4ai.utils.otel_wrapper import TracerFactory
from tests.test_utils.simple_job import SimpleJobFactory

# Configure logging
logging.basicConfig(level=logging.INFO,
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

class DelayedJob(JobABC):
    def __init__(self, name: str, time_delay: float):
        super().__init__(name)
        self.time_delay = time_delay

    async def run(self, task) -> dict:
        """Execute a delayed job with tracing."""
        logger.info(f"Executing DelayedJob for {task} with delay {self.time_delay}")
        await asyncio.sleep(self.time_delay)  # Use specified delay
        return {"task": task, "status": "complete"}

def create_delayed_job(params: dict) -> JobABC:
    time_delay = params.get('time_delay', 1.0)
    return DelayedJob("Test Job", time_delay)

# Store original load_from_file function
original_load_from_file = SimpleJobFactory._load_from_file

def setup_module(module):
    """Set up test environment"""
    SimpleJobFactory._load_from_file = create_delayed_job

def teardown_module(module):
    """Restore original implementation"""
    SimpleJobFactory._load_from_file = original_load_from_file

def dummy_result_processor(result):
    """Dummy function for processing results in tests"""
    logger.info(f"Processing result: {result}")

async def run_parallel_load_test(num_tasks: int) -> float:
    """Run a load test with specified number of parallel tasks
    
    Args:
        num_tasks: Number of tasks to run in parallel
        
    Returns:
        float: Total execution time in seconds
    """
    start_time = time.perf_counter()
    
    flowmanagerMP_context = {
            "type": "file",
            "params": {"time_delay": 0.01}  # Very small delay for load testing
        }
    loaded_job = SimpleJobFactory.load_job(flowmanagerMP_context)
    flowmanagerMP = FlowManagerMP(loaded_job, dummy_result_processor)

    # Submit all tasks immediately
    for i in range(num_tasks):
        flowmanagerMP.submit_task(f"Task_{i}")
    # Indicate there is no more input data to process to initiate shutdown
    flowmanagerMP.wait_for_completion()

    execution_time = time.perf_counter() - start_time
    logger.info(f"\nExecution time for {num_tasks} tasks: {execution_time:.2f}s")
    return execution_time

async def run_sustained_load_test(tasks_per_second: int, duration: int) -> tuple[float, float]:
    """Run a sustained load test with consistent task submission rate
    
    Args:
        tasks_per_second: Number of tasks to submit per second
        duration: Duration of the test in seconds
        
    Returns:
        tuple[float, float]: (average_latency, max_latency) in seconds
    """
    flowmanagerMP_context = {
        "type": "file",
        "params": {"time_delay": 0.05}  # 50ms baseline processing time
    }
    loaded_job = SimpleJobFactory.load_job(flowmanagerMP_context)
    flowmanagerMP = FlowManagerMP(loaded_job, dummy_result_processor)
    
    start_time = time.perf_counter()
    latencies = []
    
    # Calculate sleep time between tasks to achieve desired rate
    sleep_time = 1.0 / tasks_per_second
    
    # Submit tasks at the specified rate for the specified duration
    task_count = 0
    while time.perf_counter() - start_time < duration:
        task_start = time.perf_counter()
        flowmanagerMP.submit_task(f"SustainedTask_{task_count}")
        task_count += 1
        
        # Record latency
        latency = time.perf_counter() - task_start
        latencies.append(latency)
        
        # Sleep to maintain desired rate
        await asyncio.sleep(sleep_time)
    
    # Mark completion and calculate metrics
    flowmanagerMP.wait_for_completion()
    avg_latency = sum(latencies) / len(latencies)
    max_latency = max(latencies)
    
    return avg_latency, max_latency

def test_maximum_parallel_execution():
    """Test the maximum theoretical parallel execution capacity
    
    This test verifies the system's ability to handle increasing loads of parallel tasks,
    from 100 to 10,000 tasks. It ensures that execution times stay within expected
    thresholds as the task count increases, demonstrating effective parallel processing.
    
    The test uses progressively larger task counts and adjusts expected completion
    times based on the load, accounting for system overhead and parallel processing
    capabilities.
    
    To run this test:
    Use pytest's --full-suite option: pytest --full-suite
    """
    
    # Test with increasing number of tasks
    task_counts = [100, 500, 2500, 5000, 7500, 10000]
    
    for count in task_counts:
        execution_time = asyncio.run(run_parallel_load_test(count))
        
        # Scale expected time with task count
        if count <= 100:
            expected_time = 2.0
        elif count <= 500:
            expected_time = 4.0
        elif count <= 2500:
            expected_time = 10.0
        elif count <= 5000:
            expected_time = 15.0
        elif count <= 7500:
            expected_time = 20.0
        else:  # 10000 tasks
            expected_time = 25.0
            
        assert execution_time < expected_time, (
            f"Expected {count} tasks to complete in under {expected_time} seconds with parallel execution, "
            f"took {execution_time:.2f}s"
        )
        
        tasks_per_second = count / execution_time
        logger.info(f"Tasks per second with {count} tasks: {tasks_per_second:.2f}")

def test_sustained_load_performance():
    """Test system performance under sustained load
    
    This test verifies the system's ability to handle a consistent stream of tasks
    over time while maintaining acceptable latency. It submits tasks at a fixed rate
    and measures both average and maximum latency to ensure system stability under
    continuous load.
    
    The test runs for a fixed duration, submitting tasks at a specified rate, and
    ensures that latency stays within acceptable bounds throughout the test period.
    
    To run this test:
    Use pytest's --full-suite option: pytest --full-suite
    """
    # Test with moderate sustained load: 10 tasks per second for 30 seconds
    tasks_per_second = 10
    duration = 30
    
    avg_latency, max_latency = asyncio.run(
        run_sustained_load_test(tasks_per_second, duration)
    )
    
    # Verify latency remains within acceptable bounds
    assert avg_latency < 0.1, (
        f"Average latency ({avg_latency:.3f}s) exceeded threshold of 0.1s "
        "under sustained load"
    )
    
    assert max_latency < 0.6, (
        f"Maximum latency ({max_latency:.3f}s) exceeded threshold of 0.5s "
        "under sustained load"
    )
    
    logger.info(f"Sustained load test metrics:")
    logger.info(f"  Average latency: {avg_latency:.3f}s")
    logger.info(f"  Maximum latency: {max_latency:.3f}s")

@pytest.fixture
def trace_file():
    """Fixture to create and clean up a temporary trace file"""
    trace_file = "tests/temp_parallel_trace.json"
    # Initialize trace file
    os.makedirs("tests", exist_ok=True)
    with open(trace_file, 'w') as f:
        json.dump([], f)
    yield trace_file
    # Cleanup
    if os.path.exists(trace_file):
        os.unlink(trace_file)

@pytest.fixture
def setup_file_exporter(trace_file):
    """Fixture to set up file exporter configuration"""
    config_path = "tests/otel_config.yaml"
    
    # Create and write config file
    config = {
        "exporter": "file",
        "service_name": "ParallelLoadTest",
        "batch_processor": {
            "max_queue_size": 10000,  # Increased for parallel load
            "schedule_delay_millis": 100  # Decreased for faster processing
        },
        "file_exporter": {
            "path": trace_file
        }
    }
    
    with open(config_path, 'w') as f:
        yaml.dump(config, f)
    
    # Reset TracerFactory state
    TracerFactory._instance = None
    TracerFactory._config = None
    
    # Set config path in environment
    os.environ['FLOW4AI_OT_CONFIG'] = config_path
    
    yield
    
    # Cleanup
    if os.path.exists(config_path):
        os.unlink(config_path)
    if 'FLOW4AI_OT_CONFIG' in os.environ:
        del os.environ['FLOW4AI_OT_CONFIG']
    TracerFactory._instance = None
    TracerFactory._config = None
    time.sleep(0.1)

def test_maximum_parallel_file_trace(trace_file, setup_file_exporter):
    """Test the impact of file tracing on parallel execution capacity
    
    This test verifies the system's ability to handle increasing loads of parallel tasks
    while simultaneously writing trace data to a file. It helps identify any potential
    bottlenecks in the AsyncFileExporter when under heavy parallel load.
    
    The test uses progressively larger task counts and measures both execution time
    and trace file integrity to ensure proper operation under load.
    
    To run this test:
    Use pytest's --full-suite option: pytest --full-suite
    """
    # Test with increasing number of tasks
    task_counts = [100, 500, 2500, 5000]  # Reduced counts for trace testing
    total_tasks = 0
    
    for count in task_counts:
        execution_time = asyncio.run(run_parallel_load_test(count))
        total_tasks += count
        
        # Scale expected time with task count, allowing extra time for tracing
        if count <= 100:
            expected_time = 3.0  # Additional second for tracing overhead
        elif count <= 500:
            expected_time = 6.0
        elif count <= 2500:
            expected_time = 15.0
        else:  # 5000 tasks
            expected_time = 25.0
            
        assert execution_time < expected_time, (
            f"Expected {count} tasks to complete in under {expected_time} seconds with parallel execution and tracing, "
            f"took {execution_time:.2f}s"
        )
        
        tasks_per_second = count / execution_time
        logger.info(f"Tasks per second with {count} tasks (with tracing): {tasks_per_second:.2f}")
        
        # Verify trace file integrity
        time.sleep(1.0)  # Allow time for traces to be written
        with open(trace_file, 'r') as f:
            trace_data = json.load(f)
            
            # Verify we have traces
            assert len(trace_data) > 0, "No traces were recorded"
            
            # Verify trace structure for a sample of traces
            sample_size = min(10, len(trace_data))
            for trace in trace_data[:sample_size]:
                assert 'name' in trace, "Trace missing name"
                assert 'context' in trace, "Trace missing context"
                assert 'trace_id' in trace['context'], "Trace missing trace_id"
                assert 'span_id' in trace['context'], "Trace missing span_id"
                assert 'attributes' in trace, "Trace missing attributes"
    
    # After all tests complete, verify total number of traces matches total tasks executed
    time.sleep(2.0)  # Additional time to ensure all traces are written
    with open(trace_file, 'r') as f:
        trace_data = json.load(f)
        trace_count = len(trace_data)
        # We expect at least one trace per task (there might be more due to additional instrumentation)
        assert trace_count >= total_tasks, (
            f"Expected at least {total_tasks} traces for all executed tasks, "
            f"but found only {trace_count} traces"
        )
        logger.info(f"Total traces recorded: {trace_count} for {total_tasks} tasks executed")



================================================
FILE: tests/test_fmmp_queue_stress.py
================================================
"""
Tests for stress testing queues:

        - Tests high-volume task processing (10,000 tasks)
        - Tests memory pressure handling
        - Tests queue backpressure with slow consumers
        - Tests CPU-intensive workloads
        - Tests mixed workload scenarios
"""
import os
import time

import psutil
import pytest

from flow4ai import f4a_logging as logging
from flow4ai.flowmanagerMP import FlowManagerMP
from flow4ai.job import JobABC


class StressTestJob(JobABC):
    """Job implementation for stress testing queues"""
    def __init__(self):
        super().__init__(name="StressTestJob")
    
    async def run(self, task):
        task = task[self.name] if isinstance(task, dict) and self.name in task else task
        if isinstance(task, dict):
            if task.get('memory_intensive'):
                # Create temporary large data
                large_data = [i for i in range(task.get('size', 1000000))]
                return {'task': task, 'data_size': len(large_data)}
            if task.get('cpu_intensive'):
                # Perform CPU-intensive calculation
                result = sum(i * i for i in range(task.get('iterations', 1000000)))
                return {'task': task, 'result': result}
        return {'task': task, 'completed': True}

def get_process_memory(pid):
    """Get memory usage of a specific process"""
    try:
        process = psutil.Process(pid)
        return process.memory_info().rss
    except (psutil.NoSuchProcess, psutil.AccessDenied):
        return 0

def test_queue_high_volume():
    """Test queue with high volume of tasks"""
    results = []
    def collect_result(result):
        results.append(result)
    
    job = StressTestJob()
    flowmanagerMP = FlowManagerMP(job, collect_result, serial_processing=True)
    
    # Submit a high volume of tasks
    num_tasks = 10000
    for i in range(num_tasks):
        flowmanagerMP.submit_task({job.name: {'task_id': i}}, fq_name=job.name)
    
    flowmanagerMP.wait_for_completion()
    
    assert len(results) == num_tasks
    assert len({r['task']['task_id'] for r in results}) == num_tasks

def test_queue_memory_pressure():
    """Test queue behavior under memory pressure"""
    results = []
    def collect_result(result):
        results.append(result)
    
    job = StressTestJob()
    flowmanagerMP = FlowManagerMP(job, collect_result, serial_processing=True)
    initial_memory = get_process_memory(os.getpid())
    
    # Submit memory-intensive tasks
    num_tasks = 50
    for i in range(num_tasks):
        flowmanagerMP.submit_task({
            job.name: {
                'task_id': i,
                'memory_intensive': True,
                'size': 1000000  # 1M integers
            }
        }, fq_name=job.name)
    
    flowmanagerMP.wait_for_completion()
    
    # Verify all tasks completed
    assert len(results) == num_tasks
    
    # Check memory was properly released
    final_memory = get_process_memory(os.getpid())
    # Allow for some memory overhead, but shouldn't retain all task data
    assert final_memory < initial_memory * 2

def test_queue_backpressure():
    """Test queue backpressure handling with slow consumer"""
    results = []
    def slow_result_processor(result):
        time.sleep(0.01)  # Simulate slow processing
        results.append(result)
    
    job = StressTestJob()
    flowmanagerMP = FlowManagerMP(job, slow_result_processor, serial_processing=True)
    
    # Submit tasks faster than they can be processed
    num_tasks = 100
    start_time = time.time()
    
    for i in range(num_tasks):
        flowmanagerMP.submit_task({job.name: {'task_id': i}}, fq_name=job.name)
        if i % 10 == 0:
            time.sleep(0.001)  # Small delay to prevent overwhelming
    
    flowmanagerMP.wait_for_completion()
    
    # Verify all tasks eventually complete
    assert len(results) == num_tasks
    # Verify results maintained order
    task_ids = [r['task']['task_id'] for r in results]
    assert sorted(task_ids) == list(range(num_tasks))

def test_queue_cpu_intensive():
    """Test queue behavior with CPU-intensive tasks"""
    results = []
    def collect_result(result):
        results.append(result)
    
    flowmanagerMP = FlowManagerMP(StressTestJob(), collect_result, serial_processing=True)
    
    # Submit CPU-intensive tasks
    num_tasks = 4  # Number of CPU cores typically available
    for i in range(num_tasks):
        flowmanagerMP.submit_task({
            'task_id': i,
            'cpu_intensive': True,
            'iterations': 5000000
        })
    
    flowmanagerMP.wait_for_completion()
    
    assert len(results) == num_tasks
    # Verify all tasks produced valid results
    assert all('result' in r for r in results)

def test_queue_mixed_workload():
    """Test queue handling mixed types of workloads"""
    results = []
    def collect_result(result):
        results.append(result)
    
    job = StressTestJob()
    flowmanagerMP = FlowManagerMP(job, collect_result, serial_processing=True)
    
    # Submit mix of different task types
    tasks = [
        {'task_id': 1, 'memory_intensive': True, 'size': 500000},
        {'task_id': 2, 'cpu_intensive': True, 'iterations': 1000000},
        {'task_id': 3},  # Simple task
        {'task_id': 4, 'memory_intensive': True, 'size': 100000},
        {'task_id': 5, 'cpu_intensive': True, 'iterations': 500000}
    ]
    
    for task in tasks:
        flowmanagerMP.submit_task({job.name: task}, fq_name=job.name)
    
    flowmanagerMP.wait_for_completion()
    
    assert len(results) == len(tasks)
    # Verify each task type completed correctly
    for result in results:
        task = result['task']
        if task.get('memory_intensive'):
            assert 'data_size' in result
        elif task.get('cpu_intensive'):
            assert 'result' in result

if __name__ == '__main__':
    pytest.main(['-v', 'test_queue_stress.py'])



================================================
FILE: tests/test_fmmp_result_processing.py
================================================
"""
    Tests parallel and serial functionality with picklable and non-picklable result
        processing functions.
"""

import asyncio
import os
import time

from flow4ai.flowmanagerMP import FlowManagerMP
from flow4ai.job import JobABC


class ResultTimingJob(JobABC):
    def __init__(self):
        super().__init__("Result Timing Job")
        self.executed_tasks = set()

    async def run(self, task) -> dict:
        # Extract the actual task from the wrapped task
        actual_task = task.get(self.name, task)
        # Record task execution using the task string
        if isinstance(actual_task, dict) and 'task' in actual_task:
            task_str = actual_task['task']
        else:
            task_str = str(actual_task)
        self.executed_tasks.add(task_str)
        # Simulate some work
        await asyncio.sleep(0.1)
        current_time = time.time()
        print(f"Executing task {actual_task} at {current_time}")
        return {"task": actual_task, "timestamp": current_time}

class UnpicklableState:
    def __init__(self):
        # Create an unpicklable attribute (file handle)
        self.log_file = open('temp.log', 'w')
    
    def __del__(self):
        try:
            self.log_file.close()
            if os.path.exists('temp.log'):
                os.remove('temp.log')
        except:
            pass

def parallel_mode():
    # Clean up any existing log file
    if os.path.exists('temp.log'):
        os.remove('temp.log')

    print("\nTesting parallel mode (should fail):")
    try:
        # Create unpicklable state
        unpicklable = UnpicklableState()
        
        def unpicklable_processor(result):
            """A result processor that uses unpicklable state"""
            unpicklable.log_file.write(f"Processing: {result}\n")
            unpicklable.log_file.flush()
            print(f"Logged result: {result}")

        # Create FlowManagerMP in parallel mode
        job = ResultTimingJob()
        flowmanagerMP = FlowManagerMP(job, unpicklable_processor, serial_processing=False)

        # Submit some tasks
        for i in range(3):
            flowmanagerMP.submit_task(f"Task {i}")
            time.sleep(0.1)
        
        flowmanagerMP.wait_for_completion()
        assert False, "Expected parallel mode to fail with unpicklable processor"
    except Exception as e:
        print(f"Parallel mode failed as expected: {e}")
        assert "pickle" in str(e).lower(), "Expected pickling error"

def serial_mode():
    print("\nTesting serial mode (should work):")
    try:
        # Clean up any existing log file
        if os.path.exists('temp.log'):
            os.remove('temp.log')
            
        # Create new unpicklable state
        unpicklable = UnpicklableState()
        
        def unpicklable_processor(result):
            """A result processor that uses unpicklable state"""
            unpicklable.log_file.write(f"Processing: {result}\n")
            unpicklable.log_file.flush()
            print(f"Logged result: {result}")

        # Create FlowManagerMP in serial mode
        job = ResultTimingJob()
        flowmanagerMP = FlowManagerMP(job, unpicklable_processor, serial_processing=True)

        # Submit some tasks
        expected_tasks = {f"Task {i}" for i in range(3)}
        for task in expected_tasks:
            flowmanagerMP.submit_task({job.name: {'task': task}}, fq_name=job.name)
            time.sleep(0.1)
        
        # Process tasks and wait for completion
        flowmanagerMP.wait_for_completion()  # Not awaited since it's synchronous
        
        # Give a small delay to ensure all results are processed
        time.sleep(0.5)
        
        # Verify results were written to file
        with open('temp.log', 'r') as f:
            content = f.read()
        
        # Verify all tasks were processed by checking log content
        for task in expected_tasks:
            assert f"'task': {{'task': '{task}'}}" in content, f"Expected task {task} to be processed"
        
        print("All tasks were successfully processed and logged")

    except Exception as e:
        assert False, f"Serial mode should not fail: {e}"


def test_parallel_result_processor_fails_with_unpicklable():
    parallel_mode()
    
def test_serial_result_processor_succeeds_with_unpicklable():
    serial_mode()



================================================
FILE: tests/test_fqname_collision.py
================================================
"""Tests for FQ name collision handling in FlowManager."""

import pytest

from flow4ai import f4a_logging as logging
from flow4ai.dsl import DSLComponent, JobsDict, wrap
from flow4ai.flowmanager import FlowManager
from flow4ai.job import JobABC, Task

# Configure logging
logger = logging.getLogger(__name__)


class SimpleProducerJob(JobABC):
    """Job that produces a value specified at initialization time."""
    
    def __init__(self, name, value):
        super().__init__(name)
        self.value = value
        
    async def run(self, task):
        logger.info(f"Producer {self.name} producing value: {self.value}")
        return {"value": self.value}


class SimpleConsumerJob(JobABC):
    """Job that consumes input from a producer."""
    
    def __init__(self, name, expected_value=None):
        super().__init__(name)
        self.expected_value = expected_value
        
    async def run(self, task):
        # Get inputs from predecessor jobs
        inputs = self.get_inputs()
        logger.info(f"Consumer {self.name} received inputs: {inputs}")
        
        # Get value from producer - using short job name
        producer_value = inputs.get("producer", {}).get("value", None)
        
        return {
            "received_value": producer_value,
            "expected_value": self.expected_value,
            "match": producer_value == self.expected_value
        }


def test_fq_name_collision_resolution():
    """Test that FQ name collisions are properly resolved with unique variants."""
    
    # Create a FlowManager
    fm = FlowManager()
    
    # --- First DSL with producer="A" ---
    producer_a = SimpleProducerJob("producer", "A")
    consumer_a = SimpleConsumerJob("consumer", expected_value="A")
    
    jobs_a = wrap({
        "producer": producer_a,
        "consumer": consumer_a
    })
    
    # Create pipeline: producer_a >> consumer_a
    dsl_a = jobs_a["producer"] >> jobs_a["consumer"]
    
    # Add first DSL - should get standard FQ name
    logger.info("Adding first DSL (A)")
    fq_name_a = fm.add_dsl(dsl_a, "test_pipeline")
    logger.info(f"First DSL FQ name: {fq_name_a}")
    
    # --- Second DSL with producer="B" but same structure ---
    producer_b = SimpleProducerJob("producer", "B")  # Same job name but different value
    consumer_b = SimpleConsumerJob("consumer", expected_value="B")
    
    jobs_b = wrap({
        "producer": producer_b,
        "consumer": consumer_b
    })
    
    # Create pipeline with same structure: producer_b >> consumer_b
    dsl_b = jobs_b["producer"] >> jobs_b["consumer"]
    
    # Add second DSL - should get a unique variant suffix
    logger.info("Adding second DSL (B)")
    fq_name_b = fm.add_dsl(dsl_b, "test_pipeline")
    logger.info(f"Second DSL FQ name: {fq_name_b}")
    
    # Verify two different FQ names were generated
    assert fq_name_a != fq_name_b, "Different DSLs with same structure should get unique FQ names"
    
    # --- Submit Tasks ---
    # Create and submit task for first DSL
    task_a = Task({"id": "A"})
    logger.info(f"Submitting task A to DSL A with FQ name: {fq_name_a}")
    fm.submit_task(task_a, fq_name_a)
    success = fm.wait_for_completion()
    assert success, "Task A did not complete successfully"
    
    # Get and check results 
    results_a = fm.pop_results()
    logger.info(f"Results for task A: {results_a}")
    
    # Extract consumer result from task A
    consumer_result_a = None
    for job_name, job_results in results_a.get("completed", {}).items():
        if job_name == fq_name_a and job_results:
            consumer_result_a = job_results[0]
            break
            
    assert consumer_result_a is not None, "No result found for DSL A"
    assert consumer_result_a.get("received_value") == "A", "Consumer A should have received value 'A'"
    assert consumer_result_a.get("expected_value") == "A", "Consumer A expected value should be 'A'"
    assert consumer_result_a.get("match") == True, "Consumer A expected and received values should match"
    
    # Create and submit task for second DSL
    task_b = Task({"id": "B"})
    logger.info(f"Submitting task B to DSL B with FQ name: {fq_name_b}")
    fm.submit_task(task_b, fq_name_b)
    success = fm.wait_for_completion()
    assert success, "Task B did not complete successfully"
    
    # Get and check results
    results_b = fm.pop_results()
    logger.info(f"Results for task B: {results_b}")
    
    # Extract consumer result from task B
    consumer_result_b = None
    for job_name, job_results in results_b.get("completed", {}).items():
        if job_name == fq_name_b and job_results:
            consumer_result_b = job_results[0]
            break
            
    assert consumer_result_b is not None, "No result found for DSL B"
    assert consumer_result_b.get("received_value") == "B", "Consumer B should have received value 'B'"
    assert consumer_result_b.get("expected_value") == "B", "Consumer B expected value should be 'B'"
    assert consumer_result_b.get("match") == True, "Consumer B expected and received values should match"
    
    # --- Add a third DSL with same structure ---
    producer_c = SimpleProducerJob("producer", "C")
    consumer_c = SimpleConsumerJob("consumer", expected_value="C")
    
    jobs_c = wrap({
        "producer": producer_c,
        "consumer": consumer_c
    })
    
    # Create pipeline with same structure: producer_c >> consumer_c
    dsl_c = jobs_c["producer"] >> jobs_c["consumer"]
    
    # Add third DSL - should get a different unique variant suffix
    logger.info("Adding third DSL (C)")
    fq_name_c = fm.add_dsl(dsl_c, "test_pipeline")
    logger.info(f"Third DSL FQ name: {fq_name_c}")
    
    # Verify third FQ name is different from the first two
    assert fq_name_c != fq_name_a, "Third DSL should get a different FQ name than first DSL"
    assert fq_name_c != fq_name_b, "Third DSL should get a different FQ name than second DSL"
    
    # Create and submit task for third DSL
    task_c = Task({"id": "C"})
    logger.info(f"Submitting task C to DSL C with FQ name: {fq_name_c}")
    fm.submit_task(task_c, fq_name_c)
    success = fm.wait_for_completion()
    assert success, "Task C did not complete successfully"
    
    # Get and check results
    results_c = fm.pop_results()
    logger.info(f"Results for task C: {results_c}")
    
    # Extract consumer result from task C
    consumer_result_c = None
    for job_name, job_results in results_c.get("completed", {}).items():
        if job_name == fq_name_c and job_results:
            consumer_result_c = job_results[0]
            break
            
    assert consumer_result_c is not None, "No result found for DSL C"
    assert consumer_result_c.get("received_value") == "C", "Consumer C should have received value 'C'"
    assert consumer_result_c.get("expected_value") == "C", "Consumer C expected value should be 'C'"
    assert consumer_result_c.get("match") == True, "Consumer C expected and received values should match"



================================================
FILE: tests/test_graph_config_parsing.py
================================================
import pytest

from flow4ai.f4a_graph import (add_edge_anywhere, check_graph_for_cycles,
                               find_head_nodes, find_tail_nodes, print_graph,
                               validate_graph, validate_graph_references)

graph1: dict = {
    'A': {'next': ['B', 'C']},
    'B': {'next': ['C', 'D']},
    'C': {'next': ['D']},
    'D': {'next': []}
}

graph2: dict = {
    'A': {'next': ['B', 'C']},
    'B': {'next': ['C', 'D']},
    'C': {
        'next': ['D'],
        'subgraph': {  # Attributed subgraph
            'V': {'next': ['W']},
            'W': {'next': ['X', 'Y']},
            'X': {'next': ['Z']},
            'Y': {'next': ['Z']},
            'Z': {'next': []}
        }
    },
    'D': {'next': []}
}

graph3: dict = {
    'A': {'next': ['B', 'C']},
    'B': {'next': ['C', 'D']},
    'C': {
        'next': ['D'],
        'subgraph': {  # Attributed subgraph
            'V': {'next': ['W']},
            'W': {
                'next': ['X', 'Y'],
                'subgraph': {  # Attributed subgraph
                    'alpha': {'next': ['beta']},
                    'beta': {'next': ['sigma', 'pi']},
                    'sigma': {'next': ['zeta']},
                    'pi': {'next': ['zeta']},
                    'zeta': {'next': []}
                }
            },
            'X': {'next': ['Z']},
            'Y': {'next': ['Z']},
            'Z': {'next': []}
        }
    },
    'D': {'next': []}
}

def test_head_graph():
    """
    Test that graphs have exactly one head node (node with no incoming edges).
    Tests include:
    1. Valid graphs with single head node (graph1, graph2, graph3)
    2. Invalid graph with no head nodes (cycle)
    3. Invalid graph with multiple head nodes
    4. Invalid graph with no nodes
    """
    # Test valid graphs (should have exactly one head node)
    assert find_head_nodes(graph1) == {'A'}, "graph1 should have exactly one head node 'A'"
    assert find_head_nodes(graph2) == {'A'}, "graph2 should have exactly one head node 'A'"
    assert find_head_nodes(graph3) == {'A'}, "graph3 should have exactly one head node 'A'"
    
    # Test graph with no head nodes (cycle)
    cycle_graph = {
        'X': {'next': ['Y']},
        'Y': {'next': ['Z']},
        'Z': {'next': ['X']}
    }
    assert len(find_head_nodes(cycle_graph)) == 0, "Cycle graph should have no head nodes"
    
    # Test graph with multiple head nodes
    multi_head_graph = {
        'A': {'next': ['C']},
        'B': {'next': ['C']},
        'C': {'next': []}
    }
    heads = find_head_nodes(multi_head_graph)
    assert len(heads) > 1, f"Multi-head graph should have multiple head nodes, found: {heads}"
    
    # Test empty graph
    empty_graph = {}
    assert len(find_head_nodes(empty_graph)) == 0, "Empty graph should have no head nodes"

def test_tail_graph():
    """
    Test that graphs have exactly one tail node (node with no outgoing edges).
    Tests include:
    1. Valid graphs with single tail node (graph1, graph2, graph3)
    2. Invalid graph with no tail nodes (cycle)
    3. Invalid graph with multiple tail nodes
    4. Invalid graph with no nodes
    """
    # Test valid graphs (should have exactly one tail node)
    assert find_tail_nodes(graph1) == {'D'}, "graph1 should have exactly one tail node 'D'"
    assert find_tail_nodes(graph2) == {'D'}, "graph2 should have exactly one tail node 'D'"
    assert find_tail_nodes(graph3) == {'D'}, "graph3 should have exactly one tail node 'D'"
    
    # Test graph with no tail nodes (cycle)
    cycle_graph = {
        'X': {'next': ['Y']},
        'Y': {'next': ['Z']},
        'Z': {'next': ['X']}
    }
    assert len(find_tail_nodes(cycle_graph)) == 0, "Cycle graph should have no tail nodes"
    
    # Test graph with multiple tail nodes
    multi_tail_graph = {
        'A': {'next': []},
        'B': {'next': []},
        'C': {'next': ['A', 'B']}
    }
    tails = find_tail_nodes(multi_tail_graph)
    assert len(tails) > 1, f"Multi-tail graph should have multiple tail nodes, found: {tails}"
    
    # Test empty graph
    empty_graph = {}
    assert len(find_tail_nodes(empty_graph)) == 0, "Empty graph should have no tail nodes"
    
    # Test graph with subgraph tail nodes
    subgraph_tail_graph = {
        'A': {'next': ['B']},
        'B': {
            'next': [],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {'next': []},
            }
        }
    }
    assert find_tail_nodes(subgraph_tail_graph) == {'Y'}, "Should find tail node in subgraph"

def test_print_format():
    """
    Test the print formatting of different graph structures.
    Tests include:
    1. Simple graph with multiple paths
    2. Graph with single subgraph
    3. Graph with nested subgraphs
    """
    print("--------")       
    print("Graph 1:")
    print("--------")
    print_graph(graph1)

    print("--------")
    print("\nGraph 2:")
    print("--------")
    print_graph(graph2)

    print("--------")
    print("\nGraph 3:")
    print("--------")
    print_graph(graph3)

def test_simple_cross_graph_cycle_detection():
    """
    Test cycle detection and edge addition rules for simple graphs and cross-graph scenarios.
    Tests include:
    1. Basic cycle detection on predefined graphs
    2. Edge addition with cycle prevention
    3. Cross-graph edge addition rules with subgraphs
    """
    # Check each graph for cycles
    assert check_graph_for_cycles(graph1, "Graph 1") == False, "Graph 1 should not have cycles"
    assert check_graph_for_cycles(graph2, "Graph 2") == False, "Graph 2 should not have cycles"
    assert check_graph_for_cycles(graph3, "Graph 3") == False, "Graph 3 should not have cycles"

    # Create a test graph
    test_graph = {
        'A': {'next': ['B']},
        'B': {'next': ['C']},
        'C': {'next': ['D']},
        'D': {'next': []}
    }
    original_graph = test_graph.copy()

    # Try to add edges that would create cycles - should be prevented
    result = add_edge_anywhere(test_graph, 'D', 'A')
    assert result == False, "Adding edge D->A should fail as it creates a cycle"
    assert test_graph == original_graph, "Graph should remain unchanged after failed edge addition"

    # Add valid shortcut paths
    result = add_edge_anywhere(test_graph, 'A', 'C')
    assert result == True, "Adding edge A->C should succeed as it's a valid shortcut"
    assert 'C' in test_graph['A']['next'], "Edge A->C should be added to graph"

    result = add_edge_anywhere(test_graph, 'B', 'D')
    assert result == True, "Adding edge B->D should succeed as it's a valid shortcut"
    assert 'D' in test_graph['B']['next'], "Edge B->D should be added to graph"

    # Test subgraph edge addition
    test_graph_with_sub = {
        'A': {'next': ['B']},
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {'next': ['Z']},
                'Z': {'next': []}
            }
        },
        'C': {'next': []}
    }
    original_subgraph = test_graph_with_sub.copy()

    # Test cases for edge addition rules
    # 1. Adding edge within same subgraph
    result = add_edge_anywhere(test_graph_with_sub, 'X', 'Z')
    assert result == True, "Adding edge X->Z within subgraph should succeed"
    assert 'Z' in test_graph_with_sub['B']['subgraph']['X']['next'], "Edge X->Z should be added to subgraph"

    # 2. Adding edge that would create cycle in subgraph
    result = add_edge_anywhere(test_graph_with_sub, 'Z', 'X')
    assert result == False, "Adding edge Z->X should fail as it creates a cycle"

    # 3. Adding cross-graph edge from main to subgraph
    result = add_edge_anywhere(test_graph_with_sub, 'A', 'X')
    assert result == False, "Adding edge A->X should fail as cross-graph references are not allowed"

    # 4. Adding cross-graph edge from subgraph to main
    result = add_edge_anywhere(test_graph_with_sub, 'Z', 'C')
    assert result == False, "Adding edge Z->C should fail as cross-graph references are not allowed"

    # 5. Adding edge in main graph
    result = add_edge_anywhere(test_graph_with_sub, 'A', 'C')
    assert result == True, "Adding edge A->C in main graph should succeed"
    assert 'C' in test_graph_with_sub['A']['next'], "Edge A->C should be added to main graph"

def test_comprehensive_cycle_detection():
    """
    Comprehensive tests for cycle detection in graphs.
    Tests include:
    1. Simple cycles in main graph
    2. Complex cycles in main graph
    3. Cycles in subgraphs
    4. Cycles across nested subgraphs
    5. Valid acyclic graphs with complex paths
    """
    # Test Case 1: Simple cycle in main graph
    simple_cycle = {
        'A': {'next': ['B']},
        'B': {'next': ['C']},
        'C': {'next': ['A']}  # Creates cycle A -> B -> C -> A
    }
    assert check_graph_for_cycles(simple_cycle, "Simple cycle") == True, "Should detect cycle in simple graph A->B->C->A"

    # Test Case 2: Complex cycle in main graph
    complex_cycle = {
        'A': {'next': ['B', 'C']},
        'B': {'next': ['D']},
        'C': {'next': ['E']},
        'D': {'next': ['F']},
        'E': {'next': ['F']},
        'F': {'next': ['B']}  # Creates cycle B -> D -> F -> B
    }
    assert check_graph_for_cycles(complex_cycle, "Complex cycle") == True, "Should detect cycle in complex graph B->D->F->B"

    # Test Case 3: Cycle in subgraph
    subgraph_cycle = {
        'A': {'next': ['B']},
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {'next': ['Z']},
                'Z': {'next': ['X']}  # Creates cycle X -> Y -> Z -> X
            }
        },
        'C': {'next': []}
    }
    assert check_graph_for_cycles(subgraph_cycle, "Subgraph cycle") == True, "Should detect cycle in subgraph X->Y->Z->X"

    # Test Case 4: Valid acyclic graph with multiple paths
    valid_complex = {
        'A': {'next': ['B', 'C', 'D']},
        'B': {'next': ['E']},
        'C': {'next': ['E']},
        'D': {'next': ['E']},
        'E': {'next': []}
    }
    assert check_graph_for_cycles(valid_complex, "Valid complex") == False, "Should not detect cycles in valid complex graph"

    # Test Case 5: Valid acyclic graph with nested subgraphs
    valid_nested = {
        'A': {'next': ['B']},
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {
                    'next': ['Z'],
                    'subgraph': {
                        'alpha': {'next': ['beta']},
                        'beta': {'next': ['gamma']},
                        'gamma': {'next': []}
                    }
                },
                'Z': {'next': []}
            }
        },
        'C': {'next': []}
    }
    assert check_graph_for_cycles(valid_nested, "Valid nested") == False, "Should not detect cycles in valid nested graph"

    # Test Case 6: Multiple paths to same node (diamond pattern)
    diamond_pattern = {
        'A': {'next': ['B', 'C']},
        'B': {'next': ['D']},
        'C': {'next': ['D']},
        'D': {'next': []}
    }
    assert check_graph_for_cycles(diamond_pattern, "Diamond pattern") == False, "Should not detect cycles in diamond pattern"

    # Test Case 7: Complex nested subgraph with cycle
    nested_cycle = {
        'A': {'next': ['B']},
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {
                    'next': ['Z'],
                    'subgraph': {
                        'alpha': {'next': ['beta']},
                        'beta': {'next': ['gamma']},
                        'gamma': {'next': ['alpha']}  # Creates cycle in nested subgraph
                    }
                },
                'Z': {'next': []}
            }
        },
        'C': {'next': []}
    }
    assert check_graph_for_cycles(nested_cycle, "Nested cycle") == True, "Should detect cycle in deeply nested subgraph alpha->beta->gamma->alpha"

def test_cross_graph_references():
    """
    Test validation of graph references to ensure nodes only reference other nodes
    within their own graph level (main graph or subgraph).
    
    Tests include:
    1. Valid graph with proper references
    2. Invalid graph with main graph referencing subgraph node
    3. Invalid graph with subgraph referencing main graph node
    """
    # Test Case 1: Valid graph with proper references
    valid_graph = {
        'A': {'next': ['B']},
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {'next': ['Z']},
                'Z': {'next': []}
            }
        },
        'C': {'next': []}
    }
    is_valid, violations = validate_graph_references(valid_graph)
    assert is_valid, "Valid graph should be valid"
    assert len(violations) == 0, "Valid graph should have no reference violations"

    # Test Case 2: Invalid - main graph referencing subgraph node
    invalid_main_ref = {
        'A': {'next': ['B', 'X']},  # 'X' is in subgraph, can't be referenced from main
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {'next': ['Z']},
                'Z': {'next': []}
            }
        },
        'C': {'next': []}
    }
    is_valid, violations = validate_graph_references(invalid_main_ref)
    assert not is_valid, "Graph with main->subgraph reference should be invalid"
    assert len(violations) == 1, "Should detect one violation for main graph referencing subgraph node"
    assert "Node 'A' references 'X' which is not in the same graph level" in violations[0], \
        "Should detect invalid reference from main graph to subgraph node"

    # Test Case 3: Invalid - subgraph referencing main graph node
    invalid_sub_ref = {
        'A': {'next': ['B']},
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {'next': ['C']},  # 'C' is in main graph, can't be referenced from subgraph
                'Z': {'next': []}
            }
        },
        'C': {'next': []}
    }
    is_valid, violations = validate_graph_references(invalid_sub_ref)
    assert not is_valid, "Graph with subgraph->main reference should be invalid"
    assert len(violations) == 1, "Should detect one violation for subgraph referencing main graph node"
    assert "Node 'B -> Y' references 'C' which is not in the same graph level" in violations[0], \
        "Should detect invalid reference from subgraph to main graph node"

    # Test Case 4: Invalid - multiple cross-graph references
    invalid_multiple_refs = {
        'A': {'next': ['B', 'X', 'Y']},  # Multiple invalid references
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y', 'C']},  # Invalid reference to main graph
                'Y': {'next': ['Z', 'A']},  # Invalid reference to main graph
                'Z': {'next': []}
            }
        },
        'C': {'next': []}
    }
    is_valid, violations = validate_graph_references(invalid_multiple_refs)
    assert not is_valid, "Graph with multiple cross-graph references should be invalid"
    assert len(violations) == 4, "Should detect all cross-graph reference violations"
    violation_texts = ' '.join(violations)
    assert "Node 'A' references 'X' which is not in the same graph level" in violation_texts, "Should detect A->X violation"
    assert "Node 'A' references 'Y' which is not in the same graph level" in violation_texts, "Should detect A->Y violation"
    assert "Node 'B -> X' references 'C' which is not in the same graph level" in violation_texts, "Should detect X->C violation"
    assert "Node 'B -> Y' references 'A' which is not in the same graph level" in violation_texts, "Should detect Y->A violation"

    # Test Case 5: Valid - nested subgraphs with proper references
    valid_nested = {
        'A': {'next': ['B']},
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {
                    'next': ['Z'],
                    'subgraph': {
                        'alpha': {'next': ['beta']},
                        'beta': {'next': ['gamma']},
                        'gamma': {'next': []}
                    }
                },
                'Z': {'next': []}
            }
        },
        'C': {'next': []}
    }
    is_valid, violations = validate_graph_references(valid_nested)
    assert is_valid, "Valid nested subgraphs should be valid"
    assert len(violations) == 0, "Valid nested subgraphs should have no reference violations"

def test_validate_graph(caplog):
    """
    Test comprehensive graph validation.
    Tests include:
    1. Valid graphs (graph1, graph2, graph3)
    2. Graph with cycles
    3. Graph with cross-graph reference violations
    4. Graph with no head node
    5. Graph with multiple head nodes
    6. Graph with no tail node
    7. Graph with multiple tail nodes
    """
    # Test valid graphs
    validate_graph(graph1, "graph1")  # Should pass
    validate_graph(graph2, "graph2")  # Should pass
    validate_graph(graph3, "graph3")  # Should pass
    
    # Test graph with cycles
    cycle_graph = {
        'X': {'next': ['Y']},
        'Y': {'next': ['Z']},
        'Z': {'next': ['X']}
    }
    with pytest.raises(ValueError, match="contains cycles"):
        validate_graph(cycle_graph, "cycle_graph")
    
    # Test graph with cross-graph reference violations
    invalid_ref_graph = {
        'A': {'next': ['B']},
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['C']}  # Invalid: subgraph node references main graph node
            }
        },
        'C': {'next': []}
    }
    with pytest.raises(ValueError, match="invalid cross-graph references"):
        validate_graph(invalid_ref_graph, "invalid_ref_graph")
    
    # Test graph with no head node (all nodes have incoming edges)
    no_head_graph = {
        'A': {'next': ['B']},
        'B': {'next': ['A']}  # Cycle creates no head nodes
    }
    with pytest.raises(ValueError, match="no head nodes"):
        validate_graph(no_head_graph, "no_head_graph")
    
    # Test graph with multiple head nodes
    multi_head_graph = {
        'A': {'next': ['C']},
        'B': {'next': ['C']},
        'C': {'next': []}
    }
    validate_graph(multi_head_graph, "multi_head_graph")
    assert "multiple head nodes" in caplog.text
    assert "Exactly one head node is required" in caplog.text
    
    # Test graph with no tail node (all nodes have outgoing edges)
    no_tail_graph = {
        'A': {'next': ['B']},
        'B': {'next': ['A']}  # Cycle creates no tail nodes
    }
    with pytest.raises(ValueError, match="no tail nodes"):
        validate_graph(no_tail_graph, "no_tail_graph")
    
    # Test graph with multiple tail nodes
    multi_tail_graph = {
        'A': {'next': []},
        'B': {'next': []},
        'C': {'next': ['A', 'B']}
    }
    validate_graph(multi_tail_graph, "multi_tail_graph")
    assert "multiple tail nodes" in caplog.text
    assert "Exactly one tail node is required" in caplog.text



================================================
FILE: tests/test_job_graph.py
================================================
import asyncio
import time
from typing import Any, Dict

from flow4ai.flowmanagerMP import FlowManagerMP
from flow4ai.job import JobABC, Task, job_graph_context_manager
from flow4ai.job_loader import JobFactory
from flow4ai.jobs.default_jobs import DefaultHeadJob


class MockJob(JobABC):
    def run(self):
        pass

class MockJobSubclass(MockJob):
    pass



def test_job_name_always_present():
    # Test with explicit name
    job1 = MockJob(name="explicit_name")
    assert job1.name == "explicit_name"
    
    # Test with auto-generated name
    job2 = MockJob()
    assert job2.name is not None
    assert isinstance(job2.name, str)
    assert len(job2.name) > 0

    # Test subclass with explicit name
    job3 = MockJobSubclass(name="subclass_name")
    assert job3.name == "subclass_name"
    
    # Test subclass with auto-generated name
    job4 = MockJobSubclass()
    assert job4.name is not None
    assert isinstance(job4.name, str)
    assert len(job4.name) > 0

def test_auto_generated_names_are_unique():
    # Create multiple jobs without explicit names
    num_jobs = 100  # Test with a significant number of jobs
    jobs = [MockJob() for _ in range(num_jobs)]
    
    # Collect all names in a set
    names = {job.name for job in jobs}
    
    # If all names are unique, the set should have the same length as the list
    assert len(names) == num_jobs
    
    # Test uniqueness across different subclass instances
    mixed_jobs = [
        MockJob() if i % 2 == 0 else MockJobSubclass()
        for i in range(num_jobs)
    ]
    mixed_names = {job.name for job in mixed_jobs}
    assert len(mixed_names) == num_jobs

def collect_result(results):
    """Create a picklable result collector function that appends to the given list."""
    def collector(result):
        results.append(result)
    return collector


class A(JobABC):
  async def run(self, task: Dict[str, Any]) -> Any:
    inputs = self.get_inputs()
    print(f"\nA expected inputs: {self.expected_inputs}")
    print(f"A data inputs: {inputs}")
    dataA:dict = {
        'dataA1': {},
        'dataA2': {}
    }
    print(f"A returned: {dataA}")
    return dataA

class B(JobABC):
  async def run(self, task: Dict[str, Any]) -> Any:
    inputs = self.get_inputs()
    print(f"\nB expected inputs: {self.expected_inputs}")
    print(f"B data inputs: {inputs}")
    dataB:dict = {
        'dataB1': {},
        'dataB2': {}
    }
    print(f"B returned: {dataB}")
    return dataB

class C(JobABC):
  async def run(self, task: Dict[str, Any]) -> Any:
    inputs = self.get_inputs()
    print(f"\nC expected inputs: {self.expected_inputs}")
    print(f"C data inputs: {inputs}")
    dataC:dict = {
        'dataC1': {},
        'dataC2': {}
    } 
    print(f"C returned: {dataC}")
    return dataC

class D(JobABC):
  async def run(self, task: Dict[str, Any]) -> Any:
    inputs = self.get_inputs()
    print(f"\nD expected inputs: {self.expected_inputs}")
    print(f"D data inputs: {inputs}")
    dataD:dict = {
        'dataD1': {},
        'dataD2': {}
    } 
    print(f"D returned: {dataD}")
    return dataD

class E(MockJob):
    async def run(self, task: Dict[str, Any]) -> Dict[str, Any]:
        return {'result': f'processed by {self.name}'}

class F(MockJob):
    async def run(self, task: Dict[str, Any]) -> Dict[str, Any]:
        return {'result': f'processed by {self.name}'}

class G(MockJob):
    async def run(self, task: Dict[str, Any]) -> Dict[str, Any]:
        return {'result': f'processed by {self.name}'}

class H(MockJob):
    async def run(self, task: Dict[str, Any]) -> Dict[str, Any]:
        return {'result': f'processed by {self.name}'}

class I(MockJob):
    async def run(self, task: Dict[str, Any]) -> Dict[str, Any]:
        return {'result': f'processed by {self.name}'}

class J(MockJob):
    async def run(self, task: Dict[str, Any]) -> Dict[str, Any]:
        return {'result': f'processed by {self.name}'}

jobs = {
    'A': A('A'),
    'B': B('B'),
    'C': C('C'),
    'D': D('D')
}
jobs.update({
    'E': E('E'),
    'F': F('F'),
    'G': G('G'),
    'H': H('H'),
    'I': I('I'),
    'J': J('J'),
})

data:dict = {
    '1': {},
    '2': {}
}

graph_definition1 = {
    'A': {'next': ['B', 'C']},
    'B': {'next': ['C', 'D']},
    'C': {'next': ['D']},
    'D': {'next': []}
} 

graph_definition_complex = {
    'A': {'next': ['B', 'C', 'E']},  # Head job with 3 branches
    'B': {'next': ['D', 'F']},       # Branch 1
    'C': {'next': ['F', 'G']},       # Branch 2
    'D': {'next': ['H']},            # Merge point 1
    'E': {'next': ['G', 'I']},       # Branch 3
    'F': {'next': ['H']},            # Merge point 2
    'G': {'next': ['I']},            # Merge point 3
    'H': {'next': ['J']},            # Pre-final merge
    'I': {'next': ['J']},            # Pre-final merge
    'J': {'next': []}                # Tail job
}

async def execute_graph(graph_definition: dict, jobs: dict, data: dict) -> Any:
    head_job = JobFactory.create_job_graph(graph_definition, jobs)
    job_set = JobABC.job_set(head_job)
    async with job_graph_context_manager(job_set):
        final_result = await head_job._execute(Task(data))
    return final_result

def test_execute_graph1():
    final_result1 = asyncio.run(execute_graph(graph_definition1, jobs, data))
    # Extract just the job result data, ignoring task_pass_through
    result_data = {k: v for k, v in final_result1.items() if k not in ['task_pass_through', 'RETURN_JOB']}
    assert result_data == {
            'dataD1': {},
            'dataD2': {}
        }

def test_job_set():
    head_job = JobFactory.create_job_graph(graph_definition1, jobs)
    job_name_set = head_job.job_set_str()
    assert job_name_set == {'A', 'B', 'C', 'D'}

def test_execute_graph2():
    final_result2 = asyncio.run(execute_graph(graph_definition2, jobs, data))
    # Extract just the job result data, ignoring task_pass_through
    result_data = {k: v for k, v in final_result2.items() if k not in ['task_pass_through', 'RETURN_JOB']}
    assert result_data == {
            'dataC1': {},
            'dataC2': {}
        }
    

graph_definition2 = {
    'A': {'next': ['B', 'C']},
    'B': {'next': ['C']},
    'C': {'next': []},
} 

def test_complex_job_set():
    """
    Test job_set() with a complex graph structure containing:
    - Multiple paths from head to tail
    - Diamond patterns (multiple paths converging)
    - Multiple levels of job dependencies
    Graph structure:
           A
        /  |  \
       B   C   E
      /\  / \  /\
     D  F    G  I
      \ /     \ /
       H       I
        \     /
          J
    """
    head_job = JobFactory.create_job_graph(graph_definition_complex, jobs)
    job_set = head_job.job_set_str()
    expected_jobs = {'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'}
    assert job_set == expected_jobs, f"Expected {expected_jobs}, but got {job_set}"

def test_complex_job_set_instances():
    """
    Test job_set() with a complex graph structure to verify it returns the actual job instances.
    Uses the same graph structure as test_complex_job_set:
           A
        /  |  \
       B   C   E
      /\  / \  /\
     D  F    G  I
      \ /     \ /
       H       I
        \     /
          J
    """
    head_job = JobFactory.create_job_graph(graph_definition_complex, jobs)
    job_instances = JobABC.job_set(head_job)
    
    # Verify we got the correct number of instances
    expected_count = 10  # A through J
    assert len(job_instances) == expected_count, f"Expected {expected_count} job instances, but got {len(job_instances)}"
    
    # Verify all instances are JobABC types
    assert all(isinstance(job, JobABC) for job in job_instances), "All items in job_set should be JobABC instances"
    
    # Verify the job names match what we expect
    job_names = {job.name for job in job_instances}
    expected_names = {'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'}
    assert job_names == expected_names, f"Expected jobs named {expected_names}, but got {job_names}"

def test_multiple_head_nodes():
    """Test that multiple head nodes are handled correctly by creating a DefaultHeadJob."""
    # Test graph with two independent heads
    graph_definition = {
        "A": {"next": ["C"]},
        "B": {"next": ["C"]},
        "C": {"next": []}
    }
    
    job_instances = {
        "A": A("A"),
        "B": B("B"), 
        "C": C("C")
    }
    
    # Create job graph
    head_job = JobFactory.create_job_graph(graph_definition, job_instances)
    
    # Verify default head was created and is correct type
    assert isinstance(head_job, DefaultHeadJob)
    assert head_job.name in job_instances
    
    # Verify graph was updated correctly
    assert head_job.name in graph_definition
    assert set(graph_definition[head_job.name]["next"]) == {"A", "B"}
    
    # Verify next_jobs were set correctly
    assert len(head_job.next_jobs) == 2
    next_job_names = {job.name for job in head_job.next_jobs}
    assert next_job_names == {"A", "B"}


def test_multiple_tail_nodes():
    """Test that multiple tail nodes are handled correctly by creating a DefaultTailJob."""
    # Test graph with two independent tails
    graph_definition = {
        "A": {"next": ["B", "C"]},
        "B": {"next": []},
        "C": {"next": []}
    }
    
    job_instances = {
        "A": A("A"),
        "B": B("B"), 
        "C": C("C")
    }
    
    # Create job graph
    head_job = JobFactory.create_job_graph(graph_definition, job_instances)
    
    # Find the default tail job in the job_instances
    default_tail_jobs = [job for name, job in job_instances.items() 
                        if "DefaultTailJob" in name]
    assert len(default_tail_jobs) == 1, "Expected exactly one DefaultTailJob"
    default_tail_job = default_tail_jobs[0]
    
    # Verify default tail was created and is correct type
    from flow4ai.jobs.default_jobs import DefaultTailJob
    assert isinstance(default_tail_job, DefaultTailJob)
    assert default_tail_job.name in job_instances
    
    # Verify graph was updated correctly
    assert default_tail_job.name in graph_definition
    assert graph_definition[default_tail_job.name]["next"] == []
    
    # Verify original tail nodes now point to the default tail
    assert graph_definition["B"]["next"] == [default_tail_job.name]
    assert graph_definition["C"]["next"] == [default_tail_job.name]
    
    # Verify next_jobs were set correctly for original tail nodes
    b_job = job_instances["B"]
    c_job = job_instances["C"]
    assert len(b_job.next_jobs) == 1
    assert len(c_job.next_jobs) == 1
    assert b_job.next_jobs[0].name == default_tail_job.name
    assert c_job.next_jobs[0].name == default_tail_job.name

def test_simple_parallel_jobs():
    """Test that a graph with multiple head nodes and multiple tail nodes 
    is handled correctly by creating both a DefaultHeadJob and a DefaultTailJob."""
    # Test graph with three independent parallel jobs
    graph_definition = {
        "A": {"next": []},
        "B": {"next": []},
        "C": {"next": []}
    }
    
    job_instances = {
        "A": A("A"),
        "B": B("B"), 
        "C": C("C")
    }
    
    # Create job graph
    head_job = JobFactory.create_job_graph(graph_definition, job_instances)
    
    # Verify default head was created and is correct type
    from flow4ai.jobs.default_jobs import DefaultHeadJob, DefaultTailJob
    assert isinstance(head_job, DefaultHeadJob)
    assert head_job.name in job_instances
    
    # Find the default tail job in the job_instances
    default_tail_jobs = [job for name, job in job_instances.items() 
                        if "DefaultTailJob" in name]
    assert len(default_tail_jobs) == 1, "Expected exactly one DefaultTailJob"
    default_tail_job = default_tail_jobs[0]
    
    # Verify default tail was created and is correct type
    assert isinstance(default_tail_job, DefaultTailJob)
    assert default_tail_job.name in job_instances
    
    # Verify graph structure was updated correctly
    # 1. DefaultHead points to A, B, C
    assert head_job.name in graph_definition
    assert set(graph_definition[head_job.name]["next"]) == {"A", "B", "C"}
    
    # 2. A, B, C point to DefaultTail
    assert graph_definition["A"]["next"] == [default_tail_job.name]
    assert graph_definition["B"]["next"] == [default_tail_job.name]
    assert graph_definition["C"]["next"] == [default_tail_job.name]
    
    # 3. DefaultTail has no next nodes
    assert graph_definition[default_tail_job.name]["next"] == []
    
    # Verify next_jobs were set correctly
    # 1. DefaultHead has 3 next jobs: A, B, C
    assert len(head_job.next_jobs) == 3
    head_next_job_names = {job.name for job in head_job.next_jobs}
    assert head_next_job_names == {"A", "B", "C"}
    
    # 2. A, B, C each have 1 next job: DefaultTail
    a_job = job_instances["A"]
    b_job = job_instances["B"]
    c_job = job_instances["C"]
    assert len(a_job.next_jobs) == 1
    assert len(b_job.next_jobs) == 1
    assert len(c_job.next_jobs) == 1
    assert a_job.next_jobs[0].name == default_tail_job.name
    assert b_job.next_jobs[0].name == default_tail_job.name
    assert c_job.next_jobs[0].name == default_tail_job.name
    
    # 3. DefaultTail has no next jobs
    assert len(default_tail_job.next_jobs) == 0


def test_execute_multiple_head_nodes():
    """Test execution of a graph with multiple head nodes."""
    # Create a graph with multiple head nodes
    graph_definition = {
        "A": {"next": ["D"]},
        "B": {"next": ["D"]},
        "C": {"next": ["D"]},
        "D": {"next": []}
    }
    
    # Create job instances
    job_instances = {
        "A": A("A"),
        "B": B("B"),
        "C": C("C"),
        "D": D("D")
    }
    
    # Execute the graph
    data = {"input": "test"}
    final_result = asyncio.run(execute_graph(graph_definition, job_instances, data))
    
    # Extract just the job result data, ignoring task_pass_through
    result_data = {k: v for k, v in final_result.items() if k not in ['task_pass_through', 'RETURN_JOB']}
    
    # Verify final job data is returned (only D's data is in the final result)
    assert result_data == {
        'dataD1': {},
        'dataD2': {}
    }
    
    # Verify the RETURN_JOB is D
    assert final_result['RETURN_JOB'] == 'D'



================================================
FILE: tests/test_job_loading.py
================================================
import asyncio
import os
from unittest.mock import MagicMock

import pytest
import yaml

from flow4ai import f4a_logging as logging
from flow4ai.f4a_graph import validate_graph
from flow4ai.flowmanagerMP import FlowManagerMP
from flow4ai.job import JobABC, Task, job_graph_context_manager
from flow4ai.job_loader import ConfigLoader, ConfigurationError, JobFactory

# Test configuration
TEST_CONFIG_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config"))

@pytest.fixture
def job_factory() -> JobFactory:
    factory = JobFactory()
    # Load both the test jobs and the real jobs
    #  the real jobs are always loaded by the factory
    factory.load_python_into_registries([TEST_CONFIG_DIR])
    return factory

def test_job_type_registration(job_factory: JobFactory):
    """Test that all expected job types are registered"""
    # Get all registered job types
    job_types = job_factory._job_types_registry
    
    # Expected job types from test directory
    assert "MockJob" in job_types, "MockJob should be registered"
    assert "MockFileReadJob" in job_types, "MockFileReadJob should be registered"
    assert "MockDatabaseWriteJob" in job_types, "MockDatabaseWriteJob should be registered"
    assert "DummyJob" in job_types, "DummyJob should be registered"
    
    # Expected job type from real jobs directory
    assert "OpenAIJob" in job_types, "OpenAIJob should be registered"


def test_pydantic_type_registration(job_factory: JobFactory):
    """Test that all expected pydantic models are registered"""
    # Configure JobFactory to use test_pydantic_config directory
    test_config_dir = os.path.join(os.path.dirname(__file__), "test_configs/test_pydantic_config")
    JobFactory.load_python_into_registries([test_config_dir])
    
    # Get all registered pydantic types
    pydantic_types = job_factory._pydantic_types_registry
    
    # Expected pydantic models from test directory
    assert "UserProfile" in pydantic_types, "UserProfile model should be registered"
    assert "JobMetadata" in pydantic_types, "JobMetadata model should be registered"
    assert "TaskConfig" in pydantic_types, "TaskConfig model should be registered"
    
    # Verify the registered models are actually pydantic BaseModel subclasses
    from pydantic import BaseModel
    assert issubclass(pydantic_types["UserProfile"], BaseModel)
    assert issubclass(pydantic_types["JobMetadata"], BaseModel)
    assert issubclass(pydantic_types["TaskConfig"], BaseModel)

@pytest.mark.asyncio
async def test_job_instantiation_and_execution(job_factory: JobFactory):
    """Test that jobs can be instantiated and run"""
    # Create a mock job instance
    mock_job = job_factory.create_job(
        name="test_mock_job",
        job_type="MockJob",
        job_def={"properties": {"test_param": "test_value"}}
    )
    mock_job.get_inputs = MagicMock(return_value={"test_job_input": "test_value"})
    
    # Verify job creation
    assert mock_job is not None
    assert mock_job.name == "test_mock_job"
    
    # Run the job with required inputs
    result = await mock_job.run(task={})
    assert result is not None
    assert mock_job.name in result

@pytest.mark.asyncio
async def test_openai_job_instantiation_and_execution(job_factory: JobFactory):
    """Test that OpenAIJob can be instantiated and run"""
    # Get the OpenAIJob class from the registry
    assert "OpenAIJob" in job_factory._job_types_registry, "OpenAIJob should be registered"
    OpenAIJobClass = job_factory._job_types_registry["OpenAIJob"]
    
    openai_job = job_factory.create_job(
        name="test_openai_job",
        job_type="OpenAIJob",
        job_def={
            "properties": {
                "api": {
                    "model": "gpt-4",
                    "temperature": 0.7
                },
                "rate_limit": {
                    "max_rate": 1,
                    "time_period": 4
                }
            }
        }
    )
    
    assert openai_job is not None
    assert openai_job.name == "test_openai_job"
    assert isinstance(openai_job, OpenAIJobClass), "Job should be an instance of OpenAIJob"
    
    # Run the job
    result = await openai_job.run({})
    assert result is not None
    assert "response" in result
    assert isinstance(result["response"], str)
    assert len(result["response"]) > 0

def test_config_loader_separate():
    """Test loading configurations from separate files"""
    # Get absolute paths
    test_config_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config"))
    logging.info(f"\nTest config dir: {test_config_dir}")
    logging.info(f"Directory exists: {os.path.exists(test_config_dir)}")
    logging.info(f"Directory contents: {os.listdir(test_config_dir)}")
    
    # Reset ConfigLoader state and set directories
    ConfigLoader._cached_configs = None  # Reset cached configs
    ConfigLoader._set_directories([str(test_config_dir)])  # Convert to string for anyconfig
    logging.info(f"ConfigLoader directories: {ConfigLoader.directories}")
    
    # Test graphs config
    graphs_config = ConfigLoader.get_graphs_config()
    logging.info(f"Graphs config: {graphs_config}")
    assert graphs_config is not None
    with open(os.path.join(test_config_dir, "graphs.yaml"), 'r') as f:
        expected_graphs = yaml.safe_load(f)
    logging.info(f"Expected graphs: {expected_graphs}")
    assert graphs_config == expected_graphs
    
    # Test jobs config
    jobs_config = ConfigLoader.get_jobs_config()
    assert jobs_config is not None
    with open(os.path.join(test_config_dir, "jobs.yaml"), 'r') as f:
        expected_jobs = yaml.safe_load(f)
    assert jobs_config == expected_jobs
    
    # Test parameters config
    params_config = ConfigLoader.get_parameters_config()
    assert params_config is not None
    with open(os.path.join(test_config_dir, "parameters.yaml"), 'r') as f:
        expected_params = yaml.safe_load(f)
    assert params_config == expected_params
    
    # Validate each graph separately
    for graph_name, graph in graphs_config.items():
        validate_graph(graph, graph_name)

def test_config_loader_all():
    """Test loading configurations from a single combined file"""
    # Get absolute paths
    test_config_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config_all"))
    logging.info(f"\nTest config dir: {test_config_dir}")
    logging.info(f"Directory exists: {os.path.exists(test_config_dir)}")
    logging.info(f"Directory contents: {os.listdir(test_config_dir)}")
    
    # Reset ConfigLoader state and set directories
    ConfigLoader._cached_configs = None  # Reset cached configs
    ConfigLoader._set_directories([str(test_config_dir)])  # Convert to string for anyconfig
    logging.info(f"ConfigLoader directories: {ConfigLoader.directories}")
    
    # Load the combined config file for comparison
    with open(os.path.join(test_config_dir, "flow4ai_all.yaml"), 'r') as f:
        all_config = yaml.safe_load(f)
    logging.info(f"All config: {all_config}")
    
    # Test graphs config
    graphs_config = ConfigLoader.get_graphs_config()
    logging.info(f"Graphs config: {graphs_config}")
    assert graphs_config is not None
    assert graphs_config == all_config.get('graphs', {})
    
    # Test jobs config
    jobs_config = ConfigLoader.get_jobs_config()
    logging.info(f"Jobs config: {jobs_config}")
    assert jobs_config is not None
    assert jobs_config == all_config.get('jobs', {})
    
    # Test parameters config
    params_config = ConfigLoader.get_parameters_config()
    logging.info(f"Parameters config: {params_config}")
    assert params_config is not None
    assert params_config == all_config.get('parameters', {})
    
    # Validate each graph separately
    for graph_name, graph in graphs_config.items():
        validate_graph(graph, graph_name)

def test_create_head_jobs_from_config(job_factory: JobFactory):
    """Test that create_head_jobs_from_config creates the correct number of graphs with correct structure"""
    # Set up test config directory
    test_config_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config"))
    logging.info(f"\nTest config dir: {test_config_dir}")
    logging.info(f"Directory exists: {os.path.exists(test_config_dir)}")
    logging.info(f"Directory contents: {os.listdir(test_config_dir)}")
    
    # Reset ConfigLoader state and set directories
    ConfigLoader._cached_configs = None  # Reset cached configs
    ConfigLoader._set_directories([str(test_config_dir)])  # Convert to string for anyconfig

    # Create head jobs
    head_jobs = JobFactory.get_head_jobs_from_config()
    
    # Should create 4 graphs:
    # - 2 from four_stage_parameterized (params1 and params2)
    # - 1 from three_stage (params1)
    # - 1 from three_stage_reasoning (no params)
    assert len(head_jobs) == 4, f"Expected 4 head jobs, got {len(head_jobs)}"
    
    # Get graph definitions for validation
    graphs_config = ConfigLoader.get_graphs_config()
    
    # Validate each head job's structure matches its graph definition
    for i, head_job in enumerate(head_jobs):
        print(f"\nJob Graph {i + 1}:")
        
        # Print all jobs in this graph using DFS
        visited = set()
        def print_job_graph(job):
            if job in visited:
                return
            visited.add(job)
            print(str(job))
            for child in job.next_jobs:
                print_job_graph(child)
        
        print_job_graph(head_job)
        print("----------------------")
        
        # Extract graph name and param group from job name
        job_parts = head_job.name.split("_")
        if len(job_parts) >= 3 and job_parts[0] in graphs_config:
            graph_name = job_parts[0]
            param_group = job_parts[1] if job_parts[1].startswith("params") else None
            
            # Get graph definition
            graph_def = graphs_config[graph_name]
            
            # Validate job structure matches graph definition
            def validate_job_structure(job, graph_def):
                # Get job's base name (without graph and param prefixes)
                base_job_name = "_".join(job.name.split("_")[2:]) if param_group else "_".join(job.name.split("_")[1:])
                
                # Check that next_jobs match graph definition
                expected_next = set(graph_def[base_job_name].get("next", []))
                actual_next = {next_job.name.split("_")[-1] for next_job in job.next_jobs}
                assert expected_next == actual_next, \
                    f"Mismatch in next_jobs for {job.name}. Expected: {expected_next}, Got: {actual_next}"
                
                # Recursively validate next jobs
                for next_job in job.next_jobs:
                    validate_job_structure(next_job, graph_def)
            
            validate_job_structure(head_job, graph_def)

def test_validate_all_jobs_in_graph():
    """Test that validation catches jobs referenced in graphs but not defined in jobs"""
    # Test with invalid configuration
    invalid_config_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config_invalid"))
    ConfigLoader._set_directories([invalid_config_dir])
    
    with pytest.raises(ValueError) as exc_info:
        ConfigLoader.load_all_configs()
    assert "Job 'nonexistent_job' referenced in 'next' field of job 'read_file' in graph 'four_stage_parameterized'" in str(exc_info.value)
    
    # Test with valid configuration
    valid_config_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config"))
    ConfigLoader._set_directories([valid_config_dir])
    
    try:
        ConfigLoader.load_all_configs()
    except ValueError as e:
        pytest.fail(f"Validation failed for valid configuration: {str(e)}")

def test_validate_all_parameters_filled():
    """Test that validation catches missing or invalid parameter configurations"""
    # Test with invalid parameter configuration
    invalid_config_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config_invalid_parameters"))
    ConfigLoader._set_directories([invalid_config_dir])
    
    with pytest.raises(ValueError) as exc_info:
        ConfigLoader.load_all_configs()
    
    error_msg = str(exc_info.value)
    # Should catch missing parameters for read_file in params1
    assert "Job 'read_file' in graph 'four_stage_parameterized' requires parameters {'filepath'} but has no entry in parameter group 'params1'" in error_msg
    
    # Test with valid configuration
    valid_config_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config"))
    ConfigLoader._set_directories([valid_config_dir])
    
    try:
        ConfigLoader.load_all_configs()
    except ValueError as e:
        pytest.fail(f"Validation failed for valid configuration: {str(e)}")



@pytest.mark.asyncio
async def test_job_execution_graph(caplog):
    """Test that all jobs in a graph are executed when _execute is called on the head job."""
    caplog.set_level('DEBUG')  # Set the logging level
    # Load custom job types
    JobFactory.load_python_into_registries([TEST_CONFIG_DIR])

    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config")])
    ConfigLoader.reload_configs()

    # Get head jobs from config
    head_jobs = JobFactory.get_head_jobs_from_config()

    # Get the first head job from four_stage_parameterized_params1
    head_job = [job for job in head_jobs if 'four_stage_parameterized$$params1$$read_file$$' in job.name][0]

    # Execute the head job
    job_set = JobABC.job_set(head_job)
    async with job_graph_context_manager(job_set):
        await asyncio.create_task(head_job._execute(Task({"task": "Test task"})))

    # Expected job names in the graph
    expected_jobs = {
        'four_stage_parameterized$$params1$$read_file$$',
        'four_stage_parameterized$$params1$$ask_llm$$',
        'four_stage_parameterized$$params1$$save_to_db$$',
        'four_stage_parameterized$$params1$$summarize$$'
    }

    # Check that all jobs were executed by verifying their presence in the log output
    for job_name in expected_jobs:
        assert f"{job_name} finished running" in caplog.text, f"Job {job_name} was not executed"

   

@pytest.mark.asyncio
async def test_head_jobs_in_flowmanagerMP_serial():
    """Test that head jobs from config can be executed in FlowManagerMP with serial processing"""
    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config")])
    
    # Initialize results tracking
    results = []
    
    def result_processor(result):
        results.append(result)
        logging.info(f"Processed result: {result}")
    
    # Create FlowManagerMP with serial processing to ensure deterministic results
    flowmanagerMP = FlowManagerMP(dsl=None, result_processing_function=result_processor, serial_processing=True)
    
    # Get head jobs from config to know their names
    head_jobs = flowmanagerMP.get_fq_names()
    
    # Submit tasks for each job
    for job in head_jobs:
        flowmanagerMP.submit_task({"task": "Test task"}, fq_name=job)
    
    # Mark input as completed and wait for all tasks to finish
    flowmanagerMP.wait_for_completion()
    
    # Convert shared list to regular list for sorting
    results_list = list(results)
    
    # Verify results
    # We expect one result per head job
    assert len(results_list) == len(head_jobs), f"Expected {len(head_jobs)} results, got {len(results_list)}"
    
    # Sort results by job name to ensure deterministic ordering
    results_list.sort(key=lambda x: next(iter(x.keys())))
    
    # Each result should be a dictionary with job results
    for result in results_list:
        assert isinstance(result, dict), f"Expected dict result, got {type(result)}"
        
        # Verify parameter substitution for each graph type
        if 'four_stage_parameterized_params1_summarize' in result:
            result_str = str(result['four_stage_parameterized_params1_summarize'])
            # Verify save_to_db parameters
            assert 'postgres://user1:pass1@db1/mydb' in result_str, "Database URL not correctly substituted"
            assert 'table_a' in result_str, "Table name not correctly substituted"
            # Verify read_file parameters
            assert './file1.txt' in result_str, "Filepath not correctly substituted"
            
        elif 'three_stage_params1_summarize' in result:
            result_str = str(result['three_stage_params1_summarize'])
            # Verify save_to_db2 parameters are from the job definition
            assert 'sqlite://user2:pass2@db2/mydb' in result_str, "Database URL not correctly set"
            assert 'table_b' in result_str, "Table name not correctly set"


def process_result(result):
    """Process a result by appending it to the global results list and logging to file"""
    print(f"Got result: {result}")
    # Extract just the job-specific data, excluding task_pass_through and RETURN_JOB
    job_result = {k: v for k, v in result.items() if k not in ['task_pass_through', 'RETURN_JOB']}
    with open("count_parallel_results", "a") as f:
        f.write(str(job_result) + "\n")


@pytest.mark.asyncio
async def test_head_jobs_in_flowmanagerMP_parallel():
    """Test that head jobs from config can be executed in FlowManagerMP with parallel processing"""
    # Clean up any existing results file
    if os.path.exists("count_parallel_results"):
        os.remove("count_parallel_results")
        
    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config")])
    
    # Create FlowManagerMP with parallel processing (default)
    flowmanagerMP = FlowManagerMP(dsl=None, result_processing_function=process_result)
    
    # Get head jobs from config to know their names
    head_jobs = flowmanagerMP.get_fq_names()
    
    # Submit tasks for each job
    for job in head_jobs:
        flowmanagerMP.submit_task({"task": "Test task"}, fq_name=job)
    
    # Mark input as completed and wait for all processing to finish
    flowmanagerMP.wait_for_completion()
    
    # Read results file and verify contents
    with open("count_parallel_results", "r") as f:
        results_list = [eval(line.strip()) for line in f.readlines()]
        assert len(results_list) == 4, f"Expected 4 results but got {len(results_list)}"
        
        # Each result should be a dictionary with job results
        for result in results_list:
            assert isinstance(result, dict), f"Expected dict result, got {type(result)}"
            
            # Verify parameter substitution for each graph type
            if 'four_stage_parameterized_params1_summarize' in result:
                result_str = str(result['four_stage_parameterized_params1_summarize'])
                # Verify save_to_db parameters
                assert 'postgres://user1:pass1@db1/mydb' in result_str, "Database URL not correctly substituted"
                assert 'table_a' in result_str, "Table name not correctly substituted"
                # Verify read_file parameters
                assert './file1.txt' in result_str, "Filepath not correctly substituted"
                
            elif 'four_stage_parameterized_params2_summarize' in result:
                result_str = str(result['four_stage_parameterized_params2_summarize'])
                # Verify save_to_db parameters
                assert 'sqlite://user2:pass2@db2/mydb' in result_str, "Database URL not correctly substituted"
                assert 'table_b' in result_str, "Table name not correctly substituted"
                # Verify read_file parameters
                assert './file2.txt' in result_str, "Filepath not correctly substituted"
                
            elif 'three_stage_params1_summarize' in result:
                result_str = str(result['three_stage_params1_summarize'])
                # Verify save_to_db2 parameters are from the job definition
                assert 'sqlite://user2:pass2@db2/mydb' in result_str, "Database URL not correctly set"
                assert 'table_b' in result_str, "Table name not correctly set"
                
            elif 'three_stage_reasoning__summarize' in result:
                result_str = str(result['three_stage_reasoning__summarize'])
                # Verify save_to_db2 parameters are from the job definition
                assert 'sqlite://user2:pass2@db2/mydb' in result_str, "Database URL not correctly set"
                assert 'table_b' in result_str, "Table name not correctly set"
    
    # Clean up results file
    os.remove("count_parallel_results")

def process_prompts(result):
    """Process a result by appending it to the global results list and logging to file"""
    print(f"Got result: {result}")

@pytest.mark.asyncio
async def test_single_job_multiple_prompts():
    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_single_job")])
    
    # Create FlowManagerMP with parallel processing (default)
    flowmanagerMP = FlowManagerMP(result_processing_function=process_prompts)

    prompts = ["what is the capital of france",
    "what is the capital of germany",
    "what is the capital of the UK",
    "what is the capital of the USA"
    ]

    for prompt in prompts:
        flowmanagerMP.submit_task({"prompt": prompt})
    
    # Mark input as completed and wait for all processing to finish
    flowmanagerMP.wait_for_completion()

@pytest.mark.asyncio
async def test_malformed_configuration():
    """Test that a malformed configuration file raises a clear error."""
    
    # Test malformed graphs config
    test_config_dir = os.path.join(os.path.dirname(__file__), "test_configs/test_malformed_config")
    ConfigLoader._set_directories([test_config_dir])
    
    with pytest.raises(ConfigurationError) as excinfo:
        ConfigLoader.load_all_configs()
        
    error_msg = str(excinfo.value)
    assert "Configuration is malformed" in error_msg
    assert "test_malformed_config/graphs.yaml" in error_msg
    
    # Test malformed jobs config
    test_config_dir = os.path.join(os.path.dirname(__file__), "test_configs/test_malformed_config_jobs")
    ConfigLoader._set_directories([test_config_dir])
    ConfigLoader._cached_configs = None  # Reset cached configs
    
    with pytest.raises(ConfigurationError) as excinfo:
        ConfigLoader.load_all_configs()
        
    error_msg = str(excinfo.value)
    assert "Configuration is malformed" in error_msg
    assert "test_malformed_config_jobs/jobs.yaml" in error_msg
    
    # Test malformed parameters config
    test_config_dir = os.path.join(os.path.dirname(__file__), "test_configs/test_malformed_config_params")
    ConfigLoader._set_directories([test_config_dir])
    ConfigLoader._cached_configs = None  # Reset cached configs
    
    with pytest.raises(ConfigurationError) as excinfo:
        ConfigLoader.load_all_configs()
        
    error_msg = str(excinfo.value)
    assert "Configuration is malformed" in error_msg
    assert "test_malformed_config_params/parameters.yaml" in error_msg

#@pytest.mark.skip("Skipping test due to working yet")
@pytest.mark.asyncio
async def test_pydantic_jobs_in_flowmanagerMP_serial():
    """Test that head jobs from config can be executed in FlowManagerMP with serial processing"""
    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_pydantic_config")])

   
    results = []
    
    
    def result_processor(result):
        results.append(result)
        logging.info(f"Processed result: {result}")
    
    # Create FlowManagerMP with serial processing to ensure deterministic results
    flowmanagerMP = FlowManagerMP(dsl=None, result_processing_function=result_processor, serial_processing=True)
    
    # Get head jobs from config to know their names
    head_jobs = flowmanagerMP.get_fq_names()
    
    # Submit tasks for each job
    for job in head_jobs:
        flowmanagerMP.submit_task({"prompt": "Create a male user."}, fq_name=job)
        flowmanagerMP.submit_task({"prompt": "Create a female user."}, fq_name=job)
    
    # Mark input as completed and wait for all tasks to finish
    flowmanagerMP.wait_for_completion()
    
    # Convert shared list to regular list for sorting
    results_list = list(results)
    
    # Verify results
    # We expect one result per head job
    assert len(results_list) == len(head_jobs)*2, f"Expected {len(head_jobs)} results, got {len(results_list)}"
    
    # Sort results by job name to ensure deterministic ordering
    results_list.sort(key=lambda x: next(iter(x.keys())))
    
    # Each result should be a dictionary with job results
    for result in results_list:
        assert isinstance(result, dict), f"Expected dict result, got {type(result)}"
        logging.info(f"Result: {result}")
        

@pytest.mark.asyncio
async def test_multiple_head_jobs_in_flowmanagerMP_serial(caplog):
    """Test that multiple head jobs from config can be executed in FlowManagerMP with serial processing
       The test uses a configuration with multiple parameters which means that multiple DefaultHeadJobs
       should be created.
    """
    # Enable debug logging
    caplog.set_level('DEBUG')
    
    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_multiple_heads")])
    ConfigLoader.reload_configs()
    # Initialize results tracking
    results = []
    
    def result_processor(result):
        results.append(result)
        logging.info(f"Processed result: {result}")
    
    # Create FlowManagerMP with serial processing to ensure deterministic results
    flowmanagerMP = FlowManagerMP(dsl=None, result_processing_function=result_processor, serial_processing=True)
    
    # Get head jobs from config to know their names
    head_jobs = flowmanagerMP.get_fq_names()
    logging.info(f"Identified head jobs: {head_jobs}")
    
    # Verify there is exactly two head job (the DefaultHeadJob), one for each parameter group
    assert len(head_jobs) == 2, f"Expected exactly two head jobs, got {len(head_jobs)}: {head_jobs}"

    for job in head_jobs:
        logging.info(f"DEBUG - Head job name: {job}")
        parsed_name = JobABC.parse_job_name(job)
        assert parsed_name == "DefaultHeadJob", \
            f"Parsed name mismatch. Expected 'DefaultHeadJob' got {parsed_name}"
        logging.info(f"Submitting task for job: {job}")
        flowmanagerMP.submit_task({"task": "Multi-head test task"}, fq_name=job)
    
    # Mark input as completed and wait for all tasks to finish
    logging.info("Marking input as completed")
    flowmanagerMP.wait_for_completion()
    # FlowManagerMP automatically waits for completion when mark_input_completed is called
    logging.info("FlowManagerMP completed")
    
    # Log the results for debugging
    logging.info(f"Results count: {len(results)}")
    for i, result in enumerate(results):
        logging.info(f"Result {i}: {result}")
    
    # Check if we have any results
    if len(results) == 0:
        logging.error("No results received from job execution")
        assert False, "No results received from job execution"
    
    # Verify that multiple head nodes were detected
    assert "multiple head nodes" in caplog.text.lower(), "Multiple head nodes not detected in log"
    
    # Verify we got at least one result
    assert len(results) == 2, f"Expected two results, got {len(results)}"
    
    for result in results:
        logging.info(f"DEBUG - Result: {result}")
        # Verify the result contains the expected data
        assert "storage_url" in result, "Result missing storage_url field"
        assert "status" in result, "Result missing status field"
        assert result["status"] == "success", f"Expected status 'success', got '{result.get('status')}'"



@pytest.mark.asyncio
async def test_multiple_tail_jobs_in_flowmanagerMP_serial(caplog):
    """Test that multiple tail jobs from config can be executed in FlowManagerMP with serial processing"""
    # Enable debug logging
    caplog.set_level('DEBUG')
    
    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_multiple_tails")])
    ConfigLoader.reload_configs()
    # Initialize results tracking
    results = []
    
    def result_processor(result):
        results.append(result)
        logging.info(f"Processed result: {result}")
    
    # Create FlowManagerMP with serial processing to ensure deterministic results
    flowmanagerMP = FlowManagerMP(dsl=None, result_processing_function=result_processor, serial_processing=True)
    
    # Get head jobs from config to know their names
    head_jobs = flowmanagerMP.get_fq_names()
    logging.info(f"Identified head jobs: {head_jobs}")
    
    # Print head job name for debugging
    if head_jobs:
        logging.info(f"DEBUG - Head job name: {head_jobs[0]}")
    
    # Verify there is exactly one head job
    assert len(head_jobs) == 1, f"Expected exactly one head job, got {len(head_jobs)}: {head_jobs}"
    
    # Get the head job name
    head_job_name = head_jobs[0]
    logging.debug(f"Head job name: {head_job_name}")
    
    # Submit tasks for the head job
    logging.info(f"Submitting task for job: {head_job_name}")
    flowmanagerMP.submit_task({"task": "Multi-tail test task"}, fq_name=head_job_name)
    
    # Mark input as completed and wait for all tasks to finish
    logging.info("Marking input as completed")
    flowmanagerMP.wait_for_completion()
    # FlowManagerMP automatically waits for completion when mark_input_completed is called
    logging.info("FlowManagerMP completed")
    
    # Log the results for debugging
    logging.info(f"Results count: {len(results)}")
    for i, result in enumerate(results):
        logging.info(f"Result {i}: {result}")
    
    # Check if we have any results
    if len(results) == 0:
        logging.error("No results received from job execution")
        assert False, "No results received from job execution"
    
    # Verify that multiple tail nodes were detected
    assert "multiple tail nodes" in caplog.text.lower(), "Multiple tail nodes not detected in log"
    
    # Verify we got at least one result from each tail job
    # Since we have a DefaultTailJob, we should get at least one result
    assert len(results) > 0, f"Expected at least one result, got {len(results)}"
    
    # Verify the results contain the expected data
    # The result should contain the DefaultTailJob information
    for result in results:
        assert "RETURN_JOB" in result, "Result missing RETURN_JOB field"
        assert "DefaultTailJob" in result["RETURN_JOB"], f"Expected DefaultTailJob in RETURN_JOB, got {result.get('RETURN_JOB')}"
        assert "task_pass_through" in result, "Result missing task_pass_through field"
        assert "processor_alpha" in result, "Result missing processor_alpha field"
        assert "processor_beta" in result, "Result missing processor_beta field"
        assert "accuracy" in result["processor_alpha"], "processor_alpha should contain returned values"
        assert "archived_items" in result["processor_beta"], "processor_beta should contain returned values"

@pytest.mark.asyncio
async def test_multiple_tail_jobs_2_parameters(caplog):
    """Test that multiple tail jobs from config can be executed in FlowManagerMP with serial processing"""
    # Enable debug logging
    caplog.set_level('DEBUG')
    
    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_multiple_tails2")])
    ConfigLoader.reload_configs()
    # Initialize results tracking
    results = []
    
    def result_processor(result):
        results.append(result)
        logging.info(f"Processed result: {result}")
    
    # Create FlowManagerMP with serial processing to ensure deterministic results
    flowmanagerMP = FlowManagerMP(dsl=None, result_processing_function=result_processor, serial_processing=True)
    
    # Get head jobs from config to know their names
    head_jobs = flowmanagerMP.get_fq_names()
    logging.info(f"Identified head jobs: {head_jobs}")
    
    # Verify there is exactly two head jobs
    assert len(head_jobs) == 2, f"Expected exactly two head jobs, got {len(head_jobs)}: {head_jobs}"

    for head_job_name in head_jobs:
        logging.debug(f"Head job name: {head_job_name}")
        # Submit tasks for the head job
        logging.info(f"Submitting task for job: {head_job_name}")
        flowmanagerMP.submit_task({"task": "Multi-tail test task"}, fq_name=head_job_name)
    
    # Mark input as completed and wait for all tasks to finish
    logging.info("Marking input as completed")
    flowmanagerMP.wait_for_completion()
    # FlowManagerMP automatically waits for completion when mark_input_completed is called
    logging.info("FlowManagerMP completed")
    
    # Log the results for debugging
    logging.info(f"Results count: {len(results)}")
    for i, result in enumerate(results):
        logging.info(f"Result {i}: {result}")
    
    # Check if we have any results
    if len(results) == 0:
        logging.error("No results received from job execution")
        assert False, "No results received from job execution"
    
    # Verify that multiple tail nodes were detected
    assert "multiple tail nodes" in caplog.text.lower(), "Multiple tail nodes not detected in log"
    
    # Verify we got at least one result from each tail job
    # Since we have a DefaultTailJob, we should get at least one result
    assert len(results) > 0, f"Expected at least one result, got {len(results)}"
    
    # Verify the results contain the expected data
    # The result should contain the DefaultTailJob information
    for result in results:
        assert "RETURN_JOB" in result, "Result missing RETURN_JOB field"
        assert "DefaultTailJob" in result["RETURN_JOB"], f"Expected DefaultTailJob in RETURN_JOB, got {result.get('RETURN_JOB')}"
        assert "task_pass_through" in result, "Result missing task_pass_through field"
        assert "processor_alpha" in result, "Result missing processor_alpha field"
        assert "processor_beta" in result, "Result missing processor_beta field"
        assert "accuracy" in result["processor_alpha"], "processor_alpha should contain returned values"
        assert "archived_items" in result["processor_beta"], "processor_beta should contain returned values"


@pytest.mark.asyncio
async def test_simple_parallel_jobs_in_flowmanagerMP_serial(caplog):
    """Test that a simple parallel job graph with multiple independent jobs can be executed in FlowManagerMP"""
    # Enable debug logging
    caplog.set_level('DEBUG')
    
    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_simple_parallel")])
    ConfigLoader.reload_configs()
    # Initialize results tracking
    results = []
    
    def result_processor(result):
        results.append(result)
        logging.info(f"Processed result: {result}")
    
    # Create FlowManagerMP with serial processing to ensure deterministic results
    flowmanagerMP = FlowManagerMP(dsl=None, result_processing_function=result_processor, serial_processing=True)
    
    # Get head jobs from config to know their names
    head_jobs = flowmanagerMP.get_fq_names()
    logging.info(f"Identified head jobs: {head_jobs}")
    
    # Verify there is exactly one head job (the DefaultHeadJob)
    assert len(head_jobs) == 1, f"Expected exactly one head job, got {len(head_jobs)}: {head_jobs}"
    
    # Get the head job name and verify it's correctly formatted
    head_job_name = head_jobs[0]
    logging.debug(f"Head job name: {head_job_name}")
    parsed_name = JobABC.parse_job_name(head_job_name)
    assert parsed_name == "DefaultHeadJob", \
        f"Parsed name mismatch. Expected 'DefaultHeadJob' got {parsed_name}"
    
    # Submit tasks for the head job
    logging.info(f"Submitting task for job: {head_job_name}")
    flowmanagerMP.submit_task({"task": "Simple parallel test task"}, fq_name=head_job_name)
    
    # Mark input as completed and wait for all tasks to finish
    logging.info("Marking input as completed")
    flowmanagerMP.wait_for_completion()
    # FlowManagerMP automatically waits for completion when mark_input_completed is called
    logging.info("FlowManagerMP completed")
    
    # Log the results for debugging
    logging.info(f"Results count: {len(results)}")
    for i, result in enumerate(results):
        logging.info(f"Result {i}: {result}")
    
    # Check if we have any results
    if len(results) == 0:
        logging.error("No results received from job execution")
        assert False, "No results received from job execution"
    
    # Verify that both multiple head nodes and multiple tail nodes were detected
    assert "multiple head nodes" in caplog.text.lower(), "Multiple head nodes not detected in log"
    assert "multiple tail nodes" in caplog.text.lower(), "Multiple tail nodes not detected in log"
    
    # Verify we got at least one result
    assert len(results) > 0, f"Expected at least one result, got {len(results)}"
    
    # Verify the result contains the DefaultTailJob information
    for result in results:
        assert "RETURN_JOB" in result, "Result missing RETURN_JOB field"
        assert "DefaultTailJob" in result["RETURN_JOB"], f"Expected DefaultTailJob in RETURN_JOB, got {result.get('RETURN_JOB')}"
        assert "task_pass_through" in result, "Result missing task_pass_through field"

@pytest.mark.asyncio
async def test_save_result():
    """Test that head jobs from config can be executed in FlowManagerMP with serial processing"""
    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_save_result")])
    
    # Initialize results tracking
    results = []
    
    def result_processor(result):
        results.append(result)
        logging.info(f"Processed result: {result}")
    
    # Create FlowManagerMP with serial processing to ensure deterministic results
    flowmanagerMP = FlowManagerMP(dsl=None, result_processing_function=result_processor, serial_processing=True)
    
    # Get head jobs from config to know their names
    head_jobs = flowmanagerMP.get_fq_names()
    
    # Submit tasks for each job
    for job in head_jobs:
        flowmanagerMP.submit_task({"task": "Test task"}, fq_name=job)
    
    # Mark input as completed and wait for all tasks to finish
    flowmanagerMP.wait_for_completion()
    
    # Convert shared list to regular list for sorting
    results_list = list(results)
    
    # Verify results
    # We expect one result per head job
    assert len(results_list) == len(head_jobs), f"Expected {len(head_jobs)} results, got {len(results_list)}"
    
    # Sort results by job name to ensure deterministic ordering
    results_list.sort(key=lambda x: next(iter(x.keys())))
    
    # Each result should be a dictionary with job results
    for result in results_list:
        assert isinstance(result, dict), f"Expected dict result, got {type(result)}"
        tail_job_name = result.get('RETURN_JOB')
        tail_job_value = result.get("dummy_job_result")
        # Verify parameter substitution for each graph type
        if 'four_stage_parameterized$$params1$$summarize$$' == tail_job_name:
            # Verify save_to_db parameters
            assert 'postgres://user1:pass1@db1/mydb' in tail_job_value, "Database URL not correctly substituted"
            assert 'table_a' in tail_job_value, "Table name not correctly substituted"
            # Verify read_file parameters
            assert './file1.txt' in tail_job_value, "Filepath not correctly substituted"
            assert JobABC.SAVED_RESULTS not in result, f"{JobABC.SAVED_RESULTS} should not be in {result}"

        elif 'four_stage_parameterized$$params2$$summarize$$' == tail_job_name:
            saved_result_str = str(result[JobABC.SAVED_RESULTS]['save_to_db'])
            assert 'sqlite://user2:pass2@db2/mydb' in saved_result_str, "Database URL not correctly saved"
            assert 'table_b' in saved_result_str, "Table name not correctly saved"
            
        elif 'three_stage$$params1$$summarize$$' == tail_job_name:
            # Verify save_to_db2 parameters are from the job definition
            assert 'sqlite://user2:pass2@db2/mydb' in tail_job_value, "Database URL not correctly set"
            assert 'table_b' in tail_job_value, "Table name not correctly set"
            assert JobABC.SAVED_RESULTS not in result, f"{JobABC.SAVED_RESULTS} should not be in {result}"
            
        elif 'three_stage_reasoning$$params1$$summarize$$' == tail_job_name:
            saved_result_str = str(result[JobABC.SAVED_RESULTS]['save_to_db'])
            assert 'sqlite://user2:pass2@db2/mydb' in saved_result_str, "Database URL not correctly saved"
            assert 'table_b' in saved_result_str, "Table name not correctly saved"


================================================
FILE: tests/test_job_name_parsing.py
================================================
import pytest

from flow4ai.job import JobABC


def test_parse_graph_name():
    # Valid cases
    assert JobABC.parse_graph_name("four_stage_parameterized$$params1$$read_file$$") == "four_stage_parameterized"
    assert JobABC.parse_graph_name("three_stage_reasoning$$$$ask_llm_reasoning$$") == "three_stage_reasoning"
    
    # Invalid cases
    assert JobABC.parse_graph_name("invalid_name") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_graph_name("$$$$$$") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_graph_name("name$$param$$job") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_graph_name("") == "UNSUPPORTED NAME FORMAT"

def test_parse_param_name():
    # Valid cases
    assert JobABC.parse_param_name("four_stage_parameterized$$params1$$read_file$$") == "params1"
    assert JobABC.parse_param_name("three_stage_reasoning$$$$ask_llm_reasoning$$") == ""
    
    # Invalid cases
    assert JobABC.parse_param_name("invalid_name") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_param_name("$$$$$$") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_param_name("name$$param$$job") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_param_name("") == "UNSUPPORTED NAME FORMAT"

def test_parse_job_name():
    # Valid cases
    assert JobABC.parse_job_name("four_stage_parameterized$$params1$$read_file$$") == "read_file"
    assert JobABC.parse_job_name("three_stage_reasoning$$$$ask_llm_reasoning$$") == "ask_llm_reasoning"
    
    # Invalid cases
    assert JobABC.parse_job_name("invalid_name") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_job_name("$$$$$$") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_job_name("name$$param$$job") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_job_name("") == "UNSUPPORTED NAME FORMAT"

def test_parse_job_loader_name():
    # Valid cases with parameters
    result1 = JobABC.parse_job_loader_name("four_stage_parameterized$$params1$$read_file$$")
    assert result1 == {
        "graph_name": "four_stage_parameterized",
        "param_name": "params1",
        "job_name": "read_file"
    }
    
    # Valid cases without parameters
    result2 = JobABC.parse_job_loader_name("three_stage_reasoning$$$$ask_llm_reasoning$$")
    assert result2 == {
        "graph_name": "three_stage_reasoning",
        "param_name": "",
        "job_name": "ask_llm_reasoning"
    }
    
    # Invalid cases
    invalid_cases = [
        "invalid_name",
        "$$$$$$",
        "name$$param$$job",
        "",
        "name$$param$$job$$extra$$",
        "name$$param$$job$"
    ]
    
    for invalid_case in invalid_cases:
        result = JobABC.parse_job_loader_name(invalid_case)
        assert result == {"parsing_message": "UNSUPPORTED NAME FORMAT"}, f"Failed for case: {invalid_case}"

def test_get_input_from():
    # Create mock input data with realistic job names
    mock_inputs = {
        'four_stage_parameterized$$params1$$read_file$$': {'file_content': 'data1'},
        'four_stage_parameterized$$params1$$ask_llm$$': {'llm_response': 'answer1'},
        'four_stage_parameterized$$params1$$save_to_db$$': {'db_status': 'saved'},
        'four_stage_parameterized$$params1$$summarize$$': {'summary': 'text1'},
        'three_stage$$params1$$ask_llm_mini$$': {'mini_response': 'short_answer'},
        'three_stage_reasoning$$$$ask_llm_reasoning$$': {'reasoning': 'explanation'}
    }

    # Test getting input by short job name
    assert JobABC.get_input_from(mock_inputs, 'read_file') == {'file_content': 'data1'}
    assert JobABC.get_input_from(mock_inputs, 'ask_llm') == {'llm_response': 'answer1'}
    assert JobABC.get_input_from(mock_inputs, 'save_to_db') == {'db_status': 'saved'}
    assert JobABC.get_input_from(mock_inputs, 'summarize') == {'summary': 'text1'}
    assert JobABC.get_input_from(mock_inputs, 'ask_llm_mini') == {'mini_response': 'short_answer'}
    assert JobABC.get_input_from(mock_inputs, 'ask_llm_reasoning') == {'reasoning': 'explanation'}

    # Test non-existent job name returns empty dict
    assert JobABC.get_input_from(mock_inputs, 'non_existent_job') == {}

    # Test with empty inputs dict
    assert JobABC.get_input_from({}, 'read_file') == {}

    # Test that we get the first matching job when multiple jobs have same short name
    # In this case, read_file appears in both params1 and params2
    assert JobABC.get_input_from(mock_inputs, 'read_file') == {'file_content': 'data1'}



================================================
FILE: tests/test_job_tracing.py
================================================
import inspect
from typing import Any, Dict

from flow4ai.job import JobABC, _has_own_traced_execute, _is_traced


class Level1Job(JobABC):
    """First level in the inheritance hierarchy."""
    async def run(self, task) -> Dict[str, Any]:
        return {"task": task, "level": 1}


class Level2Job(Level1Job):
    """Second level in the inheritance hierarchy."""
    async def run(self, task) -> Dict[str, Any]:
        return {"task": task, "level": 2}


class Level3Job(Level2Job):
    """Third level in the inheritance hierarchy."""
    async def run(self, task) -> Dict[str, Any]:
        return {"task": task, "level": 3}


def test_deep_hierarchy_tracing():
    """Test that only JobABC._execute is traced, not its subclasses."""
    # Create instances of each level
    level1_job = Level1Job("Level 1")
    level2_job = Level2Job("Level 2")
    level3_job = Level3Job("Level 3")

    # Count how many classes have their own traced _execute
    def count_own_traced_execute(job):
        count = 0
        cls = job.__class__
        while cls != object:
            if _has_own_traced_execute(cls):
                count += 1
            cls = cls.__base__
        return count

    # Each instance should only have one class with its own traced _execute
    # (JobABC)
    assert count_own_traced_execute(level1_job) == 1, "Should only have one class with own traced _execute"
    assert count_own_traced_execute(level2_job) == 1, "Should only have one class with own traced _execute"
    assert count_own_traced_execute(level3_job) == 1, "Should only have one class with own traced _execute"


class SimpleTestJob(JobABC):
    """A simple Job implementation for testing."""
    async def run(self, task) -> Dict[str, Any]:
        return {"task": task, "status": "complete"}


def test_abstractjob_execute_is_traced():
    """Test that JobABC's execute method is traced"""
    # Create a test job instance
    job = SimpleTestJob("Test Job")
    
    # Get the actual JobABC class
    abstract_job_cls = JobABC
    
    # Verify JobABC's _execute is traced
    assert _has_own_traced_execute(abstract_job_cls), "JobABC should have its own traced _execute"
    
    # Verify the subclass doesn't have its own traced _execute
    assert not _has_own_traced_execute(job.__class__), "Subclass should not have its own traced _execute"


def test_job_execute_no_trace_available():
    """Test that JobABC subclasses have access to untraced execute via executeNoTrace"""
    job = SimpleTestJob("Test Job")
    assert hasattr(job, 'executeNoTrace'), "Job should have executeNoTrace method"
    assert not _is_traced(job.__class__.executeNoTrace), "executeNoTrace should not be traced"


def test_subclass_execute_not_traced():
    """Test that JobABC subclasses do not have their own traced execute methods"""
    class CustomJob(JobABC):
        async def run(self, task) -> Dict[str, Any]:
            return {"task": task, "status": "success"}
    
    job = CustomJob("Test Job")
    assert not _has_own_traced_execute(job.__class__), "Subclass should not have its own traced _execute"
    assert hasattr(job, 'executeNoTrace'), "Should still have executeNoTrace method"


def test_decorator_preserves_method_signature():
    """Test that the traced execute method preserves the original signature"""
    class TestJob(JobABC):
        async def run(self,task) -> Dict[str, Any]:
            return {"task": task, "status": "complete"}
    
    # Get the signatures of the traced and untraced versions
    untraced_sig = inspect.signature(TestJob.executeNoTrace)
    traced_sig = inspect.signature(JobABC._execute)
    
    # Compare parameters and return annotation
    assert str(untraced_sig.parameters) == str(traced_sig.parameters), (
        "The traced execute method should preserve the parameter signature"
    )
    assert untraced_sig.return_annotation == traced_sig.return_annotation, (
        "The traced execute method should preserve the return type annotation"
    )


def test_job_factory_returns_untraced_jobs():
    """Test that JobFactory returns properly untraced Job instances"""
    from tests.test_utils.simple_job import SimpleJobFactory

    # Test file-based job loading
    file_job = SimpleJobFactory.load_job({"type": "file", "params": {}})
    assert isinstance(file_job, JobABC)
    assert not _has_own_traced_execute(file_job.__class__), "Factory-created jobs should not have their own traced _execute"
    assert hasattr(file_job, 'executeNoTrace')
    
    # Test datastore-based job loading
    datastore_job = SimpleJobFactory.load_job({"type": "datastore", "params": {}})
    assert isinstance(datastore_job, JobABC)
    assert not _has_own_traced_execute(datastore_job.__class__), "Factory-created jobs should not have their own traced _execute"
    assert hasattr(datastore_job, 'executeNoTrace')


def test_job_implementations_not_traced():
    """Test that Job implementations do not have their own traced execute methods"""
    def get_all_subclasses(cls):
        """Recursively get all subclasses of a class"""
        subclasses = set()
        for subclass in cls.__subclasses__():
            subclasses.add(subclass)
            subclasses.update(get_all_subclasses(subclass))
        return subclasses
    
    # Get all JobABC subclasses (excluding our test classes)
    job_subclasses = {cls for cls in get_all_subclasses(JobABC)
                     if not cls.__module__.startswith('test_job_tracing')}
    
    # Check each subclass
    traced_classes = []
    for cls in job_subclasses:
        if _has_own_traced_execute(cls):
            traced_classes.append(f"{cls.__module__}.{cls.__name__}")
    
    assert not traced_classes, (
        f"Found Job subclasses with their own traced execute method: "
        f"{', '.join(traced_classes)}\n"
        "Job subclasses should not have their own traced execute method.\n"
        "Only JobABC should have tracing."
    )


def test_execute_no_trace_matches_original():
    """Test that executeNoTrace matches the original implementation"""
    class TestJob(JobABC):
        async def run(self, task) -> Dict[str, Any]:
            return {"task": task, "result": "success"}
    
    job = TestJob("Test Job")
    
    # Get the source code of both methods
    execute_source = inspect.getsource(job.__class__.executeNoTrace)
    
    # The source code should contain key implementation details
    assert "async def" in execute_source
    assert "_execute(self, inputs" in execute_source  # More flexible check that works with type hints


def test_subclass_execute_not_traced():
    """Test that JobABC subclasses do not have their own traced execute methods"""
    class CustomJob(JobABC):
        async def run(self, task) -> Dict[str, Any]:
            return {"task": task, "status": "success"}
    
    job = CustomJob("Test Job")
    assert not _has_own_traced_execute(job.__class__), "Subclass should not have its own traced _execute"
    assert hasattr(job, 'executeNoTrace'), "Should still have executeNoTrace method"


def test_decorator_preserves_method_signature():
    """Test that the traced execute method preserves the original signature"""
    class TestJob(JobABC):
        async def run(self, task) -> Dict[str, Any]:
            return {"task": task, "status": "complete"}
    
    # Get the signatures of the traced and untraced versions
    untraced_sig = inspect.signature(TestJob.executeNoTrace)
    traced_sig = inspect.signature(JobABC._execute)
    
    # Compare parameters and return annotation
    assert str(untraced_sig.parameters) == str(traced_sig.parameters), (
        "The traced execute method should preserve the parameter signature"
    )
    assert untraced_sig.return_annotation == traced_sig.return_annotation, (
        "The traced execute method should preserve the return type annotation"
    )


def test_job_factory_returns_untraced_jobs():
    """Test that JobFactory returns properly untraced Job instances"""
    from tests.test_utils.simple_job import SimpleJobFactory

    # Test file-based job loading
    file_job = SimpleJobFactory.load_job({"type": "file", "params": {}})
    assert isinstance(file_job, JobABC)
    assert not _has_own_traced_execute(file_job.__class__), "Factory-created jobs should not have their own traced _execute"
    assert hasattr(file_job, 'executeNoTrace')
    
    # Test datastore-based job loading
    datastore_job = SimpleJobFactory.load_job({"type": "datastore", "params": {}})
    assert isinstance(datastore_job, JobABC)
    assert not _has_own_traced_execute(datastore_job.__class__), "Factory-created jobs should not have their own traced _execute"
    assert hasattr(datastore_job, 'executeNoTrace')


def test_job_implementations_not_traced():
    """Test that Job implementations do not have their own traced execute methods"""
    def get_all_subclasses(cls):
        """Recursively get all subclasses of a class"""
        subclasses = set()
        for subclass in cls.__subclasses__():
            subclasses.add(subclass)
            subclasses.update(get_all_subclasses(subclass))
        return subclasses
    
    # Get all JobABC subclasses (excluding our test classes)
    job_subclasses = {cls for cls in get_all_subclasses(JobABC)
                     if not cls.__module__.startswith('test_job_tracing')}
    
    # Check each subclass
    traced_classes = []
    for cls in job_subclasses:
        if _has_own_traced_execute(cls):
            traced_classes.append(f"{cls.__module__}.{cls.__name__}")
    
    assert not traced_classes, (
        f"Found Job subclasses with their own traced execute method: "
        f"{', '.join(traced_classes)}\n"
        "Job subclasses should not have their own traced execute method.\n"
        "Only JobABC should have tracing."
    )


def test_execute_no_trace_matches_original():
    """Test that executeNoTrace matches the original implementation"""
    class TestJob(JobABC):
        async def run(self, task) -> Dict[str, Any]:
            return {"task": task, "result": "success"}
    
    job = TestJob("Test Job")
    
    # Get the source code of both methods
    execute_source = inspect.getsource(job.__class__.executeNoTrace)
    
    # The source code should contain key implementation details
    assert "async def" in execute_source
    assert "_execute(self, task" in execute_source  # More flexible check that works with type hints



================================================
FILE: tests/test_jobs.py
================================================
import asyncio
from typing import Any, Dict

import pytest

from flow4ai import f4a_logging as logging
from flow4ai.jobs import OpenAIJob


@pytest.mark.asyncio
async def test_openai_job_async_calls():
    """Test that OpenAIJob can make multiple asynchronous calls to GPT-4o-mini."""
    # Create an OpenAIJob instance
    job = OpenAIJob(properties={
        "api": {
            "model": "gpt-4o-mini",
            "temperature": 0.7
        }
    })
    
    # Test with prompt format
    prompt_tasks = [
        {"prompt": "What is 2+2?"},
        {"prompt": "What is the capital of France?"},
        {"prompt": "What is the color of the sky?"}
    ]
    
    # Record start time
    start_time = asyncio.get_event_loop().time()
    logging.info("Starting async OpenAI calls with prompts")
    
    # Create and gather all tasks
    async_tasks: Dict[str, Any] = []
    for task in prompt_tasks:
        async_tasks.append(asyncio.create_task(job.run(task)))
    
    # Wait for all tasks to complete
    results = await asyncio.gather(*async_tasks)
    
    # Calculate elapsed time
    elapsed_time = asyncio.get_event_loop().time() - start_time
    logging.info(f"All OpenAI prompt calls completed in {elapsed_time:.2f} seconds")
    
    # Verify prompt results
    assert len(results) == 3
    for result in results:
        assert isinstance(result, dict)
        assert ("response" in result) or ("error" in result), "Expected either 'response' or 'error' in result"
        if "response" in result:
            assert isinstance(result["response"], str)
            assert len(result["response"]) > 0

    # Test with messages format
    message_tasks = [
        {"messages": [{"role": "user", "content": "What is 2+2?"}]},
        {"messages": [{"role": "user", "content": "What is the capital of France?"}]},
        {"messages": [{"role": "user", "content": "What is the color of the sky?"}]}
    ]
    
    # Record start time for messages test
    start_time = asyncio.get_event_loop().time()
    logging.info("Starting async OpenAI calls with messages")
    
    # Create and gather all tasks
    async_tasks = []
    for task in message_tasks:
        async_tasks.append(asyncio.create_task(job.run(task)))
    
    # Wait for all tasks to complete
    results = await asyncio.gather(*async_tasks)
    
    # Calculate elapsed time
    elapsed_time = asyncio.get_event_loop().time() - start_time
    logging.info(f"All OpenAI message calls completed in {elapsed_time:.2f} seconds")
    
    # Verify message results
    assert len(results) == 3
    for result in results:
        assert isinstance(result, dict)
        assert ("response" in result) or ("error" in result), "Expected either 'response' or 'error' in result"
        if "response" in result:
            assert isinstance(result["response"], str)
            assert len(result["response"]) > 0


@pytest.mark.asyncio
async def test_rate_limiting():
    """Test that rate limiting configuration is respected."""
    # Create an OpenAIJob instance with strict rate limiting
    job = OpenAIJob(properties={
        "api": {
            "model": "gpt-4o-mini",
            "temperature": 0.7
        },
        "rate_limit": {
            "max_rate": 1,
            "time_period": 4
        }
    })
    
    # Prepare multiple tasks
    tasks = [
        {"prompt": "Count to 1"},
        {"prompt": "Count to 2"},
        {"prompt": "Count to 3"}
    ]
    
    # Record start time
    start_time = asyncio.get_event_loop().time()
    
    # Create and gather all tasks
    async_tasks: Dict[str, Any] = []
    for task in tasks:
        async_tasks.append(asyncio.create_task(job.run(task)))
    
    # Wait for all tasks to complete
    results = await asyncio.gather(*async_tasks)
    
    # Calculate elapsed time
    elapsed_time = asyncio.get_event_loop().time() - start_time
    logging.info(f"Rate limited calls completed in {elapsed_time:.2f} seconds")
    
    # Verify timing - with rate limit of 1 request per 4 seconds, 3 requests should take at least 8 seconds
    # (first request at t=0, second at t=4, third at t=8)
    assert elapsed_time >= 8.0, f"Expected at least 8 seconds, but took {elapsed_time} seconds"
    
    # Verify we got responses (either success or error) for all requests
    assert len(results) == 3
    for result in results:
        assert isinstance(result, dict)
        assert ("response" in result) or ("error" in result), "Expected either 'response' or 'error' in result"


@pytest.mark.asyncio
async def test_openrouter_api():
    """Test that OpenAIJob can make multiple asynchronous calls using OpenRouter API with deepseek model."""
    # Create an OpenAIJob instance with OpenRouter configuration
    job = OpenAIJob(properties={
        "client": {
            "base_url": "https://openrouter.ai/api/v1",
            "api_key": "OPENROUTER_API_KEY"  
        },
        "api": {
            "model": "deepseek/deepseek-chat",
            "temperature": 0.7
        }
    })
    
    # Prepare multiple tasks with proper message format
    tasks = [
        {"messages": [{"role": "user", "content": "What is 2+2?"}]},
        {"messages": [{"role": "user", "content": "What is the capital of France?"}]},
        {"messages": [{"role": "user", "content": "What is the color of the sky?"}]},
        {"prompt": "Count to 1"},
        {"prompt": "Count to 2"},
        {"prompt": "Count to 3"}
    ]
    
    # Record start time
    start_time = asyncio.get_event_loop().time()
    logging.info("Starting async OpenRouter calls")
    
    # Create and gather all tasks
    async_tasks: Dict[str, Any] = []
    for task in tasks:
        async_tasks.append(asyncio.create_task(job.run(task)))
    
    # Wait for all tasks to complete
    results = await asyncio.gather(*async_tasks)
    
    # Calculate elapsed time
    elapsed_time = asyncio.get_event_loop().time() - start_time
    logging.info(f"All OpenRouter calls completed in {elapsed_time:.2f} seconds")
    
    # Verify results
    assert len(results) == 6
    for result in results:
        assert isinstance(result, dict)
        assert ("response" in result) or ("error" in result), "Expected either 'response' or 'error' in result"



================================================
FILE: tests/test_logging_config.py
================================================
import asyncio
import os

import pytest

from flow4ai import f4a_logging as logging
from flow4ai.flowmanagerMP import FlowManagerMP
from flow4ai.job import JobABC, Task


class DebugDelayedJob(JobABC):
    def __init__(self, name: str, delay: float):
        super().__init__(name)
        self.delay = delay
        self.logger = logging.getLogger(self.__class__.__name__)

    async def run(self, task: Task) -> dict:
        """Execute a delayed job with both debug and info logging."""
        task_str = task.get('task', str(task))  # Get task string or full dict
        self.logger.debug(f"Starting task {task_str} with delay {self.delay}")
        self.logger.info(f"Processing task {task_str}")
        await asyncio.sleep(self.delay)
        self.logger.debug(f"Completed task {task_str}")
        return {"task": dict(task), "status": "complete"}

@pytest.fixture
def clear_log_file():
    """Clear the log file before each test."""
    if os.path.exists('flow4ai.log'):
        os.remove('flow4ai.log')
    yield
    if os.path.exists('flow4ai.log'):
        os.remove('flow4ai.log')
    # Clear any environment variables that might affect logging
    os.environ.pop('FLOW4AI_LOG_HANDLERS', None)
    os.environ.pop('FLOW4AI_LOG_LEVEL', None)

def test_logging_handlers_default(clear_log_file):
    """Test that by default, logs are only written to console and not to file."""
    logging.setup_logging()
    logger = logging.getLogger('test')
    logger.info('This is a test message')
    
    with open('flow4ai.log', 'r') as f:
        lines = f.readlines()
    # Should only contain the header comment
    assert len(lines) == 1, "Log file should only contain header comment"
    assert lines[0].startswith('# Flow4AI log file'), "Log file should only contain header comment"

def test_logging_handlers_console_explicit(clear_log_file):
    """Test that when FLOW4AI_LOG_HANDLERS='console', logs are not written to file."""
    os.environ['FLOW4AI_LOG_HANDLERS'] = 'console'
    logging.setup_logging()
    logger = logging.getLogger('test')
    logger.info('This is a test message')
    
    with open('flow4ai.log', 'r') as f:
        lines = f.readlines()
    # Should only contain the header comment
    assert len(lines) == 1, "Log file should only contain header comment"
    assert lines[0].startswith('# Flow4AI log file'), "Log file should only contain header comment"

def test_logging_handlers_file(clear_log_file):
    """Test that when FLOW4AI_LOG_HANDLERS includes 'file', logs are written to file."""
    os.environ['FLOW4AI_LOG_HANDLERS'] = 'console,file'
    logging.setup_logging()
    logger = logging.getLogger('test')
    test_message = 'This should be in the log file'
    logger.info(test_message)
    
    with open('flow4ai.log', 'r') as f:
        lines = f.readlines()
    # Should contain both header and log message
    assert len(lines) > 1, "Log file should contain header and log messages"
    assert lines[0].startswith('# Flow4AI log file'), "First line should be header comment"
    assert any(test_message in line for line in lines[1:]), "Test message should be in log file"

def test_logging_config_debug(clear_log_file):
    """Test that DEBUG level logging works when FLOW4AI_LOG_LEVEL is set to DEBUG."""
    os.environ['FLOW4AI_LOG_LEVEL'] = 'DEBUG'
    os.environ['FLOW4AI_LOG_HANDLERS'] = 'console,file'  # Enable file logging for this test
    logging.setup_logging()  # Reload config with new log level
    
    # Create a logger and log a debug message
    logger = logging.getLogger('test')
    logger.debug('This is a debug message')
    
    # Check if the debug message appears in the log file
    with open('flow4ai.log', 'r') as f:
        log_contents = f.read()
    assert 'This is a debug message' in log_contents

def test_logging_config_info(clear_log_file):
    """Test that DEBUG logs are filtered when FLOW4AI_LOG_LEVEL is set to INFO."""
    os.environ['FLOW4AI_LOG_LEVEL'] = 'INFO'
    os.environ['FLOW4AI_LOG_HANDLERS'] = 'console,file'  # Enable file logging for this test
    logging.setup_logging()  # Reload config with new log level
    
    # Create a logger and log messages at different levels
    logger = logging.getLogger('test')
    logger.debug('This is a debug message')
    logger.info('This is an info message')
    
    # Check that only INFO message appears in the log file
    with open('flow4ai.log', 'r') as f:
        log_contents = f.read()
    assert 'This is a debug message' not in log_contents
    assert 'This is an info message' in log_contents

def test_debug_logging_in_flowmanagerMP(clear_log_file):
    """Test that both FlowManagerMP and DebugDelayedJob debug logs are visible when FLOW4AI_LOG_LEVEL=DEBUG."""
    os.environ['FLOW4AI_LOG_LEVEL'] = 'DEBUG'
    os.environ['FLOW4AI_LOG_HANDLERS'] = 'console,file'  # Enable file logging for this test
    logging.setup_logging()  # Reload config with new log level

    # Create and run FlowManagerMP with debug-enabled job
    job = DebugDelayedJob("Debug Test Job", 0.1)
    flowmanagerMP = FlowManagerMP(job)

    # Submit tasks
    for i in range(3):
        flowmanagerMP.submit_task({'task': f'Task {i}'})  # Changed to use dict format
    flowmanagerMP.wait_for_completion()

    # Check log file contents
    with open('flow4ai.log', 'r') as f:
        log_contents = f.read()
        log_lines = log_contents.splitlines()

    # Separate FlowManagerMP and DebugDelayedJob debug logs
    flowmanagermp_debug_logs = [line for line in log_lines if '[DEBUG]' in line and 'FlowManagerMP' in line]
    delayed_job_debug_logs = [line for line in log_lines if '[DEBUG]' in line and 'DebugDelayedJob' in line]

    # Verify FlowManagerMP has debug logs
    assert len(flowmanagermp_debug_logs) > 0, "No DEBUG logs found from FlowManagerMP"
    print("\nFlowManagerMP DEBUG logs:")
    for log in flowmanagermp_debug_logs[:3]:  # Print first 3 for verification
        print(log)

    # Verify DebugDelayedJob has debug logs
    assert len(delayed_job_debug_logs) > 0, "No DEBUG logs found from DebugDelayedJob"
    print("\nDebugDelayedJob DEBUG logs:")
    for log in delayed_job_debug_logs[:3]:  # Print first 3 for verification
        print(log)

    # Verify specific debug messages from DebugDelayedJob
    delayed_job_debug_messages = [line.split('] ')[-1] for line in delayed_job_debug_logs]
    assert any('Starting task Task' in msg for msg in delayed_job_debug_messages)
    assert any('Completed task Task' in msg for msg in delayed_job_debug_messages)

    # Verify info logs are also present
    info_logs = [line for line in log_lines if '[INFO]' in line]
    assert any('Processing task Task' in line for line in info_logs)

def test_info_logging_in_flowmanagerMP(clear_log_file):
    """Test that DEBUG logs are filtered when FLOW4AI_LOG_LEVEL=INFO."""
    os.environ['FLOW4AI_LOG_LEVEL'] = 'INFO'
    os.environ['FLOW4AI_LOG_HANDLERS'] = 'console,file'  # Enable file logging for this test
    logging.setup_logging()  # Reload config with new log level

    # Create and run FlowManagerMP with debug-enabled job
    job = DebugDelayedJob("Info Test Job", 0.1)
    flowmanagerMP = FlowManagerMP(job)

    # Submit tasks
    for i in range(3):
        flowmanagerMP.submit_task({'task': f'Task {i}'})  # Changed to use dict format
    flowmanagerMP.wait_for_completion()

    # Check log file contents
    with open('flow4ai.log', 'r') as f:
        log_contents = f.read()
        log_lines = log_contents.splitlines()

    # Check for absence of DEBUG logs
    debug_logs = [line for line in log_lines if '[DEBUG]' in line]
    assert len(debug_logs) == 0, f"Found unexpected DEBUG logs:\n" + "\n".join(debug_logs[:3])

    # Verify INFO logs are present for both components
    flowmanagermp_info_logs = [line for line in log_lines if '[INFO]' in line and 'FlowManagerMP' in line]
    delayed_job_info_logs = [line for line in log_lines if '[INFO]' in line and 'DebugDelayedJob' in line]

    # Verify FlowManagerMP info logs
    assert len(flowmanagermp_info_logs) > 0, "No INFO logs found from FlowManagerMP"
    print("\nFlowManagerMP INFO logs:")
    for log in flowmanagermp_info_logs[:3]:  # Print first 3 for verification
        print(log)

    # Verify DebugDelayedJob info logs
    assert len(delayed_job_info_logs) > 0, "No INFO logs found from DebugDelayedJob"
    print("\nDebugDelayedJob INFO logs:")
    for log in delayed_job_info_logs[:3]:  # Print first 3 for verification
        print(log)

    # Verify specific info messages from DebugDelayedJob
    delayed_job_info_messages = [line.split('] ')[-1] for line in delayed_job_info_logs]
    assert any('Processing task Task' in msg for msg in delayed_job_info_messages)

if __name__ == '__main__':
    pytest.main([__file__])



================================================
FILE: tests/test_opentelemetry.py
================================================
import asyncio
import json
import os
import time

import pytest
import yaml
from opentelemetry import trace
from opentelemetry.trace import Status, StatusCode

from flow4ai import f4a_logging as logging
from flow4ai.job import JobABC, Task, job_graph_context_manager
from flow4ai.utils.otel_wrapper import TracerFactory, trace_function

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@pytest.fixture
def trace_file():
    """Fixture to provide a temporary trace file path and clean up after tests."""
    temp_file = "tests/temp_otel_trace.json"
    yield temp_file
    
    # Clean up the main trace file
    if os.path.exists(temp_file):
        os.unlink(temp_file)
    
    # Clean up any rotated files
    test_dir = os.path.dirname(temp_file)
    base_name = os.path.basename(temp_file)
    rotated_files = [f for f in os.listdir(test_dir) 
                    if f.startswith(base_name + '.')]
    
    for rotated in rotated_files:
        rotated_path = os.path.join(test_dir, rotated)
        if os.path.exists(rotated_path):
            os.unlink(rotated_path)

@pytest.fixture
def setup_file_exporter(trace_file):
    """Fixture to set up file exporter configuration"""
    config_path = "tests/otel_config.yaml"
    
    # Create and write config file
    config = {
        "exporter": "file",
        "service_name": "MyService",
        "batch_processor": {
            "max_queue_size": 1000,
            "schedule_delay_millis": 1000
        },
        "file_exporter": {
            "path": trace_file
        }
    }
    
    with open(config_path, 'w') as f:
        yaml.dump(config, f)
    
    # Reset TracerFactory state and set config path
    TracerFactory._instance = None
    TracerFactory._config = None
    
    # Set config path in environment
    os.environ['FLOW4AI_OT_CONFIG'] = config_path
    
    yield
    
    # Cleanup
    if os.path.exists(config_path):
        os.unlink(config_path)
    if 'FLOW4AI_OT_CONFIG' in os.environ:
        del os.environ['FLOW4AI_OT_CONFIG']
    TracerFactory._instance = None
    TracerFactory._config = None
    time.sleep(0.1)

def verify_trace(trace_file, expected_name=None, expected_attrs=None, expected_status=None, expected_events=None, check_all_spans=False):
    """Helper function to verify trace output"""
    # Wait longer for async operations and file writing
    time.sleep(2.0)  # Increased from 1.0 to 2.0
    # Try a few times in case the file isn't written immediately
    for _ in range(3):
        try:
            with open(trace_file, 'r') as f:
                trace_data = json.load(f)
                assert isinstance(trace_data, list), "Trace data should be a list"
                if len(trace_data) == 0:
                    time.sleep(1.0)  # Wait a bit more if the file is empty
                    continue
                
                if check_all_spans:
                    # Check all spans for the expected attributes
                    spans_to_check = trace_data
                else:
                    # Get the last span (most recent)
                    spans_to_check = [trace_data[-1]]
                
                for span in spans_to_check:
                    # Verify span structure
                    assert 'name' in span, "Span should have a name"
                    assert 'context' in span, "Span should have context"
                    assert 'trace_id' in span['context'], "Span should have trace_id"
                    assert 'span_id' in span['context'], "Span should have span_id"
                    assert 'attributes' in span, "Span should have attributes"
                    
                    # Verify expected name if provided
                    if expected_name and span['name'] == expected_name:
                        # Verify expected attributes if provided
                        if expected_attrs:
                            for key, value in expected_attrs.items():
                                if value is None:
                                    assert key not in span['attributes'], f"Span should not have {key} attribute"
                                else:
                                    if key == "function.args" and "TestJob object at" in str(value):
                                        # For TestJob object, just verify it contains the expected parts
                                        assert "TestJob object at" in span['attributes'][key], f"Expected TestJob object in {key}, got {span['attributes'][key]}"
                                        if "'test task'" in span['attributes'][key]:
                                            assert "'test task'" in span['attributes'][key], "Expected 'test task' in args"
                                    elif key == "object.fields":
                                        # Convert both strings to dicts for comparison, ignoring logger
                                        actual_fields = eval(span['attributes'][key])
                                        expected_fields = eval(value)
                                        for k, v in expected_fields.items():
                                            assert actual_fields[k] == v, f"Mismatch in object.fields for key {k}"
                                    else:
                                        assert key in span['attributes'], f"Expected attribute {key} not found"
                                        assert span['attributes'][key] == value, f"Expected {key}={value}, got {span['attributes'][key]}"
                        
                        # Verify expected status if provided
                        if expected_status:
                            assert 'status' in span, "Span should have status"
                            assert span['status']['status_code'] == expected_status['status_code'], \
                                f"Expected status code {expected_status['status_code']}, got {span['status']['status_code']}"
                            if 'description' in expected_status:
                                expected_desc = expected_status['description']
                                if 'exception_type' in expected_status:
                                    expected_desc = f"{expected_status['exception_type']}: {expected_desc}"
                                assert span['status']['description'] == expected_desc, \
                                    f"Expected status description {expected_desc}, got {span['status']['description']}"
                        
                        # Verify expected events if provided
                        if expected_events:
                            assert 'events' in span, "Span should have events"
                            for event in expected_events:
                                found = False
                                for actual_event in span['events']:
                                    if actual_event['name'] == event['name']:
                                        found = True
                                        if 'attributes' in event:
                                            for key, value in event['attributes'].items():
                                                assert key in actual_event['attributes'], f"Expected event attribute {key} not found"
                                                assert actual_event['attributes'][key] == value, \
                                                    f"Expected event {key}={value}, got {actual_event['attributes'][key]}"
                                assert found, f"Expected event {event['name']} not found"
                return  # Success, exit the function
        except (json.JSONDecodeError, FileNotFoundError, AssertionError) as e:
            if _ == 2:  # On last attempt, raise the error
                raise AssertionError(f"Trace data verification failed after retries: {str(e)}")
            time.sleep(1.0)  # Wait before retrying

def test_trace_function_detailed_off(trace_file, setup_file_exporter):
    """Test that trace_function decorator without detailed_trace doesn't record args"""
    @trace_function
    def sample_function(x, y):
        TracerFactory.trace("Inside sample_function")  # Add a trace to ensure something is captured
        return x + y
    
    result = sample_function(3, 4)
    assert result == 7
    
    # Add a delay to ensure spans are exported
    time.sleep(2)  # Increased delay to ensure export completes
    
    verify_trace(
        trace_file,
        expected_attrs={
            "trace.message": "Inside sample_function",
            "function.args": None,  # Should not have these attributes
            "function.kwargs": None,
            "object.fields": None
        },
        check_all_spans=True  # Check all spans since we have both function and message traces
    )

def test_trace_function_detailed_on(trace_file, setup_file_exporter):
    """Test that trace_function decorator with detailed_trace records args"""
    @trace_function(detailed_trace=True)
    def sample_function(x, y):
        return x + y
    
    result = sample_function(3, 4)
    assert result == 7
    
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.sample_function",
        expected_attrs={
            "function.args": "(3, 4)",
            "function.kwargs": "{}"
        }
    )

def test_tracer_factory_detailed_off(trace_file, setup_file_exporter):
    """Test that TracerFactory.trace without detailed_trace doesn't record args"""
    test_message = "Test message"
    TracerFactory.trace(test_message)
    
    verify_trace(
        trace_file,
        expected_attrs={
            "trace.message": test_message,
            "function.args": None,  # Should not have these attributes
            "function.kwargs": None,
            "object.fields": None
        }
    )

def test_tracer_factory_detailed_on(trace_file, setup_file_exporter):
    """Test that TracerFactory.trace with detailed_trace records args"""
    test_message = "Test message"
    TracerFactory.trace(test_message, detailed_trace=True)
    
    verify_trace(
        trace_file,
        expected_attrs={
            "trace.message": test_message,
            "function.args": "()",
            "function.kwargs": "{}"
        }
    )

def test_job_metaclass_tracing(trace_file, setup_file_exporter):
    """Test that JobABC metaclass properly applies detailed tracing"""
    class TestJob(JobABC):
        async def run(self, task):
            # strings are converted to dicts with {'task':,<the string>}
            # Extract just the task data we need
            task_data = task['task'] if isinstance(task, dict) else task
            return {"result": task_data}
            
    async def run_job(job_set):
        async with job_graph_context_manager(job_set):
            result = await job._execute(task)
            return result
    # Create and execute job
    job = TestJob("test")
    task = Task({"task": "test task"}, job.name)
    job_set = JobABC.job_set(job)

    result = asyncio.run(run_job(job_set))
    # Extract just the result field for comparison
    result_data = result.get("result") if isinstance(result, dict) else result
    assert result_data == "test task"
    
    verify_trace(
        trace_file,
        expected_attrs={
            "function.args": "(<test_opentelemetry.test_job_metaclass_tracing.<locals>.TestJob object at",  # Just verify it starts with this
            "function.kwargs": "{}",
            "object.fields": "{'name': 'test'}"
        }
    )

def test_file_exporter_yaml(trace_file, setup_file_exporter):
    """Test file exporter using a temporary yaml config file"""
    test_message = "Test trace message"
    TracerFactory.trace(test_message)
    
    verify_trace(
        trace_file,
        expected_attrs={"trace.message": test_message}
    )

def test_direct_trace_usage(trace_file, setup_file_exporter):
    """Test direct usage of TracerFactory.trace method"""
    test_message = "Test message"
    TracerFactory.trace(test_message)
    
    verify_trace(
        trace_file,
        expected_attrs={"trace.message": test_message}
    )

def test_trace_function_decorator(trace_file, setup_file_exporter):
    """Test that the trace_function decorator properly wraps a function"""
    @trace_function
    def sample_function(x, y):
        return x + y
    
    result = sample_function(3, 4)
    assert result == 7
    
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.sample_function",
        expected_attrs={"function.args": None, "function.kwargs": None}  # Should not have these attributes
    )

def test_class_methods(trace_file, setup_file_exporter):
    """Test that methods within a class can be traced"""
    class SampleClass:
        def __init__(self, value):
            self.value = value
        
        @trace_function(detailed_trace=True)
        def multiply(self, factor):
            return self.value * factor
        
        @trace_function
        def add(self, value):
            return self.value + value
    
    obj = SampleClass(2)
    mult_result = obj.multiply(3)
    add_result = obj.add(4)
    assert mult_result == 6
    assert add_result == 6
    
    # Verify add trace (last operation)
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.add",
        expected_attrs={
            "function.args": None,  # Should not have these attributes
            "function.kwargs": None
        }
    )

def test_nested_tracing(trace_file, setup_file_exporter):
    """Test nested tracing behavior"""
    @trace_function
    def outer_function():
        return inner_function()
    
    @trace_function
    def inner_function():
        return "result"
    
    result = outer_function()
    assert result == "result"
    
    # Verify outer function trace (last operation)
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.outer_function",
        expected_attrs={
            "function.args": None,  # Should not have these attributes
            "function.kwargs": None
        }
    )

def test_exception_handling(trace_file, setup_file_exporter):
    """Test that exceptions are properly recorded in spans"""
    @trace_function(detailed_trace=True)
    def failing_function():
        raise ValueError("Test error")
    
    with pytest.raises(ValueError) as exc_info:
        failing_function()
    assert str(exc_info.value) == "Test error"
    
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.failing_function",
        expected_attrs={
            "function.args": "()",
            "function.kwargs": "{}"
        },
        expected_status={
            "status_code": "StatusCode.ERROR",
            "description": "Test error",
            "exception_type": "ValueError"
        },
        expected_events=[{
            "name": "exception",
            "attributes": {
                "exception.type": "ValueError",
                "exception.message": "Test error"
            }
        }]
    )

def test_parent_child_functions(trace_file, setup_file_exporter):
    """Test that trace context is properly propagated"""
    @trace_function(detailed_trace=True)
    def parent_function():
        current_span = trace.get_current_span()
        current_span.set_attribute("parent.attr", "parent_value")
        return child_function()
    
    @trace_function
    def child_function():
        current_span = trace.get_current_span()
        current_span.set_attribute("child.attr", "child_value")
        return "success"
    
    result = parent_function()
    assert result == "success"
    
    # Verify parent function trace (last operation)
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.parent_function",
        expected_attrs={
            "function.args": "()",
            "function.kwargs": "{}",
            "parent.attr": "parent_value"
        }
    )

def test_trace_with_status(trace_file, setup_file_exporter):
    """Test that span status can be set"""
    @trace_function(detailed_trace=True)
    def status_function(succeed):
        current_span = trace.get_current_span()
        if not succeed:
            current_span.set_status(Status(StatusCode.ERROR, "Operation failed"))
            raise ValueError("Operation failed")
        return "success"
    
    result = status_function(True)
    assert result == "success"
    
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.status_function",
        expected_attrs={
            "function.args": "(True,)",
            "function.kwargs": "{}"
        },
        expected_status={"status_code": "StatusCode.UNSET"}
    )
    
    with pytest.raises(ValueError):
        status_function(False)
    
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.status_function",
        expected_attrs={
            "function.args": "(False,)",
            "function.kwargs": "{}"
        },
        expected_status={
            "status_code": "StatusCode.ERROR",
            "description": "Operation failed",
            "exception_type": "ValueError"
        },
        expected_events=[{
            "name": "exception",
            "attributes": {
                "exception.type": "ValueError",
                "exception.message": "Operation failed"
            }
        }]
    )

def test_trace_function_with_attributes(trace_file, setup_file_exporter):
    """Test that trace_function decorator properly handles additional attributes"""
    @trace_function(attributes={"custom.attr1": "value1", "custom.attr2": 42})
    def sample_function():
        return "success"
    
    result = sample_function()
    assert result == "success"
    
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.sample_function",
        expected_attrs={
            "custom.attr1": "value1",
            "custom.attr2": "42"  # Note: all attributes are converted to strings
        }
    )

def test_tracer_factory_with_attributes(trace_file, setup_file_exporter):
    """Test that TracerFactory.trace properly handles additional attributes"""
    test_message = "Test message with attributes"
    TracerFactory.trace(test_message, attributes={
        "environment": "test",
        "version": "1.0.0",
        "priority": 1
    })
    
    verify_trace(
        trace_file,
        expected_attrs={
            "trace.message": test_message,
            "environment": "test",
            "version": "1.0.0",
            "priority": "1"  # Note: all attributes are converted to strings
        }
    )

def test_file_exporter_with_rotation_config(trace_file, setup_file_exporter):
    """Test that file exporter works with rotation configuration."""
    # Create config with rotation settings
    config_path = "tests/otel_config.yaml"
    config = {
        "exporter": "file",
        "service_name": "MyService",
        "batch_processor": {
            "max_queue_size": 1000,
            "schedule_delay_millis": 1000
        },
        "file_exporter": {
            "path": trace_file,
            "max_size_bytes": 1048576,  # 1MB
            "rotation_time_days": 1
        }
    }
    
    with open(config_path, 'w') as f:
        yaml.dump(config, f)
    
    # Reset TracerFactory state
    TracerFactory._instance = None
    TracerFactory._config = None
    
    # Generate a trace
    test_message = "Test with rotation config"
    TracerFactory.trace(test_message)
    
    # Wait for async operations
    time.sleep(2)
    
    # Verify file exists and contains the trace
    assert os.path.exists(trace_file), "Trace file should exist"
    
    # Log the file size
    file_size = os.path.getsize(trace_file)
    logger.info("Trace file size after single trace: %d bytes", file_size)
    
    with open(trace_file, 'r') as f:
        data = json.load(f)
        assert isinstance(data, list), "Trace data should be a list"
        assert len(data) > 0, "Trace data should not be empty"
        # Log size of individual trace
        single_trace = data[0]
        trace_json = json.dumps(single_trace)
        logger.info("Size of single trace JSON: %d bytes", len(trace_json))
        assert any(trace['attributes'].get('trace.message') == test_message for trace in data), \
            "Test message should be in trace data"

def test_file_rotation_with_size_limit(trace_file, setup_file_exporter):
    """Test that file rotation occurs when size limit is exceeded."""
    # Create config with small size limit
    config_path = "tests/otel_config.yaml"
    config = {
        "exporter": "file",
        "service_name": "MyService",
        "batch_processor": {
            "max_queue_size": 1000,
            "schedule_delay_millis": 1000
        },
        "file_exporter": {
            "path": trace_file,
            "max_size_bytes": 700,  # Should rotate after two traces
            "rotation_time_days": 1
        }
    }
    
    with open(config_path, 'w') as f:
        yaml.dump(config, f)
    
    # Reset TracerFactory state
    TracerFactory._instance = None
    TracerFactory._config = None
    
    # Generate first trace
    test_message_1 = "First test trace"
    TracerFactory.trace(test_message_1)
    time.sleep(2)  # Wait for async operations
    
    # Verify first trace file exists and contains data
    assert os.path.exists(trace_file), "First trace file should exist"
    with open(trace_file, 'r') as f:
        data = json.load(f)
        assert isinstance(data, list), "Trace data should be a list"
        assert len(data) > 0, "Trace data should not be empty"
        assert any(trace['attributes'].get('trace.message') == test_message_1 for trace in data), \
            "First test message should be in trace data"
    
    # Log size after first trace
    first_size = os.path.getsize(trace_file)
    logger.info("File size after first trace: %d bytes", first_size)
    
    # Generate second trace
    test_message_2 = "Second test trace"
    TracerFactory.trace(test_message_2)
    time.sleep(2)  # Wait for async operations
    
    # Generate third trace - this should trigger rotation
    test_message_3 = "Third test trace"
    TracerFactory.trace(test_message_3)
    time.sleep(2)  # Wait for async operations
    
    # Find rotated files
    test_dir = os.path.dirname(trace_file)
    base_name = os.path.basename(trace_file)
    rotated_files = sorted([f for f in os.listdir(test_dir) 
                    if f.startswith(base_name + '.') and f != base_name])
    
    assert len(rotated_files) >= 1, f"Expected at least 1 rotated file, found {len(rotated_files)}"
    
    # Get the most recent rotated file
    rotated_file = os.path.join(test_dir, rotated_files[-1])
    
    # Verify files exist and contain correct data
    assert os.path.exists(trace_file), "Original trace file should still exist"
    assert os.path.exists(rotated_file), "Rotated file should exist"
    
    # Check contents of current file (should contain third trace)
    with open(trace_file, 'r') as f:
        current_data = json.load(f)
        assert isinstance(current_data, list), "Current file should contain a list"
        assert len(current_data) > 0, "Current file should not be empty"
        assert any(trace['attributes'].get('trace.message') == test_message_3 for trace in current_data), \
            "Third test message should be in current file"
    
    # Log final sizes
    logger.info("Original file final size: %d bytes", os.path.getsize(trace_file))
    logger.info("Most recent rotated file size: %d bytes", os.path.getsize(rotated_file))
    
    # Log all rotated files for debugging
    for rotated in rotated_files:
        full_path = os.path.join(test_dir, rotated)
        logger.info("Rotated file %s size: %d bytes", rotated, os.path.getsize(full_path))



================================================
FILE: tests/test_task_passthrough.py
================================================
import json
import os
import tempfile
from functools import partial
from typing import Any, Dict

import pytest

from flow4ai import f4a_logging as logging
from flow4ai.flowmanagerMP import FlowManagerMP
from flow4ai.job_loader import ConfigLoader

test_tasks = [
        {
            'text': 'hello world',
            'task_id': 'task_0',
            'metadata': {
                'original_text': 'hello world',
                'sequence': 0,
                'type': 'text_processing',
                'priority': 'high',
                'tags': ['greeting', 'simple'],
                'timestamp': '2025-01-14T01:42:04Z',
                'nested': {
                    'source': 'user_input',
                    'language': 'en',
                    'confidence': 0.95,
                    'metrics': {
                        'word_count': 2,
                        'char_count': 11,
                        'complexity_score': 0.1
                    }
                }
            },
            'config': {
                'preserve_case': False,
                'max_length': 100,
                'filters': ['lowercase', 'trim']
            }
        },
        {
            'text': 'testing task passthrough',
            'task_id': 'task_1',
            'metadata': {
                'original_text': 'testing task passthrough',
                'sequence': 1,
                'type': 'text_processing',
                'priority': 'medium',
                'tags': ['test', 'complex'],
                'timestamp': '2025-01-14T01:42:04Z',
                'nested': {
                    'source': 'test_suite',
                    'language': 'en',
                    'confidence': 0.99,
                    'metrics': {
                        'word_count': 3,
                        'char_count': 23,
                        'complexity_score': 0.4
                    }
                }
            },
            'config': {
                'preserve_case': True,
                'max_length': 200,
                'filters': ['punctuation', 'normalize']
            }
        },
        {
            'text': 'verify original data',
            'task_id': 'task_2',
            'metadata': {
                'original_text': 'verify original data',
                'sequence': 2,
                'type': 'text_processing',
                'priority': 'low',
                'tags': ['verification', 'data'],
                'timestamp': '2025-01-14T01:42:04Z',
                'nested': {
                    'source': 'validation',
                    'language': 'en',
                    'confidence': 0.85,
                    'metrics': {
                        'word_count': 3,
                        'char_count': 19,
                        'complexity_score': 0.6
                    }
                }
            },
            'config': {
                'preserve_case': True,
                'max_length': 150,
                'filters': ['whitespace', 'special_chars']
            }
        }
    ]

def result_collector(results_file: str, result: Dict[str, Any]) -> None:
    """Collect results by appending to a JSON file.
    
    Args:
        results_file: Path to the file storing results
        result: Result dictionary to append
    """
    logging.info(f"Result collector received: {result}")
    try:
        # Initialize file with empty list if it doesn't exist
        if not os.path.exists(results_file):
            with open(results_file, 'w') as f:
                json.dump([], f)
        
        # Read existing results
        with open(results_file, 'r') as f:
            try:
                results = json.load(f)
            except json.JSONDecodeError:
                # Handle case where file exists but is empty
                results = []
        
        # Transform result to expected format
        transformed_result = result.copy()
        transformed_result['processed_text'] = transformed_result.pop('text')
        
        # Append new result
        results.append(transformed_result)
        
        # Write back all results
        with open(results_file, 'w') as f:
            json.dump(results, f)
            
        logging.info(f"Current results count: {len(results)}")
    except Exception as e:
        logging.error(f"Error collecting result: {e}")
        raise


def test_task_passthrough():
    """Test to verify task parameters are passed through from submit_task to result processing"""
    
    # Create a temporary file for storing results
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as temp_file:
        results_file = temp_file.name
        
        try:
            # Create a partial function with our results file
            collector = partial(result_collector, results_file)
            
            # Set config directory for test
            config_dir = os.path.join(os.path.dirname(__file__), "test_configs/test_task_passthrough")
            ConfigLoader._set_directories([config_dir])
            
            # Create FlowManagerMP with parallel processing
            flowmanagerMP = FlowManagerMP(result_processing_function=collector)
            
            # Submit text processing tasks with unique identifiers
            submitted_tasks = []

            head_jobs = flowmanagerMP.get_fq_names()
            
            for i, task in enumerate(test_tasks):
                submitted_tasks.append(task)
                logging.info(f"Submitting task: {task}")
                # Use a different graph for each task
                graph_num = i + 1
                flowmanagerMP.submit_task(task, fq_name=f'text_processing_graph{graph_num}$$$$text_capitalize$$')
            
            # Mark completion and wait for processing
            flowmanagerMP.wait_for_completion()
            
            # Read results from file
            with open(results_file, 'r') as f:
                results = json.load(f)
            
            # Verify basic results count
            assert len(results) == len(test_tasks), f"Expected {len(test_tasks)} results, got {len(results)}"
            
            logging.debug("Verifying task pass-through")
            
            for result in results:
                # Verify task_pass_through exists and contains all original task data
                assert 'task_pass_through' in result, "Result missing task_pass_through field"
                task_pass_through = result['task_pass_through']
                
                assert 'task_id' in task_pass_through, "task_id not passed through"
                assert 'metadata' in task_pass_through, "metadata not passed through"
                
                matching_task = next(
                    (t for t in submitted_tasks if t['task_id'] == task_pass_through['task_id']), 
                    None
                )
                assert matching_task is not None, f"No matching submitted task for {task_pass_through['task_id']}"
                
                # Verify all metadata fields are preserved exactly
                assert task_pass_through['metadata'] == matching_task['metadata'], \
                    f"Metadata mismatch for task {task_pass_through['task_id']}"
                
                # Verify nested structures are preserved
                assert task_pass_through['metadata']['nested'] == matching_task['metadata']['nested'], \
                    f"Nested metadata mismatch for task {task_pass_through['task_id']}"
                
                # Verify arrays are preserved
                assert task_pass_through['metadata']['tags'] == matching_task['metadata']['tags'], \
                    f"Tags mismatch for task {task_pass_through['task_id']}"
                
                # Verify metrics are preserved
                assert task_pass_through['metadata']['nested']['metrics'] == matching_task['metadata']['nested']['metrics'], \
                    f"Metrics mismatch for task {task_pass_through['task_id']}"
                
                # Verify config is passed through if present
                if 'config' in matching_task:
                    assert 'config' in task_pass_through, "Config not passed through"
                    assert task_pass_through['config'] == matching_task['config'], \
                        f"Config mismatch for task {task_pass_through['task_id']}"
                
                # Verify processed text field exists
                assert 'processed_text' in result, "Result missing processed_text field"
                
        finally:
            # Clean up temporary file
            try:
                os.unlink(results_file)
            except Exception as e:
                logging.warning(f"Failed to delete temporary file {results_file}: {e}")


def test_multiple_task_submissions():
    """Test submitting each task through each head job multiple times"""
    
    # Create a temporary file for storing results
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as temp_file:
        results_file = temp_file.name
        
        try:
            # Create a partial function with our results file
            collector = partial(result_collector, results_file)
            
            # Set config directory for test
            config_dir = os.path.join(os.path.dirname(__file__), "test_configs/test_task_passthrough")
            ConfigLoader._set_directories([config_dir])
            
            # Create FlowManagerMP with parallel processing
            flowmanagerMP = FlowManagerMP(result_processing_function=collector)
            
            # Get all head jobs
            head_jobs = flowmanagerMP.get_fq_names()
            
            # Submit text processing tasks with unique identifiers
            num_iterations = 2
            submitted_tasks = []

            # Submit each task through each head job multiple times
            for task in test_tasks:
                for head_job in head_jobs:
                    for iteration in range(num_iterations):
                        # Create a new task with unique ID for this iteration
                        task_copy = task.copy()
                        task_copy['task_id'] = f"{task['task_id']}_head{head_job}_iter{iteration}"
                        submitted_tasks.append(task_copy)
                        logging.info(f"Submitting task {task_copy['task_id']} to head job {head_job}")
                        flowmanagerMP.submit_task(task_copy, fq_name=head_job)
            
            # Mark completion and wait for processing
            flowmanagerMP.wait_for_completion()
            
            # Read results from file
            with open(results_file, 'r') as f:
                results = json.load(f)
            
            # Calculate expected number of tasks
            expected_tasks = len(test_tasks) * len(head_jobs) * num_iterations
            assert len(results) == expected_tasks, f"Expected {expected_tasks} results, got {len(results)}"
            
            logging.debug("Verifying task pass-through for multiple submissions")
            
            for result in results:
                # Verify task_pass_through exists and contains all original task data
                assert 'task_pass_through' in result, "Result missing task_pass_through field"
                task_pass_through = result['task_pass_through']
                
                assert 'task_id' in task_pass_through, "task_id not passed through"
                assert 'metadata' in task_pass_through, "metadata not passed through"
                
                matching_task = next(
                    (t for t in submitted_tasks if t['task_id'] == task_pass_through['task_id']), 
                    None
                )
                assert matching_task is not None, f"No matching submitted task for {task_pass_through['task_id']}"
                
                # Verify metadata fields are preserved exactly
                assert task_pass_through['metadata'] == matching_task['metadata'], \
                    f"Metadata mismatch for task {task_pass_through['task_id']}"
                
                # Verify processed text field exists
                assert 'processed_text' in result, "Result missing processed_text field"
                
        finally:
            # Clean up temporary file
            try:
                os.unlink(results_file)
            except:
                pass



================================================
FILE: tests/test_zzz_final_cleanup.py
================================================
"""
Final cleanup module that runs after all other tests to ensure test artifacts are removed.
The 'zzz' prefix in the filename ensures this runs last in alphabetical order.
"""
import os
import time

import pytest

from flow4ai import f4a_logging as logging

logger = logging.getLogger(__name__)

def test_zzz_ensure_all_trace_files_cleaned():
    """
    This test ensures all trace files are properly cleaned up after all other tests have run.
    The 'zzz' prefix in the test name ensures it runs last even within this file.
    """
    logger.info("Running final OpenTelemetry trace file cleanup")
    
    # First delay to allow any pending async operations to complete
    time.sleep(2.0)
    
    # Perform direct cleanup of trace files
    test_dir = "tests"
    try:
        # Look for any temp_otel_trace files or rotated versions
        otel_files = [f for f in os.listdir(test_dir) 
                    if f.startswith("temp_otel_trace")]
        
        logger.info(f"Found {len(otel_files)} trace files to remove")
        
        for trace_file in otel_files:
            file_path = os.path.join(test_dir, trace_file)
            try:
                if os.path.exists(file_path):
                    os.unlink(file_path)
                    logger.info(f"Removed trace file: {trace_file}")
            except (FileNotFoundError, PermissionError) as e:
                logger.error(f"Error during cleanup of {trace_file}: {e}")
    except Exception as e:
        logger.error(f"Error during cleanup: {e}")
    
    # Verify all files were actually removed
    remaining = [f for f in os.listdir(test_dir) if f.startswith("temp_otel_trace")]
    
    if remaining:
        logger.warning(f"Found {len(remaining)} trace files after cleanup: {remaining}")
        # Do a second attempt with longer delay
        logger.info("Attempting final cleanup again with longer delay")
        time.sleep(5.0)
        
        # Second cleanup attempt for remaining files
        for trace_file in remaining:
            file_path = os.path.join(test_dir, trace_file)
            try:
                if os.path.exists(file_path):
                    os.unlink(file_path)
                    logger.info(f"Removed trace file in second pass: {trace_file}")
            except (FileNotFoundError, PermissionError) as e:
                logger.error(f"Error during second cleanup of {trace_file}: {e}")
        
        # Final verification
        remaining = [f for f in os.listdir(test_dir) if f.startswith("temp_otel_trace")]
        assert len(remaining) == 0, f"Cleanup failed, {len(remaining)} trace files still remain: {remaining}"
    
    logger.info("All OpenTelemetry trace files successfully cleaned up")



================================================
FILE: tests/test_configs/test_concurrency_by_returns/graphs.yaml
================================================
test_graph:
  A: {next: [B,C,D]}
  B: {next: [C]}
  C: {next: [E]}
  D: {next: [F]}
  E: {next: [G]}
  F: {next: [G]}
  G: {next: []}


================================================
FILE: tests/test_configs/test_concurrency_by_returns/jobs.yaml
================================================
A:
  type: ConcurrencyTestJob
  properties:
    test_inputs: []
    valid_return: "A"

B:
  type: ConcurrencyTestJob
  properties:
    test_inputs: [A]
    valid_return: "A.B"

C:
  type: ConcurrencyTestJob
  properties:
    test_inputs: [A,B]
    valid_return: "A.A.B.C"

D:
  type: ConcurrencyTestJob
  properties:
    test_inputs: [A]
    valid_return: "A.D"

E:
  type: ConcurrencyTestJob
  properties:
    test_inputs: [C]
    valid_return: "A.A.B.C.E"

F:
  type: ConcurrencyTestJob
  properties:
    test_inputs: [D]
    valid_return: "A.D.F"

G:
  type: ConcurrencyTestJob
  properties:
    test_inputs: [E,F]
    valid_return: "A.A.B.C.E.A.D.F.G"



================================================
FILE: tests/test_configs/test_concurrency_by_returns/jobs/concurrent_jobs.py
================================================
import asyncio
from typing import Any, Dict, Optional

from flow4ai import f4a_logging as logging
from flow4ai.job import JobABC


class ConcurrencyTestJob(JobABC):
    def __init__(self, name: Optional[str] = None, properties: Dict[str, Any] = {}):
        super().__init__(name, properties)
        self.test_inputs: list = properties.get("test_inputs", [])
        self.valid_return: str = properties.get("valid_return", "")

    def get_random_sleep_duration(self) -> float:
        """Generate a random sleep duration between 0 and 1 second.
        
        The duration will be randomly selected from one of 10 equal deciles (0.0-0.1, 0.1-0.2, ..., 0.9-1.0)
        with equal probability for each decile.
        
        Returns:
            float: Sleep duration in seconds
        """
        import random
        decile = random.randint(0, 5)  # Choose a decile (0-5)
        base = decile * 0.1  # Base value for the chosen decile
        offset = random.random() * 0.1  # Random offset within the decile
        return base + offset

    async def run(self, task):
        await asyncio.sleep(self.get_random_sleep_duration())
        received_data = []
        if not self.is_head_job(): 
            for short_job_name in self.test_inputs: # self.test_input is guaranteed to be in the order it is loaded in
                data = self.get_input_from(task,short_job_name)
                if not data:
                    logging.error(f"Failed to get input from {short_job_name}")
                    raise Exception(f"Job {self.name} failed to get input from {short_job_name}")
                received_data.append(data['result']) # return from run() from parent job is a str it is converted to dict.
        #task = self.get_task(task)
        await asyncio.sleep(self.get_random_sleep_duration())
        short_job_name = self.parse_job_name(self.name)
        received_data.append(f"{short_job_name}")
        return_data = ".".join(received_data)
        await asyncio.sleep(self.get_random_sleep_duration())
        if self.valid_return and return_data != self.valid_return:
            logging.error(f"Invalid return data: {return_data}")
            raise Exception(f"Job {self.name} returned invalid data: {return_data}, should have been {self.valid_return}")

        logging.info(f"Job {self.name} returned: {return_data} for task {task['task']}")
        return return_data



================================================
FILE: tests/test_configs/test_jc_config/graphs.yaml
================================================
four_stage_parameterized:
    read_file:
      next:
        - ask_llm
        - save_to_db
    ask_llm:
      next:
        - save_to_db
        - summarize
    save_to_db:
      next:
        - summarize
    summarize:
      next: []

three_stage:
    ask_llm_mini:
      next:
        - save_to_db2
        - summarize
    save_to_db2:
      next:
        - summarize
    summarize:
      next: []

three_stage_reasoning:
    ask_llm_reasoning:
      next:
        - save_to_db2
        - summarize
    save_to_db2:
      next:
        - summarize
    summarize:
      next: []


================================================
FILE: tests/test_configs/test_jc_config/jobs.yaml
================================================
ask_llm:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
    rate_limit:
      max_rate: 1
      time_period: 4

read_file:
  type: MockFileReadJob
  properties:
    filepath: "$filepath"

save_to_db:
  type: MockDatabaseWriteJob
  properties:
    database_url: "$database_url"
    table_name: "$table_name"

summarize:
  type: DummyJob
  properties: {}

ask_llm_mini:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
    rate_limit:
      max_rate: 1
      time_period: 4

ask_llm_reasoning:
  type: OpenAIJob
  properties:
    api:
      model: "o1-mini"
      temperature: 0.7
    rate_limit:
      max_rate: 1
      time_period: 4

save_to_db2:
  type: MockDatabaseWriteJob
  properties:
    database_url: "sqlite://user2:pass2@db2/mydb"
    table_name: "table_b"



================================================
FILE: tests/test_configs/test_jc_config/parameters.yaml
================================================
four_stage_parameterized: 
    params1:
        ask_llm:
          - model: "gpt-4o"
        read_file:
          - filepath: "./file1.txt"
        save_to_db:
          - database_url: "postgres://user1:pass1@db1/mydb"
            table_name: "table_a"
    params2:
        ask_llm:
          - model: "gpt-4o-mini"
        read_file:
          - filepath: "./file2.txt"
        save_to_db:
          - database_url: "sqlite://user2:pass2@db2/mydb"
            table_name: "table_b"

three_stage:
    params1:
        ask_llm_mini:
          - model: "gpt-4o-mini"


================================================
FILE: tests/test_configs/test_jc_config/jobs/mock_jobs.py
================================================
from typing import Any, Dict

from flow4ai.job import JobABC


class MockFileReadJob(JobABC):
    async def run(self, task: Dict[str, Any]) -> Any:
        inputs = self.get_inputs()
        print(f"\nFileReadJob '{self.name}' reading file: {self.properties.get('filepath')}, inputs:{inputs}")
        file_content = f"Contents of {self.properties.get('filepath')}"
        return {self.name:file_content}

class MockDatabaseWriteJob(JobABC):
    async def run(self, task: Dict[str, Any]) -> Any:
        inputs = self.get_inputs()
        print(f"\nDatabaseWriteJob '{self.name}' writing to: {self.properties.get('database_url')}, table: {self.properties.get('table_name')}, inputs:{inputs}")
        result = f"Data written to table {self.properties.get('table_name')} on db {self.properties.get('database_url')} from {inputs}"
        return {self.name:result}
        
class DummyJob(JobABC):
    async def run(self, task: Dict[str, Any]) -> Any:
        inputs = self.get_inputs()
        print(f"\ndummy_job '{self.name}' with properties: {self.properties}, inputs: {inputs}")
        return {self.name:f"Ran function '{self.name}' with {inputs}"}
    


================================================
FILE: tests/test_configs/test_jc_config/jobs/mock_jobs2.py
================================================
from typing import Any, Dict

from flow4ai.job import JobABC


class MockJob(JobABC):
    async def run(self, task: Dict[str, Any]) -> Any:
        inputs = self.get_inputs()
        print(f"\nMockJob '{self.name}' with properties: {self.properties}, inputs: {inputs}")
        return {self.name:f"Ran function '{self.name}' with {inputs}"}


================================================
FILE: tests/test_configs/test_jc_config_all/flow4ai_all.yaml
================================================
graphs:
  four_stage_parameterized:
    read_file:
      next:
        - ask_llm
        - save_to_db
    ask_llm:
      next:
        - save_to_db
        - summarize
    save_to_db:
      next:
        - summarize
    summarize:
      next: []

  three_stage:
    ask_llm_mini:
      next:
        - save_to_db2
        - summarize
    save_to_db2:
      next:
        - summarize
    summarize:
      next: []

jobs:
  ask_llm:
    type: OpenAIJob
    properties:
      api:
        model: "$model"
        temperature: 0.7
      rate_limit:
        max_rate: 1
        time_period: 4

  read_file:
    type: MockFileReadJob
    properties:
      filepath: "$filepath"

  save_to_db:
    type: MockDatabaseWriteJob
    properties:
      database_url: "$database_url"
      table_name: "$table_name"

  summarize:
    type: DummyJob
    properties: {}

  ask_llm_mini:
    type: OpenAIJob
    properties:
      api:
        model: "$model"
        temperature: 0.7
      rate_limit:
        max_rate: 1
        time_period: 4

  save_to_db2:
    type: MockDatabaseWriteJob
    properties:
      database_url: "sqlite://user2:pass2@db2/mydb"
      table_name: "table_b"

parameters:
  four_stage_parameterized: 
    params1:
        ask_llm:
          - model: "gpt-4o"
        read_file:
          - filepath: "./file1.txt"
        save_to_db:
          - database_url: "postgres://user1:pass1@db1/mydb"
            table_name: "table_a"
    params2:
        ask_llm:
          - model: "gpt-4o-mini"
        read_file:
          - filepath: "./file2.txt"
        save_to_db:
          - database_url: "sqlite://user2:pass2@db2/mydb"
            table_name: "table_b"
  three_stage:
    params1:
        ask_llm_mini:
          - model: "gpt-4o-mini"


================================================
FILE: tests/test_configs/test_jc_config_all/jobs/mock_jobs.py
================================================
from typing import Any, Dict

from flow4ai.job import JobABC


class MockFileReadJob(JobABC):
    async def run(self, task: Dict[str, Any]) -> Any:
        inputs = self.get_inputs()
        print(f"\nFileReadJob '{self.name}' reading file: {self.properties.get('filepath')}, inputs:{inputs}")
        file_content = f"Contents of {self.properties.get('filepath')}"
        return {self.name:file_content}

class MockDatabaseWriteJob(JobABC):
    async def run(self, task: Dict[str, Any]) -> Any:
        inputs = self.get_inputs()
        print(f"\nDatabaseWriteJob '{self.name}' writing to: {self.properties.get('database_url')}, table: {self.properties.get('table_name')}, inputs:{inputs}")
        result = f"Data written to table {self.properties.get('table_name')} on db {self.properties.get('database_url')} from {inputs}"
        return {self.name:result}
        
class DummyJob(JobABC):
    async def run(self, task: Dict[str, Any]) -> Any:
        inputs = self.get_inputs()
        print(f"\ndummy_job '{self.name}' with properties: {self.properties}, inputs: {inputs}")
        return {self.name:f"Ran function '{self.name}' with {inputs}"}
    


================================================
FILE: tests/test_configs/test_jc_config_all/jobs/mock_jobs2.py
================================================
from typing import Any, Dict

from flow4ai.job import JobABC


class MockJob(JobABC):
    async def run(self, task: Dict[str, Any]) -> Any:
        inputs = self.get_inputs()
        print(f"\nMockJob '{self.name}' with properties: {self.properties}, inputs: {inputs}")
        return {self.name:f"Ran function '{self.name}' with {inputs}"}


================================================
FILE: tests/test_configs/test_jc_config_invalid/graphs.yaml
================================================
four_stage_parameterized:
    read_file:
      next:
        - nonexistent_job  # This job doesn't exist in jobs.yaml
        - save_to_db
    save_to_db:
      next:
        - another_missing_job  # This job doesn't exist either
    summarize:
      next: []

three_stage:
    ask_llm_mini:
      next:
        - undefined_job  # Another missing job
    save_to_db2:
      next:
        - summarize
    summarize:
      next: []



================================================
FILE: tests/test_configs/test_jc_config_invalid/jobs.yaml
================================================
save_to_db:
  type: MockDatabaseWriteJob
  properties:
    connection_string: "mock://localhost:1234"

save_to_db2:
  type: MockDatabaseWriteJob
  properties:
    connection_string: "mock://localhost:5678"

read_file:
  type: MockFileReadJob
  properties:
    file_path: "/path/to/file"

summarize:
  type: MockJob
  properties:
    test_param: "test_value"



================================================
FILE: tests/test_configs/test_jc_config_invalid/parameters.yaml
================================================
default_parameters:
  param1: value1
  param2: value2



================================================
FILE: tests/test_configs/test_jc_config_invalid_parameters/graphs.yaml
================================================
four_stage_parameterized:
    read_file:
      next:
        - ask_llm
        - save_to_db
    ask_llm:
      next:
        - save_to_db
        - summarize
    save_to_db:
      next:
        - summarize
    summarize:
      next: []

three_stage_parameterized:  # This graph has parameterized jobs but no parameters entry
    ask_llm_mini:
      next:
        - save_to_db2
        - summarize
    save_to_db2:
      next:
        - summarize
    summarize:
      next: []



================================================
FILE: tests/test_configs/test_jc_config_invalid_parameters/jobs.yaml
================================================
ask_llm:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
    rate_limit:
      max_rate: 1
      time_period: 4

read_file:
  type: MockFileReadJob
  properties:
    filepath: "$filepath"

save_to_db:
  type: MockDatabaseWriteJob
  properties:
    database_url: "$database_url"
    table_name: "$table_name"

summarize:
  type: DummyJob
  properties: {}

ask_llm_mini:
  type: OpenAIJob
  properties:
    api:
      model: "$model"  # This job is parameterized but its graph has no parameters entry
      temperature: 0.7
    rate_limit:
      max_rate: 1
      time_period: 4

save_to_db2:
  type: MockDatabaseWriteJob
  properties:
    database_url: "$database_url"  # This job is parameterized but its graph has no parameters entry
    table_name: "$table_name"



================================================
FILE: tests/test_configs/test_jc_config_invalid_parameters/parameters.yaml
================================================
four_stage_parameterized:  # Missing parameters for some jobs
    params1:  # Missing read_file parameters
        ask_llm:
          - model: "gpt-4o"
        save_to_db:
          - database_url: "postgres://user1:pass1@db1/mydb"
            table_name: "table_a"
    params2:
        ask_llm:
          - model: "gpt-4o-mini"
        save_to_db:
          - database_url: "sqlite://user2:pass2@db2/mydb"
            table_name: "table_b"



================================================
FILE: tests/test_configs/test_malformed_config/graphs.yaml
================================================
single_stage:
    ask_llm:
      next:[]



================================================
FILE: tests/test_configs/test_malformed_config/jobs.yaml
================================================
ask_llm:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
    rate_limit:
      max_rate: 10
      time_period: 1



================================================
FILE: tests/test_configs/test_malformed_config/parameters.yaml
================================================
single_stage:
    params1:
        ask_llm:
          - model: "gpt-4o"


================================================
FILE: tests/test_configs/test_malformed_config_jobs/graphs.yaml
================================================
single_stage:
    ask_llm:
      next: []



================================================
FILE: tests/test_configs/test_malformed_config_jobs/jobs.yaml
================================================
ask_llm:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
    rate_limit:
      max_rate: 10
      time_period



================================================
FILE: tests/test_configs/test_malformed_config_jobs/parameters.yaml
================================================
single_stage:
    params1:
        ask_llm:
          - model: "gpt-4o"


================================================
FILE: tests/test_configs/test_malformed_config_params/graphs.yaml
================================================
single_stage:
    ask_llm:
      next:[] # this is deliberately malformed, it should error.



================================================
FILE: tests/test_configs/test_malformed_config_params/jobs.yaml
================================================
ask_llm:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
    rate_limit:
      max_rate: 10
      time_period: 1



================================================
FILE: tests/test_configs/test_malformed_config_params/parameters.yaml
================================================
single_stage:
    params1:
        ask_llm:
          - model: "gpt-4o"
          key: 



================================================
FILE: tests/test_configs/test_multiple_heads/graphs.yaml
================================================
multi_head_demo:
    head_job_alpha:  # Head 1
        next: 
            - common_processor
    head_job_beta:  # Head 2
        next:
            - common_processor
    common_processor:  # Shared node
        next:
            - finalizer_job
    finalizer_job:  # Single tail
        next: []



================================================
FILE: tests/test_configs/test_multiple_heads/jobs.yaml
================================================
head_job_alpha:
    type: DataIngestionJob
    properties:
        source: "/data/source_a"
        batch: 500

head_job_beta:
    type: DataSamplingJob 
    properties:
        source: "/data/source_b"
        rate: 0.25

common_processor:
    type: ModelProcessorJob
    properties:
        model: "ensemble_v3"
        validation_mode: "$validation"

finalizer_job:
    type: ResultArchiverJob
    properties:
        storage_url: "s3://results-bucket/prod"



================================================
FILE: tests/test_configs/test_multiple_heads/parameters.yaml
================================================
multi_head_demo:
    params1:
        common_processor:
            - validation: "strict"
    params2:
        common_processor:
            - validation: "loose"



================================================
FILE: tests/test_configs/test_multiple_heads/jobs/mock_jobs.py
================================================
from typing import Any, Dict

from flow4ai import f4a_logging as logging
from flow4ai.job import JobABC

logger = logging.getLogger(__name__)

class DataIngestionJob(JobABC):
    """Job for ingesting data from a source."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Processing data from {self.properties.get('source')} with batch size {self.properties.get('batch')}")
        result = {
            "source": self.properties.get('source'),
            "batch": self.properties.get('batch'),
            "records_processed": 1000,
            "status": "success"
        }
        logger.info(f"{self.name}: Completed data ingestion with result: {result}")
        return result

class DataSamplingJob(JobABC):
    """Job for sampling data from a source."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Sampling data from {self.properties.get('source')} at rate {self.properties.get('rate')}")
        result = {
            "source": self.properties.get('source'),
            "rate": self.properties.get('rate'),
            "samples_collected": 250,
            "status": "success"
        }
        logger.info(f"{self.name}: Completed data sampling with result: {result}")
        return result

class ModelProcessorJob(JobABC):
    """Job for processing data through a model."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Processing with model {self.properties.get('model')} in {self.properties.get('validation_mode')} mode")
        # Include inputs from upstream jobs in the result
        result = {
            "model": self.properties.get('model'),
            "validation": self.properties.get('validation_mode'),
            "accuracy": 0.92,
            "status": "success"
        }
        logger.info(f"{self.name}: Completed model processing with result: {result}")
        return result

class ResultArchiverJob(JobABC):
    """Job for archiving results."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Archiving results to {self.properties.get('storage_url')}")
        result = {
            "storage_url": self.properties.get('storage_url'),
            "archived_items": 3,
            "timestamp": "2025-02-26T19:30:00Z",
            "status": "success"
        }
        logger.info(f"{self.name}: Completed archiving with result: {result}")
        return result



================================================
FILE: tests/test_configs/test_multiple_tails/graphs.yaml
================================================
multi_tail_demo:
    source_job:  # Single head
        next: 
            - processor_alpha
            - processor_beta
    processor_alpha:  # Tail 1
        next: []
    processor_beta:  # Tail 2
        next: []



================================================
FILE: tests/test_configs/test_multiple_tails/jobs.yaml
================================================
source_job:
    type: DataIngestionJob
    properties:
        source: "/data/source_main"
        batch: 500

processor_alpha:
    type: ModelProcessorJob
    properties:
        model: "ensemble_v3"
        validation_mode: "strict"

processor_beta:
    type: ResultArchiverJob
    properties:
        storage_url: "s3://results-bucket/prod"



================================================
FILE: tests/test_configs/test_multiple_tails/jobs/mock_jobs.py
================================================
from typing import Any, Dict

from flow4ai import f4a_logging as logging
from flow4ai.job import JobABC

logger = logging.getLogger(__name__)

class DataIngestionJob(JobABC):
    """Job for ingesting data from a source."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Processing data from {self.properties.get('source')} with batch size {self.properties.get('batch')}")
        result = {
            "source": self.properties.get('source'),
            "batch": self.properties.get('batch'),
            "records_processed": 1000,
            "status": "success"
        }
        logger.info(f"{self.name}: Completed data ingestion with result: {result}")
        return result

class DataSamplingJob(JobABC):
    """Job for sampling data from a source."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Sampling data from {self.properties.get('source')} at rate {self.properties.get('rate')}")
        result = {
            "source": self.properties.get('source'),
            "rate": self.properties.get('rate'),
            "samples_collected": 250,
            "status": "success"
        }
        logger.info(f"{self.name}: Completed data sampling with result: {result}")
        return result

class ModelProcessorJob(JobABC):
    """Job for processing data through a model."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Processing with model {self.properties.get('model')} in {self.properties.get('validation_mode')} mode")
        # Include inputs from upstream jobs in the result
        result = {
            "model": self.properties.get('model'),
            "validation": self.properties.get('validation_mode'),
            "accuracy": 0.92,
            "status": "success"
        }
        logger.info(f"{self.name}: Completed model processing with result: {result}")
        return result

class ResultArchiverJob(JobABC):
    """Job for archiving results."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Archiving results to {self.properties.get('storage_url')}")
        result = {
            "storage_url": self.properties.get('storage_url'),
            "archived_items": 3,
            "timestamp": "2025-02-26T19:30:00Z",
            "status": "success"
        }
        logger.info(f"{self.name}: Completed archiving with result: {result}")
        return result



================================================
FILE: tests/test_configs/test_multiple_tails2/graphs.yaml
================================================
multi_tail_demo:
    source_job:  # Single head
        next: 
            - processor_alpha
            - processor_beta
    processor_alpha:  # Tail 1
        next: []
    processor_beta:  # Tail 2
        next: []



================================================
FILE: tests/test_configs/test_multiple_tails2/jobs.yaml
================================================
source_job:
    type: DataIngestionJob
    properties:
        source: "/data/source_main"
        batch: 500

processor_alpha:
    type: ModelProcessorJob
    properties:
        model: "ensemble_v3"
        validation_mode: "strict"

processor_beta:
    type: ResultArchiverJob
    properties:
        storage_url: "$url"



================================================
FILE: tests/test_configs/test_multiple_tails2/parameters.yaml
================================================
multi_tail_demo:
    params1:
        processor_beta:
            - url: "s3://results-bucket/prod"
    params2:
        processor_beta:
            - url: "s3://results-bucket/prod"



================================================
FILE: tests/test_configs/test_multiple_tails2/jobs/mock_jobs.py
================================================
from typing import Any, Dict

from flow4ai import f4a_logging as logging
from flow4ai.job import JobABC

logger = logging.getLogger(__name__)

class DataIngestionJob(JobABC):
    """Job for ingesting data from a source."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Processing data from {self.properties.get('source')} with batch size {self.properties.get('batch')}")
        result = {
            "source": self.properties.get('source'),
            "batch": self.properties.get('batch'),
            "records_processed": 1000,
            "status": "success"
        }
        logger.info(f"{self.name}: Completed data ingestion with result: {result}")
        return result

class DataSamplingJob(JobABC):
    """Job for sampling data from a source."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Sampling data from {self.properties.get('source')} at rate {self.properties.get('rate')}")
        result = {
            "source": self.properties.get('source'),
            "rate": self.properties.get('rate'),
            "samples_collected": 250,
            "status": "success"
        }
        logger.info(f"{self.name}: Completed data sampling with result: {result}")
        return result

class ModelProcessorJob(JobABC):
    """Job for processing data through a model."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Processing with model {self.properties.get('model')} in {self.properties.get('validation_mode')} mode")
        # Include inputs from upstream jobs in the result
        result = {
            "model": self.properties.get('model'),
            "validation": self.properties.get('validation_mode'),
            "accuracy": 0.92,
            "status": "success"
        }
        logger.info(f"{self.name}: Completed model processing with result: {result}")
        return result

class ResultArchiverJob(JobABC):
    """Job for archiving results."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Archiving results to {self.properties.get('storage_url')}")
        result = {
            "storage_url": self.properties.get('storage_url'),
            "archived_items": 3,
            "timestamp": "2025-02-26T19:30:00Z",
            "status": "success"
        }
        logger.info(f"{self.name}: Completed archiving with result: {result}")
        return result



================================================
FILE: tests/test_configs/test_pydantic_config/graphs.yaml
================================================
simple_pydantic:
    ask_llm:
      next: []




================================================
FILE: tests/test_configs/test_pydantic_config/jobs.yaml
================================================
ask_llm:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
      response_format: "UserProfile"
    rate_limit:
      max_rate: 4
      time_period: 10



================================================
FILE: tests/test_configs/test_pydantic_config/parameters.yaml
================================================
simple_pydantic: 
    params1:
        ask_llm:
          - model: "gpt-4o"
    params2:
        ask_llm:
          - model: "gpt-4o-mini"


================================================
FILE: tests/test_configs/test_pydantic_config/jobs/mock_jobs.py
================================================
from typing import Any, Dict

from flow4ai.job import JobABC


class MockFileReadJob(JobABC):
    async def run(self, task: Dict[str, Any]) -> Any:
        inputs = self.get_inputs()
        print(f"\nFileReadJob '{self.name}' reading file: {self.properties.get('filepath')}, inputs:{inputs}")
        file_content = f"Contents of {self.properties.get('filepath')}"
        return {self.name:file_content}

class MockDatabaseWriteJob(JobABC):
    async def run(self, task: Dict[str, Any]) -> Any:
        inputs = self.get_inputs()
        print(f"\nDatabaseWriteJob '{self.name}' writing to: {self.properties.get('database_url')}, table: {self.properties.get('table_name')}, inputs:{inputs}")
        result = f"Data written to table {self.properties.get('table_name')} on db {self.properties.get('database_url')} from {inputs}"
        return {self.name:result}
        
class DummyJob(JobABC):
    async def run(self, task: Dict[str, Any]) -> Any:
        inputs = self.get_inputs()
        print(f"\ndummy_job '{self.name}' with properties: {self.properties}, inputs: {inputs}")
        return {self.name:f"Ran function '{self.name}' with {inputs}"}
    


================================================
FILE: tests/test_configs/test_pydantic_config/jobs/mock_jobs2.py
================================================
from typing import Any, Dict

from flow4ai.job import JobABC


class MockJob(JobABC):
    async def run(self, task: Dict[str, Any]) -> Any:
        inputs = self.get_inputs()
        print(f"\nMockJob '{self.name}' with properties: {self.properties}, inputs: {inputs}")
        return {self.name:f"Ran function '{self.name}' with {inputs}"}


================================================
FILE: tests/test_configs/test_pydantic_config/jobs/pydantic.py
================================================
from pydantic import BaseModel
from typing import List, Optional

class UserProfile(BaseModel):
    """User profile data model"""
    username: str
    email: str
    full_name: str
    age: Optional[int] = None

class JobMetadata(BaseModel):
    """Job metadata model"""
    job_id: str
    status: str
    created_at: str
    updated_at: Optional[str] = None
    tags: List[str] = []

class TaskConfig(BaseModel):
    """Task configuration model"""
    task_name: str
    priority: int
    max_retries: int = 3
    timeout_seconds: int = 300
    dependencies: List[str] = []



================================================
FILE: tests/test_configs/test_save_result/graphs.yaml
================================================
four_stage_parameterized:
    read_file:
      next:
        - ask_llm
        - save_to_db
    ask_llm:
      next:
        - save_to_db
        - summarize
    save_to_db:
      next:
        - summarize
    summarize:
      next: []

three_stage:
    ask_llm_mini:
      next:
        - save_to_db
        - summarize
    save_to_db:
      next:
        - summarize
    summarize:
      next: []

three_stage_reasoning:
    ask_llm_reasoning:
      next:
        - save_to_db2
        - summarize
    save_to_db2:
      next:
        - summarize
    summarize:
      next: []


================================================
FILE: tests/test_configs/test_save_result/jobs.yaml
================================================
ask_llm:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
    rate_limit:
      max_rate: 1
      time_period: 4

read_file:
  type: MockFileReadJob
  properties:
    filepath: "$filepath"

save_to_db:
  type: MockDatabaseWriteJob
  properties:
    save_result: "$save_result"
    database_url: "$database_url"
    table_name: "$table_name"

summarize:
  type: DummyJob
  properties: {}

ask_llm_mini:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
    rate_limit:
      max_rate: 1
      time_period: 4

ask_llm_reasoning:
  type: OpenAIJob
  properties:
    api:
      model: "o1-mini"
      temperature: 0.7
    rate_limit:
      max_rate: 1
      time_period: 4

save_to_db2:
  type: MockDatabaseWriteJob
  properties:
    save_result: True
    database_url: "sqlite://user2:pass2@db2/mydb"
    table_name: "table_b"



================================================
FILE: tests/test_configs/test_save_result/parameters.yaml
================================================
four_stage_parameterized: 
    params1:
        ask_llm:
          - model: "gpt-4o"
        read_file:
          - filepath: "./file1.txt"
        save_to_db:
          - database_url: "postgres://user1:pass1@db1/mydb"
            table_name: "table_a"
            save_result: False
    params2:
        ask_llm:
          - model: "gpt-4o-mini"
        read_file:
          - filepath: "./file2.txt"
        save_to_db:
          - database_url: "sqlite://user2:pass2@db2/mydb"
            table_name: "table_b"
            save_result: True

three_stage:
    params1:
        ask_llm_mini:
          - model: "gpt-4o-mini"
        save_to_db:
          - database_url: "sqlite://user2:pass2@db2/mydb"
            table_name: "table_b"
            save_result: False


================================================
FILE: tests/test_configs/test_save_result/jobs/mock_jobs.py
================================================
from typing import Any, Dict

from flow4ai.job import JobABC


class MockFileReadJob(JobABC):
    async def run(self, task: Dict[str, Any]) -> Any:
        inputs = self.get_inputs()
        print(f"\nFileReadJob '{self.name}' reading file: {self.properties.get('filepath')}, inputs:{inputs}")
        file_content = f"Contents of {self.properties.get('filepath')}"
        return {"file_content":file_content}

class MockDatabaseWriteJob(JobABC):
    async def run(self, task: Dict[str, Any]) -> Any:
        inputs = self.get_inputs()
        print(f"\nDatabaseWriteJob '{self.name}' writing to: {self.properties.get('database_url')}, table: {self.properties.get('table_name')}, inputs:{inputs}")
        data_result = f"Data written to table {self.properties.get('table_name')} on db {self.properties.get('database_url')} from {inputs}"
        return {"data_result":data_result}
        
class DummyJob(JobABC):
    async def run(self, task: Dict[str, Any]) -> Any:
        inputs = self.get_inputs()
        print(f"\ndummy_job '{self.name}' with properties: {self.properties}, inputs: {inputs}")
        return {"dummy_job_result":f"Ran function '{self.name}' with {inputs}"}
    


================================================
FILE: tests/test_configs/test_save_result/jobs/mock_jobs2.py
================================================
from typing import Any, Dict

from flow4ai.job import JobABC


class MockJob(JobABC):
    async def run(self, task: Dict[str, Any]) -> Any:
        inputs = self.get_inputs()
        print(f"\nMockJob '{self.name}' with properties: {self.properties}, inputs: {inputs}")
        return {self.name:f"Ran function '{self.name}' with {inputs}"}


================================================
FILE: tests/test_configs/test_simple_parallel/graphs.yaml
================================================
simple_parallel:
    job_a:  # Parallel job A
        next: []
    job_b:  # Parallel job B
        next: []
    job_c:  # Parallel job C
        next: []



================================================
FILE: tests/test_configs/test_simple_parallel/jobs.yaml
================================================
job_a:
    type: DataIngestionJob
    properties:
        source: "/data/source_a"
        batch: 100

job_b:
    type: DataSamplingJob
    properties:
        sample_rate: 0.25
        max_samples: 1000

job_c:
    type: ResultArchiverJob
    properties:
        storage_url: "s3://results-bucket/parallel-test"



================================================
FILE: tests/test_configs/test_simple_parallel/jobs/mock_jobs.py
================================================
from typing import Any, Dict

from flow4ai import f4a_logging as logging
from flow4ai.job import JobABC

logger = logging.getLogger(__name__)

class DataIngestionJob(JobABC):
    """Job for ingesting data from a source."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Processing data from {self.properties.get('source')} with batch size {self.properties.get('batch')}")
        result = {
            "source": self.properties.get('source'),
            "batch": self.properties.get('batch'),
            "records_processed": 1000,
            "status": "success"
        }
        logger.info(f"{self.name}: Completed data ingestion with result: {result}")
        return result

class DataSamplingJob(JobABC):
    """Job for sampling data from a source."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Sampling data from {self.properties.get('source')} at rate {self.properties.get('rate')}")
        result = {
            "source": self.properties.get('source'),
            "rate": self.properties.get('rate'),
            "samples_collected": 250,
            "status": "success"
        }
        logger.info(f"{self.name}: Completed data sampling with result: {result}")
        return result

class ModelProcessorJob(JobABC):
    """Job for processing data through a model."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Processing with model {self.properties.get('model')} in {self.properties.get('validation_mode')} mode")
        # Include inputs from upstream jobs in the result
        result = {
            "model": self.properties.get('model'),
            "validation": self.properties.get('validation_mode'),
            "accuracy": 0.92,
            "status": "success"
        }
        logger.info(f"{self.name}: Completed model processing with result: {result}")
        return result

class ResultArchiverJob(JobABC):
    """Job for archiving results."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Archiving results to {self.properties.get('storage_url')}")
        result = {
            "storage_url": self.properties.get('storage_url'),
            "archived_items": 3,
            "timestamp": "2025-02-26T19:30:00Z",
            "status": "success"
        }
        logger.info(f"{self.name}: Completed archiving with result: {result}")
        return result



================================================
FILE: tests/test_configs/test_single_job/graphs.yaml
================================================
single_stage:
    ask_llm:
      next: [] 



================================================
FILE: tests/test_configs/test_single_job/jobs.yaml
================================================
ask_llm:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
    rate_limit:
      max_rate: 10
      time_period: 1



================================================
FILE: tests/test_configs/test_single_job/parameters.yaml
================================================
single_stage:
    params1:
        ask_llm:
          - model: "gpt-4o"


================================================
FILE: tests/test_configs/test_task_passthrough/graphs.yaml
================================================
text_processing_graph1:
    text_capitalize:
      next:
        - text_reverse
    text_reverse:
      next:
        - text_wrap
    text_wrap:
      next: []

text_processing_graph2:
    text_capitalize:
      next:
        - text_reverse
    text_reverse:
      next:
        - text_wrap
    text_wrap:
      next: []

text_processing_graph3:
    text_capitalize:
      next:
        - text_reverse
    text_reverse:
      next:
        - text_wrap
    text_wrap:
      next: []



================================================
FILE: tests/test_configs/test_task_passthrough/jobs.yaml
================================================
text_capitalize:
  type: TextCapitalizeJob
  properties: {}

text_reverse:
  type: TextReverseJob
  properties: {}

text_wrap:
  type: TextWrapJob
  properties: {}



================================================
FILE: tests/test_configs/test_task_passthrough/jobs/text_processing_jobs.py
================================================
import asyncio
from typing import Any, Dict, Optional

from flow4ai import f4a_logging as logging
from flow4ai.job import JobABC


#Head Job in test
class TextCapitalizeJob(JobABC):
    """Capitalizes input text and adds stage info"""
    
    def __init__(self, name: Optional[str] = None, properties: Dict[str, Any] = {}):
        super().__init__(name, properties)
    
    async def run(self, task: Dict[str, Any]) -> Dict[str, Any]:
        inputs = self.get_inputs()
        task_id = inputs.get('task_id', 'unknown')
        logging.info(f"[TASK_TRACK] TextCapitalizeJob START task_id: {task_id}")
        logging.debug(f"TextCapitalizeJob full task: {inputs}")
        
        input_text = inputs.get('text', '')
        logging.debug(f"TextCapitalizeJob input text: {input_text}")
        
        # Simulate some processing time
        await asyncio.sleep(0.01)
        
        result = {
            'text': input_text.upper(),
            'processing_stage': 'capitalization'
        }
        logging.info(f"[TASK_TRACK] TextCapitalizeJob END task_id: {task_id}")
        logging.debug(f"TextCapitalizeJob result: {result}")
        return result

#Middle job in test
class TextReverseJob(JobABC):
    """Reverses the capitalized text and adds stage info"""
    
    def __init__(self, name: Optional[str] = None, properties: Dict[str, Any] = {}):
        super().__init__(name, properties)
    
    async def run(self, task: Dict[str, Any]) -> Dict[str, Any]:
        inputs = self.get_inputs()
        logging.debug(f"TextReverseJob full task: {inputs}")
        
        # Get task data from previous job
        input_data = next(iter(inputs.values())) if inputs else {}
        logging.debug(f"TextReverseJob task_data: {input_data}")
        
        # Check for task_pass_through key in task_data
        task = self.get_task()
        if not task:
            logging.error(f"[TASK_TRACK] TextReverseJob missing task_pass_through")
            raise KeyError("Required key 'task_pass_through' not found in task")
            
        input_text = input_data.get('text', '')
        logging.debug(f"TextReverseJob input text: {input_text}")
        
        # Simulate some processing time
        await asyncio.sleep(0.01)
        
        result = {
            'text': input_text[::-1],
            'processing_stage': 'reversal'
        }
        logging.info(f"[TASK_TRACK] TextReverseJob END")
        logging.debug(f"TextReverseJob result: {result}")
        return result


class TextWrapJob(JobABC):
    """Wraps the text in brackets and adds stage info"""
    
    def __init__(self, name: Optional[str] = None, properties: Dict[str, Any] = {}):
        super().__init__(name, properties)
    
    async def run(self, task: Dict[str, Any]) -> Dict[str, Any]:
        inputs = self.get_inputs()
        logging.debug(f"TextWrapJob full task: {inputs}")
        
        # Get task data from previous job
        input_data = next(iter(inputs.values())) if inputs else {}
        logging.debug(f"TextWrapJob task_data: {input_data}")
        
        # Check for task_pass_through key in task_data
        task = self.get_task()
        if not task:
            logging.error(f"[TASK_TRACK] TextWrapJob missing task_pass_through")
            raise KeyError("Required key 'task_pass_through' not found in task")
            
        input_text = input_data.get('text', '')
        logging.debug(f"TextWrapJob input text: {input_text}")
        
        # Simulate some processing time
        await asyncio.sleep(0.01)
        
        result = {
            'text': f"[{input_text}]",
            'processing_stage': 'wrapping'
        }
        logging.info(f"[TASK_TRACK] TextWrapJob END")
        logging.debug(f"TextWrapJob result: {result}")
        return result



================================================
FILE: tests/test_utils/__init__.py
================================================




================================================
FILE: tests/test_utils/graph_evaluation.py
================================================
"""
Utility functions for evaluating graph objects in tests.
"""
from flow4ai.job import JobABC
from flow4ai.dsl import DSLComponent, Parallel, Serial


class GraphCreator:
    @staticmethod
    async def evaluate(graph_obj: DSLComponent, prefix="0"):
        """
        Process/evaluate the graph object and return the result.
        This is where you would implement the actual graph processing logic.
        
        Args:
            graph_obj: The graph object to evaluate
            prefix: The prefix to use for hierarchical numbering (e.g., "1.2.3")
        """
        indent = "    " * (prefix.count("."))
        
        if isinstance(graph_obj, Parallel):
            result_lines = [f"{prefix})Executed in parallel: ["]
            for i, component in enumerate(graph_obj.components):
                component_prefix = f"{prefix}.{i+1}"
                component_result = await GraphCreator.evaluate(component, component_prefix)
                result_lines.append(f"{indent}    {component_result},")
            result_lines.append(f"{indent}]")
            return "\n".join(result_lines)
        
        elif isinstance(graph_obj, Serial):
            result_lines = [f"{prefix})Executed in series: ["]
            for i, component in enumerate(graph_obj.components):
                component_prefix = f"{prefix}.{i+1}"
                component_result = await GraphCreator.evaluate(component, component_prefix)
                result_lines.append(f"{indent}    {component_result},")
            result_lines.append(f"{indent}]")
            return "\n".join(result_lines)
        
        elif isinstance(graph_obj, JobABC):
            # Simple case - just a single component
            result = await graph_obj.run({})
            return f"{prefix}) {result}"
        
        else:
            # Raw object (shouldn't normally happen)
            return f"{prefix}) Executed {graph_obj}"


# Create a convenient access to the evaluation method
evaluate = GraphCreator.evaluate

def print_diff(graph, expected_graph, test_name="Unknown"):
    """
    Print the differences between the two graphs.
    Also returns False if there are differences, True if graphs match.
    """
    print(f"\n❌ The generated graph for {test_name} does NOT match the expected structure!")
    print("Differences:")
    has_differences = False
    
    for k in set(list(graph.keys()) + list(expected_graph.keys())):
        if k not in graph:
            print(f"  Missing key {k} in generated graph")
            has_differences = True
        elif k not in expected_graph:
            print(f"  Extra key {k} in generated graph")
            has_differences = True
        else:
            # Check the 'next' attribute in each node
            if 'next' in graph[k] and 'next' in expected_graph[k]:
                if set(graph[k]['next']) != set(expected_graph[k]['next']):
                    print(f"  For key {k}, 'next' values differ:")
                    print(f"    Expected: {sorted(expected_graph[k]['next'])}")
                    print(f"    Actual:   {sorted(graph[k]['next'])}")
                    
                    # Show which elements were added or removed
                    added = set(graph[k]['next']) - set(expected_graph[k]['next'])
                    removed = set(expected_graph[k]['next']) - set(graph[k]['next'])
                    if added:
                        print(f"    Added elements: {sorted(added)}")
                    if removed:
                        print(f"    Removed elements: {sorted(removed)}")
                    has_differences = True
            else:
                # Check for other differences in the node dictionaries
                if graph[k] != expected_graph[k]:
                    print(f"  For key {k}, values differ:")
                    print(f"    Expected: {expected_graph[k]}")
                    print(f"    Actual:   {graph[k]}")
                    has_differences = True
    
    if has_differences:
        # Print the graph as a dictionary for reference
        print("\nActual graph as dictionary:")
        print("{")
        for node, edges in graph.items():
            print(f"    '{node}': {edges},")
        print("}")
    else:
        print("✅ No differences found! This is unexpected since the assertion failed.")
    
    validate_graph(graph, name=test_name)
    
    return not has_differences



================================================
FILE: tests/test_utils/simple_job.py
================================================
"""
Test utilities for Flow4AI tests.

This module contains simple job implementations that are used in tests.
These are not intended for production use.
"""
import asyncio
from typing import Any, Dict, Union

from flow4ai import f4a_logging as logging
from flow4ai.job import JobABC, Task


class SimpleJob(JobABC):
    """A Job implementation that provides a simple default behavior."""
    
    async def run(self, task: Union[Dict[str, Any], Task]) -> Dict[str, Any]:
        """Run a simple job that logs and returns the task."""
        logging.info(f"Async JOB for {task}")
        await asyncio.sleep(1)  # Simulate network delay
        return {"task": task, "status": "complete"}


class SimpleJobFactory:
    """Factory class for creating Job instances with proper tracing."""
    
    @staticmethod
    def _load_from_file(params: Dict[str, Any]) -> JobABC:
        """Create a traced job instance from file configuration."""
        logging.info(f"Loading job with params: {params}")
        return SimpleJob("File Job")

    @staticmethod
    def _load_from_datastore(params: Dict[str, Any]) -> JobABC:
        """Create a traced job instance from datastore."""
        logging.info(f"Loading job from datastore with params: {params}")
        return SimpleJob("Datastore Job")

    @staticmethod
    def load_job(job_context: Dict[str, Any]) -> JobABC:
        """Load a job instance with proper tracing based on context."""
        load_type = job_context.get("type", "").lower()
        params = job_context.get("params", {})

        if load_type == "file":
            return SimpleJobFactory._load_from_file(params)
        elif load_type == "datastore":
            return SimpleJobFactory._load_from_datastore(params)
        else:
            raise ValueError(f"Unsupported job type: {load_type}")


