Directory structure:
└── JobChain/
    ├── src/
    │   ├── __init__.py
    │   └── jobchain/
    │       ├── __init__.py
    │       ├── jc_graph.py
    │       ├── jc_logging.py
    │       ├── job.py
    │       ├── job_chain.py
    │       ├── job_loader.py
    │       ├── taskmanager.py
    │       ├── jobs/
    │       │   ├── __init__.py
    │       │   ├── default_jobs.py
    │       │   └── llm_jobs.py
    │       ├── resources/
    │       │   ├── __init__.py
    │       │   └── otel_config.yaml
    │       └── utils/
    │           ├── __init__.py
    │           ├── llm_utils.py
    │           ├── monitor_utils.py
    │           ├── otel_wrapper.py
    │           ├── print_utils.py
    │           └── timing.py
    └── tests/
        ├── README.md
        ├── conftest.py
        ├── test_aa.py
        ├── test_async_functionality.py
        ├── test_concurrency.py
        ├── test_error_conditions.py
        ├── test_graph_config_parsing.py
        ├── test_job_graph.py
        ├── test_job_loading.py
        ├── test_job_name_parsing.py
        ├── test_job_tracing.py
        ├── test_jobchain_factory.py
        ├── test_jobs.py
        ├── test_logging_config.py
        ├── test_opentelemetry.py
        ├── test_parallel_execution.py
        ├── test_parallel_load.py
        ├── test_queue_stress.py
        ├── test_result_processing.py
        ├── test_task_passthrough.py
        ├── test_configs/
        │   ├── test_concurrency_by_returns/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   └── jobs/
        │   │       └── concurrent_jobs.py
        │   ├── test_jc_config/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   ├── parameters.yaml
        │   │   └── jobs/
        │   │       ├── mock_jobs.py
        │   │       └── mock_jobs2.py
        │   ├── test_jc_config_all/
        │   │   ├── jobchain_all.yaml
        │   │   └── jobs/
        │   │       ├── mock_jobs.py
        │   │       └── mock_jobs2.py
        │   ├── test_jc_config_invalid/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   └── parameters.yaml
        │   ├── test_jc_config_invalid_parameters/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   └── parameters.yaml
        │   ├── test_malformed_config/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   └── parameters.yaml
        │   ├── test_malformed_config_jobs/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   └── parameters.yaml
        │   ├── test_malformed_config_params/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   └── parameters.yaml
        │   ├── test_multiple_heads/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   ├── parameters.yaml
        │   │   └── jobs/
        │   │       └── mock_jobs.py
        │   ├── test_pydantic_config/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   ├── parameters.yaml
        │   │   └── jobs/
        │   │       ├── mock_jobs.py
        │   │       ├── mock_jobs2.py
        │   │       └── pydantic.py
        │   ├── test_single_job/
        │   │   ├── graphs.yaml
        │   │   ├── jobs.yaml
        │   │   └── parameters.yaml
        │   └── test_task_passthrough/
        │       ├── graphs.yaml
        │       ├── jobs.yaml
        │       └── jobs/
        │           └── text_processing_jobs.py
        └── test_utils/
            ├── __init__.py
            └── simple_job.py

================================================
File: src/jobchain/__init__.py
================================================
"""
JobChain - A scalable AI job scheduling and execution platform
"""

from .job import JobABC
from .job_chain import JobChain
from . import jc_logging

__all__ = ['JobABC', 'JobChain', 'jc_logging']


================================================
File: src/jobchain/jc_graph.py
================================================
"""
JobChain Graph module for handling directed acyclic graphs with subgraphs.
Provides functionality for graph traversal, cycle detection, and validation.
"""

from typing import Any, Dict, List, Optional, Set, Tuple

from . import jc_logging

logging = jc_logging.getLogger(__name__)


def has_cycle(graph: Dict[str, Dict[str, Any]], node: str, 
             visited: Optional[Set[str]] = None, path: Optional[Set[str]] = None) -> Tuple[bool, List[str]]:
    """
    Check if the graph has a cycle starting from the given node.
    
    Args:
        graph: The graph structure to check
        node: Starting node for cycle detection
        visited: Set of all visited nodes (for recursive calls)
        path: Set of nodes in current path (for cycle detection)
    
    Returns:
        Tuple[bool, List[str]]: (has_cycle, cycle_path)
    """
    if visited is None:
        visited = set()
    if path is None:
        path = set()
    
    visited.add(node)
    path.add(node)
    
    node_obj = graph[node]
    # Check next nodes
    for next_node in node_obj.get('next', []):
        if next_node in path:  # Cycle detected
            return True, [*path, next_node]
        if next_node not in visited:
            has_cycle_result, cycle_path = has_cycle(graph, next_node, visited, path)
            if has_cycle_result:
                return True, cycle_path
    
    # Check subgraph if it exists
    if 'subgraph' in node_obj:
        subgraph = node_obj['subgraph']
        for subnode in subgraph:
            if subnode not in visited:
                has_cycle_result, cycle_path = has_cycle(subgraph, subnode, visited, path)
                if has_cycle_result:
                    return True, cycle_path
    
    path.remove(node)
    return False, []

def check_graph_for_cycles(graph: Dict[str, Dict[str, Any]], name: str = "") -> bool:
    """
    Check entire graph for cycles.
    
    Args:
        graph: The graph structure to check
        name: Optional name for the graph (for logging)
    
    Returns:
        bool: True if cycles were found, False otherwise
    """
    print(f"\nChecking {name} for cycles...")
    for node in graph:
        has_cycle_result, cycle_path = has_cycle(graph, node)
        if has_cycle_result:
            print(f"Cycle detected! Path: {' -> '.join(cycle_path)}")
            return True
    print("No cycles detected")
    return False

def find_node_and_graph(main_graph: Dict[str, Dict[str, Any]], target_node: str, 
                       current_graph: Optional[Dict[str, Dict[str, Any]]] = None) -> Tuple[Optional[Dict[str, Dict[str, Any]]], List[str]]:
    """
    Recursively finds a node and its containing graph in the graph structure.
    
    Args:
        main_graph: The root graph structure
        target_node: The node to find
        current_graph: Current graph being searched (for recursive calls)
    
    Returns:
        Tuple[Optional[Dict], List[str]]: (containing_graph, path_to_node)
    """
    if current_graph is None:
        current_graph = main_graph
        
    # Check if node is in current level
    if target_node in current_graph:
        return current_graph, []
        
    # Search in subgraphs
    for node in current_graph:
        if 'subgraph' in current_graph[node]:
            subgraph = current_graph[node]['subgraph']
            result_graph, path = find_node_and_graph(main_graph, target_node, subgraph)
            if result_graph is not None:
                return result_graph, [node] + path
                
    return None, []

def add_edge(graph: Dict[str, Dict[str, Any]], from_node: str, to_node: str) -> bool:
    """
    Attempts to add an edge between two nodes in the same graph level.
    
    Args:
        graph: The graph containing both nodes
        from_node: Source node
        to_node: Target node
    
    Returns:
        bool: True if edge was added successfully
    """
    if from_node not in graph:
        print(f"Error: Source node {from_node} not found in graph")
        return False
    
    # Initialize 'next' list if it doesn't exist
    if 'next' not in graph[from_node]:
        graph[from_node]['next'] = []
    
    # Check if edge already exists
    if to_node in graph[from_node]['next']:
        print(f"Edge {from_node} -> {to_node} already exists")
        return True
    
    # Temporarily add the edge
    graph[from_node]['next'].append(to_node)
    
    # Check for cycles
    has_cycle_result, cycle_path = has_cycle(graph, from_node)
    
    if has_cycle_result:
        # Remove the edge if it would create a cycle
        graph[from_node]['next'].remove(to_node)
        print(f"Cannot add edge {from_node} -> {to_node} as it would create a cycle")
        print(f"Cycle detected: {' -> '.join(cycle_path)}")
        return False
    
    print(f"Successfully added edge {from_node} -> {to_node}")
    return True

def add_edge_anywhere(main_graph: Dict[str, Dict[str, Any]], from_node: str, to_node: str) -> bool:
    """
    Attempts to add an edge between two nodes in the graph structure.
    
    Rules for edge addition:
    1. Both nodes must exist in the graph structure
    2. Nodes can only reference other nodes within the same graph level:
       - Main graph nodes can only reference other main graph nodes
       - Subgraph nodes can only reference nodes within the same subgraph
    3. No cycles are allowed within any graph level
    
    Args:
        main_graph: The root graph structure
        from_node: Source node
        to_node: Target node
    
    Returns:
        bool: True if edge was added successfully
    """
    # Find the containing graphs for both nodes
    from_graph, from_path = find_node_and_graph(main_graph, from_node)
    to_graph, to_path = find_node_and_graph(main_graph, to_node)
    
    if from_graph is None:
        print(f"Error: Source node {from_node} not found in graph")
        return False
        
    if to_graph is None:
        print(f"Error: Target node {to_node} not found in graph")
        return False
    
    # Check if nodes are in the same graph level
    if from_graph is not to_graph:
        print(f"Error: Cannot create edge between different graph levels")
        print(f"Source node {from_node} is in {' -> '.join(['main'] + from_path) if from_path else 'main graph'}")
        print(f"Target node {to_node} is in {' -> '.join(['main'] + to_path) if to_path else 'main graph'}")
        return False
    
    return add_edge(from_graph, from_node, to_node)

def print_graph(graph: Dict[str, Dict[str, Any]], spaces: int = 0) -> None:
    """
    Traverse and print the graph structure.
    
    Args:
        graph: The graph structure to traverse
        spaces: Number of spaces for indentation
    """
    for key in graph.keys():
        print("." * spaces + key)
        print_visit_node(graph, key, spaces)

def print_visit_node(graph: Dict[str, Dict[str, Any]], key: str, spaces: int = 0) -> None:
    """
    Visit and print a node's details.
    
    Args:
        graph: The graph containing the node
        key: The node key to visit
        spaces: Number of spaces for indentation
    """
    node_key_obj = graph[key]
    sub_graph_obj = node_key_obj.get('subgraph')
    if sub_graph_obj:
        print("-" * (spaces + 2) + "subgraph:")
        print_graph(sub_graph_obj, spaces+2)
        print("-" * (spaces + 2) + "end subgraph.")
    next_obj = node_key_obj.get('next')
    if next_obj:
        print_traverse_list(next_obj, graph, spaces+2)

def print_traverse_list(nodes: List[str], graph: Dict[str, Dict[str, Any]], spaces: int = 0) -> None:
    """
    Traverse and print a list of nodes.
    
    Args:
        nodes: List of node names
        graph: The graph containing the nodes
        spaces: Number of spaces for indentation
    """
    for node in nodes:
        print("." * spaces + " has dependent " + node)

def validate_graph_references(graph: Dict[str, Dict[str, Any]], path: Optional[List[str]] = None) -> Tuple[bool, List[str]]:
    """
    Validates that all node references in a graph structure are within their own graph level.
    This includes the main graph and all subgraphs.
    
    Args:
        graph: The graph structure to validate
        path: Current path in the graph (for error reporting)
        
    Returns:
        tuple: (is_valid, list_of_violations)
        where violations are strings describing each cross-graph reference found
    """
    if path is None:
        path = []
        
    violations = []
    graph_nodes = set(graph.keys())
    
    # Check each node's references
    for node, node_data in graph.items():
        current_path = path + [node] if path else [node]
        
        # Check 'next' references
        next_nodes = node_data.get('next', [])
        for next_node in next_nodes:
            if next_node not in graph_nodes:
                violations.append(
                    f"Node '{' -> '.join(current_path)}' references '{next_node}' "
                    f"which is not in the same graph level"
                )
        
        # Recursively check subgraphs
        if 'subgraph' in node_data:
            subgraph_valid, subgraph_violations = validate_graph_references(
                node_data['subgraph'], 
                current_path
            )
            violations.extend(subgraph_violations)
    
    return len(violations) == 0, violations

def find_head_nodes(graph: Dict[str, Dict[str, Any]]) -> Set[str]:
    """
    Find all nodes that have no incoming edges (head nodes) in a graph.
    
    Args:
        graph: The graph structure to check
        
    Returns:
        Set[str]: Set of nodes that have no incoming edges
    """
    # First collect all nodes that are destinations
    has_incoming_edges = set()
    for node in graph:
        # Check next nodes
        for next_node in graph[node].get('next', []):
            has_incoming_edges.add(next_node)
        # Check subgraph if it exists
        if 'subgraph' in graph[node]:
            subgraph = graph[node]['subgraph']
            # Recursively find head nodes in subgraph
            subgraph_heads = find_head_nodes(subgraph)
            # All nodes in subgraph are considered to have an incoming edge
            # from the parent node that contains the subgraph
            has_incoming_edges.update(subgraph.keys())
    
    # Head nodes are those that exist in the graph but have no incoming edges
    return set(graph.keys()) - has_incoming_edges

def find_tail_nodes(graph: Dict[str, Dict[str, Any]]) -> Set[str]:
    """
    Find all nodes that have no outgoing edges (tail nodes) in a graph.
    
    Args:
        graph: The graph structure to check
        
    Returns:
        Set[str]: Set of nodes that have no outgoing edges
    """
    tail_nodes = set()
    for node, node_data in graph.items():
        has_next = bool(node_data.get('next', []))
        has_subgraph = 'subgraph' in node_data
        
        if not has_next:
            if has_subgraph:
                # If node has no next but has subgraph, the tail nodes are in the subgraph
                subgraph_tails = find_tail_nodes(node_data['subgraph'])
                tail_nodes.update(subgraph_tails)
            else:
                # Node with no next and no subgraph is a tail
                tail_nodes.add(node)
    
    return tail_nodes

def validate_graph(graph: Dict[str, Dict[str, Any]], name: str = "") -> None:
    """
    Performs comprehensive validation of a graph structure.
    Checks for:
    1. Graph cycles
    2. Cross-graph reference violations
    3. Head node requirements (exactly one head node)
    4. Tail node requirements (exactly one tail node)
    
    Args:
        graph: The graph structure to validate
        name: Optional name for the graph (for logging)
        
    Raises:
        ValueError: If any validation fails, with detailed error message
    """
    errors = []
    
    # Check for cycles
    has_cycles = check_graph_for_cycles(graph, name)
    if has_cycles:
        msg = f"Graph {name} contains cycles"
        logging.error(msg)
        errors.append(msg)
    
    # Check for cross-graph reference violations
    is_valid_refs, violations = validate_graph_references(graph)
    if not is_valid_refs:
        msg = f"Graph {name} contains invalid cross-graph references:\n" + "\n".join(violations)
        logging.error(msg)
        errors.append(msg)
    
    # Check for head node requirements
    head_nodes = find_head_nodes(graph)
    if len(head_nodes) == 0:
        msg = f"Graph {name} has no head nodes (nodes with no incoming edges)"
        logging.error(msg)
        errors.append(msg)
    elif len(head_nodes) > 1:
        msg = f"Graph {name} has multiple head nodes: {head_nodes}. Exactly one head node is required."
        logging.warning(msg)
    
    # Check for tail node requirements
    tail_nodes = find_tail_nodes(graph)
    if len(tail_nodes) == 0:
        msg = f"Graph {name} has no tail nodes (nodes with no outgoing edges)"
        logging.error(msg)
        errors.append(msg)
    elif len(tail_nodes) > 1:
        msg = f"Graph {name} has multiple tail nodes: {tail_nodes}. Exactly one tail node is required."
        logging.warning(msg)
    
    # If any errors were found, raise exception with all error messages
    if errors:
        raise ValueError(f"Graph validation failed:\n" + "\n".join(errors))
    
    # Log success if no errors
    logging.info(f"Graph {name} passed all validations")


================================================
File: src/jobchain/jc_logging.py
================================================
"""
Logging configuration for JobChain.

Environment Variables:
    JOBCHAIN_LOG_LEVEL: Set the logging level (e.g., 'DEBUG', 'INFO'). Defaults to 'INFO'.
    JOBCHAIN_LOG_HANDLERS: Set logging handlers. Options:
        - Not set or 'console': Log to console only (default)
        - 'console,file': Log to both console and file
        
Example:
    To enable both console and file logging:
    $ export JOBCHAIN_LOG_HANDLERS='console,file'
    
    To set debug level logging:
    $ export JOBCHAIN_LOG_LEVEL='DEBUG'
"""


import os

# Initializing the flag here stops logging caching root levels to another value
# for reasons I'm not completely sure about.
WINDSURF_LOG_FLAG = None #None #"DEBUG"
os.environ['JOBCHAIN_LOG_LEVEL'] = WINDSURF_LOG_FLAG or os.getenv('JOBCHAIN_LOG_LEVEL', 'INFO')
import logging
from logging.config import dictConfig


def get_logging_config():
    """Get the logging configuration based on current environment variables."""
    
    
    return {
        'version': 1,
        'disable_existing_loggers': False,
        'formatters': {
            'detailed': {
                'format': '%(asctime)s [%(levelname)s] %(name)s:%(lineno)d - %(message)s'
            }
        },
        'handlers': {
            'console': {
                'class': 'logging.StreamHandler',
                'level': os.getenv('JOBCHAIN_LOG_LEVEL', 'INFO'),
                'formatter': 'detailed',
                'stream': 'ext://sys.stdout'
            },
            'file': {
                'class': 'logging.FileHandler',
                'level': os.getenv('JOBCHAIN_LOG_LEVEL', 'INFO'),
                'formatter': 'detailed',
                'filename': 'jobchain.log',
                'mode': 'a'
            }
        },
        'loggers': {
            'ExampleCustom': {
                'level': 'DEBUG',
                'handlers': ['console', 'file'],
                'propagate': False
            }
        },
        'root': {
            'level':  os.getenv('JOBCHAIN_LOG_LEVEL', 'INFO'),
            # Set JOBCHAIN_LOG_HANDLERS='console,file' to enable both console and file logging
            'handlers': os.getenv('JOBCHAIN_LOG_HANDLERS', 'console').split(',')
        }
    }

def setup_logging():
    """Setup logging with current configuration."""
    config = get_logging_config()
    
    # Always create log file with header, actual logging will only happen if handlers use it
    if not os.path.exists('jobchain.log'):
        with open('jobchain.log', 'w') as f:
            f.write('# JobChain log file - This file is created empty and will be written to only when file logging is enabled\n')
    
    print(f"Logging level: {config['root']['level']}")
    # Apply configuration
    dictConfig(config)

# Apply configuration when module is imported
setup_logging()

# Re-export everything from logging
# Constants
CRITICAL = logging.CRITICAL
ERROR = logging.ERROR
WARNING = logging.WARNING
INFO = logging.INFO
DEBUG = logging.DEBUG
NOTSET = logging.NOTSET

# Functions
getLogger = logging.getLogger
basicConfig = logging.basicConfig
shutdown = logging.shutdown
debug = logging.debug
info = logging.info
warning = logging.warning
error = logging.error
critical = logging.critical
exception = logging.exception
log = logging.log

# Classes
Logger = logging.Logger
Handler = logging.Handler
Formatter = logging.Formatter
Filter = logging.Filter
LogRecord = logging.LogRecord

# Handlers
StreamHandler = logging.StreamHandler
FileHandler = logging.FileHandler
NullHandler = logging.NullHandler

# Configuration
dictConfig = dictConfig  # Already imported from logging.config
fileConfig = logging.config.fileConfig

# Exceptions
exception = logging.exception
captureWarnings = logging.captureWarnings

# Additional utilities
getLevelName = logging.getLevelName
makeLogRecord = logging.makeLogRecord


================================================
File: src/jobchain/job.py
================================================
import asyncio
import uuid
from abc import ABC, ABCMeta, abstractmethod
from contextlib import asynccontextmanager
from contextvars import ContextVar
from typing import Any, Dict, Optional, Type, Union

from . import jc_logging as logging
from .utils.otel_wrapper import trace_function


def _is_traced(method):
    """Helper function to check if a method is traced."""
    return hasattr(method, '_is_traced') and method._is_traced


def _has_own_traced_execute(cls):
    """Helper function to check if a class has its own traced _execute (not inherited)."""
    return '_execute' in cls.__dict__ and _is_traced(cls.__dict__['_execute'])


def _mark_traced(method):
    """Helper function to mark a method as traced."""
    method._is_traced = True
    return method


def traced_job(cls: Type) -> Type:
    """
    Class decorator that ensures the execute method is traced.
    This is only applied to the JobABC class itself.
    """
    if hasattr(cls, '_execute'):
        original_execute = cls._execute
        traced_execute = trace_function(original_execute, detailed_trace=True)
        traced_execute = _mark_traced(traced_execute)
        # Store original as executeNoTrace
        cls.executeNoTrace = original_execute
        # Replace execute with traced version
        cls._execute = traced_execute
    return cls


class JobMeta(ABCMeta):
    """Metaclass that automatically applies the traced_job decorator to JobABC only."""
    def __new__(mcs, name, bases, namespace):
        cls = super().__new__(mcs, name, bases, namespace)
        if name == 'JobABC':  # Only decorate the JobABC class itself
            return traced_job(cls)
        # For subclasses, ensure they inherit JobABC's traced _execute
        if '_execute' in namespace:
            # If subclass defines its own _execute, ensure it's not traced again
            # but still inherits the tracing from JobABC
            del namespace['_execute']
            cls = super().__new__(mcs, name, bases, namespace)
        return cls


class Task(dict):
    """A task dictionary with a unique identifier.
    
    Args:
        data (Union[Dict[str, Any], str]): The task data as a dictionary or string. If a string,
                                            it will be converted to a dictionary with a 'task' key.
        job_name (Optional[str], optional): The name of the job that will process this task.
                                            Required if there is more than one job graph in the
                                            JobChain class"""
    def __init__(self, data: Union[Dict[str, Any], str], job_name: Optional[str] = None):
        # Convert string input to dict
        if isinstance(data, str):
            data = {'task': data}
        elif isinstance(data, dict):
            data = data.copy()  # Create a copy to avoid modifying the original
        else:
            data = {'task': str(data)}
        
        super().__init__(data)
        self.task_id:str = str(uuid.uuid4())
        if job_name is not None:
            self['job_name'] = job_name

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, Task):
            return NotImplemented
        return self.task_id == other.task_id

    # mypy highlights this as an error because dicts are mutable
    #   and so not hashable, but I want each Task to have a unique id
    #   so it is hashable.
    def __hash__(self) -> int:
        return hash(self.task_id)

    def __repr__(self) -> str:
        job_name = self.get('job_name', 'None')
        task_preview = str(dict(self))[:50] + '...' if len(str(dict(self))) > 50 else str(dict(self))
        return f"Task(id={self.task_id}, job_name={job_name}, data={task_preview})"

class JobState:
  def __init__(self):
      self.inputs: Dict[str, Dict[str, Any]] = {}
      self.input_event = asyncio.Event()
      self.execution_started = False

job_graph_context : ContextVar[dict] = ContextVar('job_graph_context')

@asynccontextmanager
async def job_graph_context_manager(job_set: set['JobABC']):
  """Create a new context for job execution, with a new JobState."""
  new_state = {}
  for job in job_set:
      new_state[job.name] = JobState()
  new_state[JobABC.CONTEXT] = {}
  token = job_graph_context.set(new_state)
  try:
      yield new_state
  finally:
      job_graph_context.reset(token)

class JobABC(ABC, metaclass=JobMeta):
    """
    Abstract base class for jobs. Only this class will have tracing enabled through the JobMeta metaclass.
    Subclasses will inherit the traced version of _execute but won't add additional tracing.
    """

    # class variable to keep track of instance counts for each class
    _instance_counts: Dict[Type, int] = {}
    
    # Key used to pass task metadata through the job chain
    TASK_PASSTHROUGH_KEY: str = 'task_pass_through'
    RETURN_JOB='RETURN_JOB'
    CONTEXT='CONTEXT'

    def __init__(self, name: Optional[str] = None, properties: Dict[str, Any] = {}):
        """
        Initialize an JobABC instance.

        Args:
            name (Optional[str], optional): Must be a unique identifier for this job within the context of a JobChain.
                                            If not provided, a unique name will be auto-generated.
            properties (Dict[str, Any], optional): configuration properties passed in by jobs.yaml
        """
        self.name:str = self._getUniqueName() if name is None else name
        self.properties:Dict[str, Any] = properties
        self.expected_inputs:set[str] = set()
        self.next_jobs:list[JobABC] = [] 
        self.timeout = 3000
        self.logger = logging.getLogger(self.__class__.__name__)

    @classmethod
    def parse_job_loader_name(cls, name: str) -> Dict[str, str]:
        """Parse a job loader name into its constituent parts.
        
        Args:
            name: The full job loader name string in format:
                 graph_name$$param_name$$job_name$$
                 
        Returns:
            dict: A dictionary containing graph_name, param_name, and job_name,
                 or {'parsing_message': 'UNSUPPORTED NAME FORMAT'} if invalid
        """
        try:
            parts = name.split("$$")
            if len(parts) != 4 or parts[3] != "" or not parts[0]:
                return {"parsing_message": "UNSUPPORTED NAME FORMAT"}
                
            return {
                "graph_name": parts[0],
                "param_name": parts[1],
                "job_name": parts[2]
            }
        except:
            return {"parsing_message": "UNSUPPORTED NAME FORMAT"}

    @classmethod
    def parse_graph_name(cls, name: str) -> str:
        """Parse and return the graph name from a job loader name.
        
        Args:
            name: The full job loader name string
            
        Returns:
            str: The graph name or 'UNSUPPORTED NAME FORMAT' if invalid
        """
        result = cls.parse_job_loader_name(name)
        return result.get("graph_name", "UNSUPPORTED NAME FORMAT")

    @classmethod
    def parse_param_name(cls, name: str) -> str:
        """Parse and return the parameter name from a job loader name.
        
        Args:
            name: The full job loader name string
            
        Returns:
            str: The parameter name or 'UNSUPPORTED NAME FORMAT' if invalid
        """
        result = cls.parse_job_loader_name(name)
        return result.get("param_name", "UNSUPPORTED NAME FORMAT")

    @classmethod
    def parse_job_name(cls, name: str) -> str:
        """Parse and return the job name from a job loader name.
        
        Args:
            name: The full job loader name string
            
        Returns:
            str: The job name or 'UNSUPPORTED NAME FORMAT' if invalid
        """
        result = cls.parse_job_loader_name(name)
        return result.get("job_name", "UNSUPPORTED NAME FORMAT")

    @classmethod
    def _getUniqueName(cls):
        # Increment the counter for the current class
        cls._instance_counts[cls] = cls._instance_counts.get(cls, 0) + 1
        # Return a unique name based on the current class
        return f"{cls.__name__}_{cls._instance_counts[cls]}"

    @classmethod
    def get_input_from(cls, inputs: Dict[str, Any], job_name: str) -> Dict[str, Any]:
        """Get input data from a specific job in the inputs dictionary.
        
        Args:
            inputs (Dict[str, Any]): Dictionary of inputs from various jobs
            job_name (str): Name of the job whose input we want to retrieve
            
        Returns:
            Dict[str, Any]: The input data from the specified job, or empty dict if not found
        """
        for key in inputs.keys():
            if cls.parse_job_name(key) == job_name:
                return inputs[key]
        return {}


    @classmethod
    def job_set(cls, job) -> set['JobABC']:
        """
        Returns a set of all unique job instances in the job graph by recursively traversing
        all possible paths through next_jobs.
        
        Returns:
            set[JobABC]: A set containing all unique job instances in the graph
        """
        result = {job}  # Start with current job instance
        
        # Base case: if no next jobs, return current set
        if not job.next_jobs:
            return result
            
        # Recursive case: add all jobs from each path
        for job in job.next_jobs:
            result.update(cls.job_set(job))
            
        return result

    def __repr__(self):
        next_jobs_str = [job.name for job in self.next_jobs]
        expected_inputs_str = [input_name for input_name in self.expected_inputs]
        return (f"name: {self.name}\n"
                f"next_jobs: {next_jobs_str}\n"
                f"expected_inputs: {expected_inputs_str}\n"
                f"properties: {self.properties}")

    async def _execute(self, task: Union[Task, None]) -> Dict[str, Any]:
        """ Responsible for executing the job graph, maintaining state of the graph
        by updating the JobState object and propagating the tail results back up the graph
        when a tail job is reached.

        WARNING: DO NOT OVERRIDE THIS METHOD IN CUSTOM JOB CLASSES.
        This method is part of the core JobChain execution flow and handles critical operations
        including job graph traversal, state management, and result propagation.
        
        Instead, implement the abstract 'run' method to define custom job behavior.

        Can only be used within a job_graph_context set up with:
           ```python
            async with job_graph_context_manager(job_set):
                        result = await job._execute(task)
            ```

        Args:
            task (Union[Task, None]): the input to the first (head) job of the job graph, is None in child jobs.

        Returns:
            Dict[str, Any]: The output of the job graph execution
        """
        job_state_dict:dict = job_graph_context.get()
        job_state = job_state_dict.get(self.name)
        if isinstance(task, dict):
            job_state.inputs.update(task)
            self.get_context()[JobABC.TASK_PASSTHROUGH_KEY] = task
        elif task is None:
            pass 
        else:
            job_state.inputs[self.name] = task

        if self.expected_inputs:
            if job_state.execution_started:
                return None
            
            job_state.execution_started = True
            try:
                await asyncio.wait_for(job_state.input_event.wait(), self.timeout)
            except asyncio.TimeoutError:
                job_state.execution_started = False
                raise TimeoutError(
                    f"Timeout waiting for inputs in {self.name}. "
                    f"Expected: {self.expected_inputs}, "
                    f"Received: {list(job_state.inputs.keys())}"
                )

        result = await self.run(job_state.inputs)
        self.logger.debug(f"Job {self.name} finished running")

        if not isinstance(result, dict):
            result = {'result': result}

        # if isinstance(task, dict):
        #     result[JobABC.TASK_PASSTHROUGH_KEY] = task
        # else:
        #     for input_data in job_state.inputs.values():
        #         if isinstance(input_data, dict) and JobABC.TASK_PASSTHROUGH_KEY in input_data:
        #             result[JobABC.TASK_PASSTHROUGH_KEY] = input_data[JobABC.TASK_PASSTHROUGH_KEY]
        #             break

        # Clear state for potential reuse
        job_state.inputs.clear()
        job_state.input_event.clear()
        job_state.execution_started = False

        # Store the job name that returns the result
        result[JobABC.RETURN_JOB] = self.name

        # If this is a tail job, return immediately
        if not self.next_jobs:
            self.logger.debug(f"Tail Job {self.name} returning result: {result}")
            task = self.get_context()[JobABC.TASK_PASSTHROUGH_KEY]
            result[JobABC.TASK_PASSTHROUGH_KEY] = task
            return result

        # Execute child jobs
        executing_jobs = []
        for next_job in self.next_jobs:
            input_data = result.copy()
            await next_job.receive_input(self.name, input_data)
            next_job_inputs = job_state_dict.get(next_job.name).inputs
            if next_job.expected_inputs.issubset(set(next_job_inputs.keys())):
                executing_jobs.append(next_job._execute(task=None))

        if executing_jobs:
            child_results = await asyncio.gather(*executing_jobs)
            # Find the tail job result (the one that has no next_jobs)
            tail_results = [r for r in child_results if r is not None]
            if tail_results:
                # Always return the first valid tail result
                tail_result = tail_results[0]
                self.logger.debug(f"Job {self.name} propagating tail result: {tail_result}")
                # Preserve the original tail job that generated the result
                return tail_result

        # If no child jobs executed or no tail result found, return None
        return None

    async def receive_input(self, from_job: str, data: Dict[str, Any]) -> None:
        """Receive input from a predecessor job"""
        job_state_dict:dict = job_graph_context.get()
        job_state = job_state_dict.get(self.name)
        job_state.inputs[from_job] = data
        if self.expected_inputs.issubset(set(job_state.inputs.keys())):
            job_state.input_event.set()

    def job_set_str(self) -> set[str]:
        """
        Returns a set of all unique job names in the job graph by recursively traversing
        all possible paths through next_jobs.
        
        Returns:
            set[str]: A set containing all unique job names in the graph
        """
        result = {self.name}  # Start with current job's name
        
        # Base case: if no next jobs, return current set
        if not self.next_jobs:
            return result
            
        # Recursive case: add all jobs from each path
        for job in self.next_jobs:
            result.update(job.job_set_str())
            
        return result

    def is_head_job(self) -> bool:
        """
        Check if this job is a head job (has no expected inputs).

        Returns:
            bool: True if this is a head job (no expected inputs), False otherwise
        """
        return len(self.expected_inputs) == 0

    def get_context(self) -> Dict[str, Any]:
        """A repository to store state across jobs in a graph for a single coroutine.
           can only be used within a job_graph_context set up with:
           ```python
            async with job_graph_context_manager(job_set):
                        result = await job._execute(task)
            ```
        """
        job_state_dict:dict = job_graph_context.get()
        context = job_state_dict[JobABC.CONTEXT]
        return context

    def get_task(self) -> Union[Dict[str, Any], Task]:
        """
        Get the task associated with this job.

        Returns:
            Union[Dict[str, Any], Task]: The task associated with this job.
        """
        task = self.get_context()[JobABC.TASK_PASSTHROUGH_KEY]
        return task
        
        # if not self.is_head_job(): 
        #     first_parent_result = next(iter(inputs.values()))
        #     task = first_parent_result[JobABC.TASK_PASSTHROUGH_KEY]
        # else:
        #     task = inputs
        # return task

    @abstractmethod
    async def run(self, task: Union[Dict[str, Any], Task]) -> Dict[str, Any]:
        """Execute the job on the given task. Must be implemented by subclasses."""
        pass

# SimpleJob and SimpleJobFactory have been moved to tests/test_utils/simple_job.py
# They are only used for testing purposes and not for production code.


================================================
File: src/jobchain/job_chain.py
================================================
import asyncio
# In theory it makes sense to use dill with the "multiprocess" package
# instead of pickle with "multiprocessing", but in practice it leads to 
# performance and stability issues.
import multiprocessing as mp
import pickle
import queue
from collections import OrderedDict
from multiprocessing import freeze_support, set_start_method
from typing import Any, Callable, Collection, Dict, Optional, Union

from pydantic import BaseModel

from . import jc_logging as logging
from .job import JobABC, Task, job_graph_context_manager
from .job_loader import ConfigLoader, JobFactory
from .utils.monitor_utils import should_log_task_stats


class JobChain:
    """
    JobChain executes up to thousands of tasks in parallel using one or more Jobs passed into constructor.
    Optionally passes results to a pre-existing result processing function after task completion.

    Args:
        job (Union[Dict[str, Any], JobABC, Collection[JobABC]]): If missing, jobs will be loaded from config file.
            Otherwise either a dictionary containing job configuration,
            a single JobABC instance, or a collection of JobABC instances.

        result_processing_function (Optional[Callable[[Any], None]]): Code to handle results after the Job executes its task.
            By default, this hand-off happens in parallel, immediately after a Job processes a task.
            Typically, this function is from an existing codebase that JobChain is supplementing.
            This function must be picklable, for parallel execution, see serial_processing parameter below.
            This code is not assumed to be asyncio compatible.

        serial_processing (bool, optional): Forces result_processing_function to execute only after all tasks are completed by the Job.
            Enables an unpicklable result_processing_function to be used by setting serial_processing=True.
            However, in most cases changing result_processing_function to be picklable is straightforward and should be the default.
            Defaults to False.
    """
    # Constants
    JOB_MAP_LOAD_TIME = 5  # Timeout in seconds for job map loading

    def __init__(self, job: Optional[Any] = None, result_processing_function: Optional[Callable[[Any], None]] = None, 
                 serial_processing: bool = False):
        # Get logger for JobChain
        self.logger = logging.getLogger('JobChain')
        self.logger.info("Initializing JobChain")
        if not serial_processing and result_processing_function:
            self._check_picklable(result_processing_function)
        # tasks are created by submit_task(), with ["job_name"] added to the task dict
        # tasks are then sent to queue for processing
        self._task_queue: mp.Queue[Task] = mp.Queue()  
        # INTERNAL USE ONLY. DO NOT ACCESS DIRECTLY.
        # This queue is for internal communication between the job executor and result processor.
        # To process results, use the result_processing_function parameter in the JobChain constructor.
        # See test_result_processing.py for examples of proper result handling.
        self._result_queue = mp.Queue()  # type: mp.Queue
        self.job_executor_process = None
        self.result_processor_process = None
        self._result_processing_function = result_processing_function
        self._serial_processing = serial_processing
        # This holds a map of job name to job, 
        # when _execute is called on the job, the task must have a job_name
        # associated with it, if there is more than one job in the job_map
        self.job_map: OrderedDict[str, JobABC] = OrderedDict()
        
        # Create a manager for sharing objects between processes
        self._manager = mp.Manager()
        # Create a shared dictionary for job name mapping
        self._job_name_map = self._manager.dict()
        # Create an event to signal when jobs are loaded
        self._jobs_loaded = mp.Event()

        if job:
            self.create_job_map(job)
        
        self._start()

    def create_job_map(self, job):
        if isinstance(job, Dict):
            pass # SimpleJobFactory is deprecated
            # job_context: Dict[str, Any] = job.get("job_context") or {}
            # loaded_job = SimpleJobFactory.load_job(job_context)
            # if isinstance(loaded_job, JobABC):
            #     self.job_map[loaded_job.name] = loaded_job
        elif isinstance(job, JobABC):
            self.job_map[job.name] = job
        elif isinstance(job, Collection) and not isinstance(job, (str, bytes, bytearray)):
            if not job:  # Check if collection is empty
                raise ValueError("Job collection cannot be empty")
            if not all(isinstance(j, JobABC) for j in job):
                raise TypeError("All items in job collection must be JobABC instances")
            for j in job:
                if isinstance(j, JobABC):
                    if j.name in self.job_map:
                        raise ValueError(f"Duplicate job name found: {j.name}")
                    self.job_map[j.name] = j
                else:
                    raise TypeError("Items in job collection must be JobABC instances")
        else:
            raise TypeError("job must be either Dict[str, Any], JobABC instance, or Collection[JobABC]")

        self._job_name_map.clear()
        self._job_name_map.update({job.name: job.job_set_str() for job in self.job_map.values()})

    # We will not to use context manager as it makes semantics of JobChain use less flexible
    # def __enter__(self):
    #     """Initialize resources when entering the context."""
    #     self._start()
    #     return self

    # def __exit__(self, exc_type, exc_val, exc_tb):
    #     """Clean up resources when exiting the context."""
    #     self._cleanup()

    # belt and braces, _cleanup is called by _wait_for_completion() via mark_input_completed()
    def __del__(self):
        self._cleanup

    def _cleanup(self):
        """Clean up resources when the object is destroyed."""
        self.logger.info("Cleaning up JobChain resources")
        
        if self.job_executor_process:
            if self.job_executor_process.is_alive():
                self.logger.debug("Terminating job executor process")
                self.job_executor_process.terminate()
                self.logger.debug("Joining job executor process")
                self.job_executor_process.join()
                self.logger.debug("Job executor process joined")
        
        if self.result_processor_process:
            if self.result_processor_process.is_alive():
                self.logger.debug("Terminating result processor process")
                self.result_processor_process.terminate()
                self.logger.debug("Joining result processor process")
                self.result_processor_process.join()
                self.logger.debug("Result processor process joined")
        
        if hasattr(self, '_task_queue'):
            self.logger.debug("Closing task queue")
            self._task_queue.close()
            self.logger.debug("Joining task queue thread")
            self._task_queue.join_thread()
            self.logger.debug("Task queue thread joined")
        
        if hasattr(self, '_result_queue'):
            self.logger.debug("Closing result queue")
            self._result_queue.close()
            self.logger.debug("Joining result queue thread")
            self._result_queue.join_thread()
            self.logger.debug("Result queue thread joined")
        
        self.logger.debug("Cleanup completed")

    def _check_picklable(self, result_processing_function):
        try:
            # Try to pickle just the function itself
            pickle.dumps(result_processing_function)
            
            # Try to pickle any closure variables
            if hasattr(result_processing_function, '__closure__') and result_processing_function.__closure__:
                for cell in result_processing_function.__closure__:
                    pickle.dumps(cell.cell_contents)
                    
        except Exception as e:
            self.logger.error(f"""Result processing function or its closure variables cannot be pickled: {e}.  
                              Use serial_processing=True for unpicklable functions.""")
            raise TypeError(f"Result processing function must be picklable in parallel mode: {e}")

    
    def _start(self):
        """Start the job executor and result processor processes - non-blocking."""
        self.logger.debug("Starting job executor process")
        self.job_executor_process = mp.Process(
            target=self._async_worker,
            args=(self.job_map, self._task_queue, self._result_queue, self._job_name_map, self._jobs_loaded, ConfigLoader.directories),
            name="JobExecutorProcess"
        )
        self.job_executor_process.start()
        self.logger.info(f"Job executor process started with PID {self.job_executor_process.pid}")

        if self._result_processing_function and not self._serial_processing:
            self.logger.debug("Starting result processor process")
            self.result_processor_process = mp.Process(
                target=self._result_processor,
                args=(self._result_processing_function, self._result_queue),
                name="ResultProcessorProcess"
            )
            self.result_processor_process.start()
            self.logger.info(f"Result processor process started with PID {self.result_processor_process.pid}")
    # TODO: add ability to submit a task or an iterable: Iterable
    # TODO: add resource usage monitoring which returns False if resource use is too high.
    def submit_task(self, task: Union[Dict[str, Any], str], job_name: Optional[str] = None):
        """
        Submit a task to be processed by the job.

        Args:
            task: Either a dictionary containing task data or a string that will be converted to a task.
                    if this is None then the task will be skipped.
            job_name: The name of the job to execute this task. Required if there is more than one job in the job_map,
                     unless the task is a dictionary that includes a 'job_name' key.

        Raises:
            ValueError: If job_name is required but not provided, or if the specified job cannot be found.
            TypeError: If the task is not a dictionary or string.
        """
        try:
            # Wait for jobs to be loaded
            if not self._jobs_loaded.wait(timeout=self.JOB_MAP_LOAD_TIME):
                raise TimeoutError("Timed out waiting for jobs to be loaded")

            if task is None:
                self.logger.warning("Received None task, skipping")
                return
            
            if isinstance(task, str):
                task_dict = {'task': task}
            elif isinstance(task, dict):
                task_dict = task.copy()
            else:
                self.logger.warning(f"Received invalid task type {type(task)}, converting to string")
                task_dict = {'task': str(task)}

            # If job_name parameter is provided, it takes precedence
            if job_name is not None:
                if job_name not in self._job_name_map:
                    raise ValueError(
                        f"Job '{job_name}' not found. Available jobs: {list(self._job_name_map.keys())}"
                    )
                task_dict['job_name'] = job_name
            
            # If there's more than one job, we need a valid job name
            if len(self._job_name_map) > 1:
                if 'job_name' not in task_dict or not isinstance(task_dict['job_name'], str) or not task_dict['job_name']:
                    raise ValueError(
                        "When multiple jobs are present, you must either:\n"
                        "1) Provide the job_name parameter in submit_task() OR\n"
                        "2) Include a non-empty string 'job_name' in the task dictionary"
                    )
                if task_dict['job_name'] not in self._job_name_map:
                    raise ValueError(
                        f"Job '{task_dict['job_name']}' not found. Available jobs: {list(self._job_name_map.keys())}"
                    )
            elif len(self._job_name_map) == 1:
                # If there's only one job, use its name
                task_dict['job_name'] = next(iter(self._job_name_map.keys()))
            else:
                raise ValueError("No jobs available in JobChain")

            task_obj = Task(task_dict)
            self._task_queue.put(task_obj)
        except Exception as e:
            self.logger.error(f"Error submitting task: {e}")
            self.logger.info("Detailed stack trace:", exc_info=True)

    def mark_input_completed(self):
        """Signal completion of input and wait for all processes to finish and shut down."""
        self.logger.debug("Marking input as completed")
        self.logger.info("*** task_queue ended ***")
        self._task_queue.put(None)
        self._wait_for_completion()

    # Must be static because it's passed as a target to multiprocessing.Process
    # Instance methods can't be pickled properly for multiprocessing
    # TODO: it may be necessary to put a flag to execute this using asyncio event loops
    #          for example, when handing off to an async web service
    @staticmethod
    def _result_processor(process_fn: Callable[[Any], None], result_queue: 'mp.Queue'):
        """Process that handles processing results as they arrive."""
        logger = logging.getLogger('ResultProcessor')
        logger.debug("Starting result processor")

        while True:
            try:
                result = result_queue.get()
                if result is None:
                    logger.debug("Received completion signal from result queue")
                    break
                logger.debug(f"ResultProcessor received result: {result}")
                try:
                    # Handle both dictionary and non-dictionary results
                    task_id = result.get('task', str(result)) if isinstance(result, dict) else str(result)
                    logger.debug(f"Processing result for task {task_id}")
                    process_fn(result)
                    logger.debug(f"Finished processing result for task {task_id}")
                except Exception as e:
                    logger.error(f"Error processing result: {e}")
                    logger.info("Detailed stack trace:", exc_info=True)
            except queue.Empty:
                continue

        logger.debug("Result processor shutting down")

    def _wait_for_completion(self):
        """Wait for completion of all processing."""
        self.logger.debug("Entering wait for completion")

        if self._result_processing_function and self._serial_processing:
            self._process_serial_results()
        
        # Wait for job executor to finish
        if self.job_executor_process and self.job_executor_process.is_alive():
            self.logger.debug("Waiting for job executor process")
            self.job_executor_process.join()
            self.logger.debug("Job executor process completed")

        # Wait for result processor to finish
        if self.result_processor_process and self.result_processor_process.is_alive():
            self.logger.debug("Waiting for result processor process")
            self.result_processor_process.join()
            self.logger.debug("Result processor process completed")
        
        self._cleanup()

    def _process_serial_results(self):
        while True:
            try:
                self.logger.debug("Attempting to get result from queue")
                result = self._result_queue.get(timeout=0.1)
                if result is None:
                    self.logger.debug("Received completion signal (None) from result queue")
                    self.logger.info("No more results to process.")
                    break
                if self._result_processing_function:
                    try:
                        # Handle both dictionary and non-dictionary results
                        task_id = result.get('task', str(result)) if isinstance(result, dict) else str(result)
                        self.logger.debug(f"Processing result for task {task_id}")
                        self._result_processing_function(result)
                        self.logger.debug(f"Finished processing result for task {task_id}")
                    except Exception as e:
                        self.logger.error(f"Error processing result: {e}")
                        self.logger.info("Detailed stack trace:", exc_info=True)
            except queue.Empty:
                job_executor_is_alive = self.job_executor_process and self.job_executor_process.is_alive()
                self.logger.debug(f"Queue empty, job executor process alive status = {job_executor_is_alive}")
                if not job_executor_is_alive:
                    self.logger.debug("Job executor process is not alive, breaking wait loop")
                    break
                continue

    @staticmethod
    def _replace_pydantic_models(data: Any) -> Any:
        """Recursively replace pydantic.BaseModel instances with their JSON dumps."""
        logger = logging.getLogger('JobChain')
        logger.debug(f'Processing data type: {type(data)}')

        if isinstance(data, dict):
            return {k: JobChain._replace_pydantic_models(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [JobChain._replace_pydantic_models(item) for item in data]
        elif isinstance(data, BaseModel):
            logger.info(f'Converting pydantic model {data.__class__.__name__}')
            return data.model_dump_json()
        return data

    # Must be static because it's passed as a target to multiprocessing.Process
    # Instance methods can't be pickled properly for multiprocessing
    @staticmethod
    def _async_worker(job_map: Dict[str, JobABC], task_queue: 'mp.Queue', result_queue: 'mp.Queue', 
                     job_name_map: 'mp.managers.DictProxy', jobs_loaded: 'mp.Event', 
                     directories: list[str] = []):
        """Process that handles making workflow calls using asyncio."""
        # Get logger for AsyncWorker
        logger = logging.getLogger('AsyncWorker')
        logger.debug("Starting async worker")

        # If job_map is empty, create it from SimpleJobLoader
        if not job_map:
            # logger.info("Creating job map from SimpleJobLoader")
            # job = SimpleJobFactory.load_job({"type": "file", "params": {}})
            # job_map = {job.name: job}
            logger.info("Creating job map from JobLoader")
            logger.info(f"Using directories from process: {directories}")
            ConfigLoader._set_directories(directories)
            ConfigLoader.reload_configs()
            head_jobs = JobFactory.get_head_jobs_from_config()
            job_map = {job.name: job for job in head_jobs}
            # Update the shared job_name_map with each head job's complete set of reachable jobs
            job_name_map.clear()
            job_name_map.update({job.name: job.job_set_str() for job in head_jobs})
            logger.info(f"Created job map with head jobs: {list(job_name_map.keys())}")

        # Signal that jobs are loaded
        jobs_loaded.set()

        async def process_task(task: Task):
            """Process a single task and return its result"""
            task_id = task.task_id  # task_id is not held in the dictionary itself i.e. NOT task['task_id']
            logger.debug(f"[TASK_TRACK] Starting task {task_id}")
            try:
                # If there's only one job, use it directly
                if len(job_map) == 1:
                    job = next(iter(job_map.values()))
                else:
                    # Otherwise, get the job from the map using job_name
                    job_name = task.get('job_name')
                    if not job_name:
                        raise ValueError("Task missing job_name when multiple jobs are present")
                    job = job_map[job_name]
                job_set = JobABC.job_set(job) #TODO: create a map of job to jobset in _async_worker
                async with job_graph_context_manager(job_set):
                    result = await job._execute(task)
                    processed_result = JobChain._replace_pydantic_models(result)
                    logger.info(f"[TASK_TRACK] Completed task {task_id}, returned by job {processed_result[JobABC.RETURN_JOB]}")

                    result_queue.put(processed_result)
                    logger.debug(f"[TASK_TRACK] Result queued for task {task_id}")
            except Exception as e:
                logger.error(f"[TASK_TRACK] Failed task {task_id}: {e}")
                logger.info("Detailed stack trace:", exc_info=True)
                raise

        async def queue_monitor():
            """Monitor the task queue and create tasks as they arrive"""
            logger.debug("Starting queue monitor")
            tasks = set()
            pending_tasks = []
            tasks_created = 0
            tasks_completed = 0
            end_signal_received = False

            while not end_signal_received or tasks:
                # Get all available tasks from the queue
                while True:
                    try:
                        task = task_queue.get_nowait()
                        if task is None:
                            logger.info("Received end signal in task queue")
                            end_signal_received = True
                            break
                        pending_tasks.append(task)
                    except queue.Empty:
                        break

                # Create tasks in batch if we have any pending
                if pending_tasks:
                    logger.debug(f"Creating {len(pending_tasks)} new tasks")
                    new_tasks = {asyncio.create_task(process_task(pending_tasks[i])) for i in range(len(pending_tasks))}
                    tasks.update(new_tasks)
                    tasks_created += len(new_tasks)
                    logger.debug(f"Total tasks created: {tasks_created}")
                    pending_tasks.clear()

                # Clean up completed tasks
                done_tasks = {t for t in tasks if t.done()}
                if done_tasks:
                    for done_task in done_tasks:
                        try:
                            # Check if task raised an exception
                            exc = done_task.exception()
                            if exc:
                                logger.error(f"Task failed with exception: {exc}")
                                logger.info("Detailed stack trace:", exc_info=True)
                        except asyncio.InvalidStateError:
                            pass  # Task was cancelled or not done
                    tasks_completed += len(done_tasks)
                    logger.debug(f"Cleaned up {len(done_tasks)} completed tasks. Total completed: {tasks_completed}")
                    logger.debug(f"Active tasks remaining: {len(tasks)}")
                tasks.difference_update(done_tasks)

                # Log task stats periodically
                if tasks_completed != 0 and tasks_completed % 5 == 0:
                    if should_log_task_stats(queue_monitor, tasks_created, tasks_completed):
                        logger.info(f"Tasks stats - Created: {tasks_created}, Completed: {tasks_completed}, Active: {len(tasks)}")

                # A short pause to reduce CPU usage and avoid a busy-wait state.             
                await asyncio.sleep(0.0001)

            # Wait for remaining tasks to complete
            if tasks:
                logger.debug(f"Waiting for {len(tasks)} remaining tasks")
                await asyncio.gather(*tasks)
                logger.debug("All remaining tasks completed")

            # Signal completion
            logger.debug("Sending completion signal to result queue")
            logger.debug(f"Final stats - Created: {tasks_created}, Completed: {tasks_completed}")
            logger.info("*** result_queue ended ***")
            result_queue.put(None)

        # Run the event loop
        logger.debug("Creating event loop")
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            logger.debug("Running queue monitor")
            loop.run_until_complete(queue_monitor())
        except Exception as e:
            import traceback
            logger.error(f"Error in async worker: {e}\n{traceback.format_exc()}")
            logger.info("Detailed stack trace:", exc_info=True)
        finally:
            logger.info("Closing event loop")
            loop.close()

    def get_job_names(self) -> list[str]:
        """
        Returns a list of job names after ensuring the job_name_map is loaded.

        Returns:
            list[str]: List of job names from the job_name_map

        Raises:
            TimeoutError: If waiting for jobs to be loaded exceeds timeout
        """
        self.logger.debug("Waiting for jobs to be loaded before returning job names")
        if not self._jobs_loaded.wait(timeout=self.JOB_MAP_LOAD_TIME):
            raise TimeoutError("Timed out waiting for jobs to be loaded")
        
        return list(self._job_name_map.keys())

    def get_job_graph_mapping(self) -> dict[str, set[str]]:
        """
        Returns a mapping of head job names to their complete set of job names in their graph.
        
        Returns:
            dict[str, set[str]]: Dictionary mapping each head job name to a set of all job names
                                reachable from that job (including itself).

        Raises:
            TimeoutError: If waiting for jobs to be loaded exceeds timeout
        """
        self.logger.debug("Waiting for jobs to be loaded before returning job graph mapping")
        if not self._jobs_loaded.wait(timeout=self.JOB_MAP_LOAD_TIME):
            raise TimeoutError("Timed out waiting for jobs to be loaded")
        
        return dict(self._job_name_map)

class JobChainFactory:
    _instance = None
    _job_chain = None

    def __init__(self, *args, **kwargs):
        if not JobChainFactory._instance:
            self._job_chain = JobChain(*args, **kwargs)
            JobChainFactory._instance = self

    @classmethod
    def init(cls, start_method="spawn", *args, **kwargs):
      """
      Initializes the JobChainFactory using the given start method.
      args and kwargs are passed down to the JobChain constructor.

      Args:
        start_method: The start method of multiprocessing. Defaults to "spawn".
        args: The parameters to be passed to the JobChain's constructor
        kwargs: The keyword parameters to be passed to the JobChain's constructor
        
      """
      freeze_support()
      set_start_method(start_method)
      if not cls._instance:
        cls._instance = cls(*args, **kwargs)
      return cls._instance

    @staticmethod
    def get_instance()->JobChain:
        if not JobChainFactory._instance:
            raise RuntimeError("JobChainFactory not initialized")
        return JobChainFactory._instance._job_chain


================================================
File: src/jobchain/job_loader.py
================================================
import importlib.util
import inspect
import os
import sys
from pathlib import Path
from typing import Any, Collection, Dict, List, Type, Union

import yaml
from pydantic import BaseModel

from . import jc_logging as logging
from .jc_graph import validate_graph
from .job import JobABC

logger = logging.getLogger(__name__)


class JobValidationError(Exception):
    """Raised when a custom job fails validation"""
    pass


class ConfigurationError(Exception):
    """Exception raised when configuration is malformed."""
    pass


class PythonLoader:
    JOBS = "jobs"
    PYDANTIC = "pydantic"

    @staticmethod
    def validate_pydantic_class(job_class: Type) -> bool:
        """Validate that a class meets the requirements to be a valid pydantic model:
        - Inherits from BaseModel
        """
        return inspect.isclass(job_class) and issubclass(job_class, BaseModel)

    @staticmethod
    def validate_job_class(job_class: Type) -> bool:
        """
        Validate that a class meets the requirements to be a valid job:
        - Inherits from JobABC
        - Has required methods
        - Has required attributes
        """
        # Check if it's a class and inherits from JobABC
        if not (inspect.isclass(job_class) and issubclass(job_class, JobABC)):
            return False

        # Check for required async run method
        if not hasattr(job_class, 'run'):
            return False

        # Check if run method is async
        run_method = getattr(job_class, 'run')
        if not inspect.iscoroutinefunction(run_method):
            return False

        return True

    @classmethod
    def load_python(cls, python_dir: str, type_name: str = JOBS) -> Dict[str, Union[Type[JobABC], Type[BaseModel]]]:
        """
        Load all custom job classes from the specified directory
        """
        python_classes = {}
        python_path = Path(python_dir)

        if not python_path.exists():
            logger.info(f"Python directory not found: {python_dir}")
            return python_classes

        # Add the custom jobs directory to Python path
        logger.debug(f"Python path before: {sys.path}")
        sys.path.append(str(python_path))
        logger.info(f"Added {python_path} to Python path")
        logger.debug(f"Python path after: {sys.path}")

        # Scan for Python files
        for file_path in python_path.glob("**/*.py"):
            if file_path.name.startswith("__"):
                continue

            logger.info(f"Loading python classes from {file_path}")
            try:
                # Load the module
                module_name = file_path.stem
                spec = importlib.util.spec_from_file_location(module_name, str(file_path))
                if spec is None or spec.loader is None:
                    logger.warning(f"Could not create module spec for {file_path}")
                    continue

                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)

                # Find all classes in the module that inherit from JobABC
                for name, obj in inspect.getmembers(module):
                    if inspect.isclass(obj) and obj.__module__ == module.__name__:
                        try:
                            if ((type_name == cls.JOBS and cls.validate_job_class(obj)) or
                                (type_name == cls.PYDANTIC and cls.validate_pydantic_class(obj))):
                                logger.info(f"Found valid python class: {name}")
                                python_classes[name] = obj
                        except Exception as e:
                            logger.error(f"Error validating python class {name} in {file_path}: {str(e)}")
                            raise JobValidationError(
                                f"Error validating python class {name} in {file_path}: {str(e)}"
                            )

            except Exception as e:
                logger.error(f"Error loading custom python class from {file_path}: {str(e)}")
                raise ImportError(
                    f"Error loading custom python class from {file_path}: {str(e)}"
                )

        return python_classes


class JobFactory:
    _job_types_registry: Dict[str, Type[JobABC]] = {}
    _pydantic_types_registry: Dict[str, Type[BaseModel]] = {}
    # Default jobs directory is always checked first
    _default_jobs_dir: str = os.path.join(os.path.dirname(__file__), "jobs") # site-package directory when this is a package
    _cached_job_graphs: List[JobABC] = None

    @classmethod
    def load_python_into_registries(cls, custom_python_dirs: list[str] = None):
        """
        Load and register all custom jobs from specified config directories.
        Will look for jobs in the 'jobs' subdirectory of each config directory.
        Loads jobs from all directories.

        Args:
            custom_python_dirs: List of config directory paths. Jobs will be loaded from the 'jobs' subdirectory
                            of each config directory.
        """
        loader = PythonLoader()
        # Create an iterable of job directories, including the default and any custom directories
        python_dirs = [cls._default_jobs_dir]
        if custom_python_dirs:
            # Add local jobs directories from each config directory
            for config_dir in custom_python_dirs:
                python_dir = os.path.join(config_dir, "jobs")
                if os.path.exists(python_dir):
                    python_dirs.append(python_dir)
            
        found_valid_jobs = False
        for python_dir in python_dirs:
            # Load and register jobs
            jobs = loader.load_python(python_dir, PythonLoader.JOBS)
            if jobs:
                found_valid_jobs = True
                # Register all valid custom jobs
                for job_name, job_class in jobs.items():
                    cls.register_job_type(job_name, job_class)
                    print(f"Registered custom job: {job_name}")
            
            # Load and register pydantic models
            pydantic_models = loader.load_python(python_dir, PythonLoader.PYDANTIC)
            if pydantic_models:
                for model_name, model_class in pydantic_models.items():
                    cls.register_pydantic_type(model_name, model_class)
                    print(f"Registered pydantic model: {model_name}")
            else:
                logger.info("No pydantic classes found")
        
        if not found_valid_jobs:
            # This is a critical error as we need at least one valid job directory
            raise FileNotFoundError(f"No valid jobs found in any of the directories: {python_dirs}")

    @classmethod
    def create_job(cls, name: str, job_type: str, job_def: Dict[str, Any]) -> JobABC:
        if job_type not in cls._job_types_registry:
            logger.error(f"*** Unknown job type: {job_type} ***")
            raise ValueError(f"Unknown job type: {job_type}")
        
        properties = job_def.get('properties', {})
        if not properties:
            logger.info(f"No properties specified for job {name} of type {job_type}")
            
        return cls._job_types_registry[job_type](name, properties)

    @classmethod
    def register_job_type(cls, type_name: str, job_class: Type[JobABC]):
        cls._job_types_registry[type_name] = job_class

    @classmethod
    def register_pydantic_type(cls, type_name: str, model_class: Type[BaseModel]):
        """Register a Pydantic model type with the factory"""
        cls._pydantic_types_registry[type_name] = model_class

    @classmethod
    def get_pydantic_class(cls, type_name: str) -> Type[BaseModel]:
        """Retrieve a registered Pydantic model class by its type name."""
        if type_name not in cls._pydantic_types_registry:
            raise ValueError(f"Pydantic type {type_name} not registered.")
        return cls._pydantic_types_registry[type_name]

    @classmethod
    def get_head_jobs_from_config(cls) -> Collection[JobABC]:
        JobFactory.load_python_into_registries(ConfigLoader.directories)
        """Create job graphs from configuration, using cache if available"""
        if cls._cached_job_graphs is None:
            job_graphs: list[JobABC] = []
            graphs_config = ConfigLoader.get_graphs_config()
            graph_names = list(graphs_config.keys())
            for graph_name in graph_names:
                graph_def = graphs_config[graph_name]
                job_names_in_graph = list(graph_def.keys())
                param_groups_for_graph_name = ConfigLoader.get_parameters_config().get(graph_name, {})
                if param_groups_for_graph_name:
                    param_jobs_graphs: List[JobABC] = cls.create_job_graph_using_parameters(graph_def, graph_name,
                                                                                            param_groups_for_graph_name,
                                                                                            job_names_in_graph)
                    job_graphs += param_jobs_graphs
                else:
                    job_graph_no_params: JobABC = cls.create_job_graph_no_params(graph_def, graph_name,
                                                                                 job_names_in_graph)
                    job_graphs.append(job_graph_no_params)
            cls._cached_job_graphs = job_graphs
        return cls._cached_job_graphs

    @classmethod
    def create_job_graph_using_parameters(cls, graph_def, graph_name, param_groups_for_graph_name,
                                          job_names_in_graph) -> List[JobABC]:
        job_graphs: list[JobABC] = []
        parameter_names_list = list(param_groups_for_graph_name.keys())
        for parameter_name in parameter_names_list:
            job_instances: dict[str, JobABC] = {}
            for graph_job_name in job_names_in_graph:
                raw_job_def: Dict[str, Any] = ConfigLoader.get_jobs_config()[graph_job_name]
                if ConfigLoader.is_parameterized_job(raw_job_def):
                    job_def: Dict[str, Any] = ConfigLoader.fill_job_with_parameters(raw_job_def, graph_name, parameter_name)
                else:
                    job_def = raw_job_def
                unique_job_name = graph_name + "$$" + parameter_name + "$$" + graph_job_name + "$$"
                job_type: str = job_def["type"]
                job: JobABC = cls.create_job(unique_job_name, job_type, job_def)
                job_instances[graph_job_name] = job
            job_graph: JobABC = cls.create_job_graph(graph_def, job_instances)
            job_graphs.append(job_graph)
        return job_graphs

    @classmethod
    def create_job_graph_no_params(cls, graph_def, graph_name, job_names_in_graph)-> JobABC:
        job_instances: dict[str, JobABC] = {}
        for graph_job_name in job_names_in_graph:
                job_def: Dict[str, Any] = ConfigLoader.get_jobs_config()[graph_job_name]
                unique_job_name = graph_name + "$$" + "$$" + graph_job_name +"$$"
                job_type: str = job_def["type"]
                job: JobABC = cls.create_job(unique_job_name, job_type, job_def)
                job_instances[graph_job_name] = job
        job_graph: JobABC = cls.create_job_graph(graph_def, job_instances)
        return job_graph

    @classmethod
    def create_job_graph(cls, graph_definition: dict[str, dict], job_instances: dict[str, JobABC]) -> JobABC:
        """
        graph definition defines the job graph and looks like this:

        graph_definition: dict[str, Any] = {
            "A": {"next": ["B", "C"]},
            "B": {"next": ["D"]},
            "C": {"next": ["D"]},
            "D": {"next": []},
        }

        job instances are a dictionary of job instances in the job graph and looks like this:

        job_instances: dict[str, JobABC] = {
            "A": SimpleJob("A"),
            "B": SimpleJob("B"),
            "C": SimpleJob("C"),
            "D": SimpleJob("D"),
        }
        
        """
        from jobchain.jobs.default_jobs import DefaultHeadJob
        nodes:dict[str, JobABC] = {} # nodes holds Jobs which will be hydrated with next_jobs 
                                    # and expected_inputs fields from the graph_definition.
        for job_name in graph_definition:
            job_obj = job_instances[job_name]
            nodes[job_name] = job_obj

        # determine the incoming edges i.e the Jobs that each Job depends on
        # so we can determine the head node ( which depends on no Jobs) 
        # and set the expected_inputs (i.e. the dependencies) for each Job.
        incoming_edges: dict[str, set[str]] = {job_name: set() for job_name in graph_definition}
        for job_name, config in graph_definition.items():
            for next_job in config['next']:
                incoming_edges[next_job].add(job_name)
        
        # 1) Find the head node (node with no incoming edges)
        head_jobs = [job_name for job_name, inputs in incoming_edges.items() if not inputs]
        
        if len(head_jobs) > 1:
            # Get naming from first job instance in job_instances
            sample_job = next(iter(job_instances.values()))
            sample_name = sample_job.name
            parsed = JobABC.parse_job_name(sample_name)
            
            # Debug logging for sample name and parsed name
            logger.debug(f"DEBUG - Sample name (long): {sample_name}")
            logger.debug(f"DEBUG - Parsed name (short): {parsed}")
            
            if parsed != 'UNSUPPORTED NAME FORMAT':
                # Replace the short job name with "DefaultHeadJob" while maintaining the $$ format
                # Example: "multi_head_demo$$params1$$head_job_alpha$$" -> "multi_head_demo$$params1$$DefaultHeadJob$$"
                new_name = sample_name.replace(parsed + "$$", "DefaultHeadJob$$")
                default_head = DefaultHeadJob(name=new_name)
                logger.debug(f"Constructed DefaultHeadJob name: {new_name}")
            else:
                default_head = DefaultHeadJob()
                logger.warning("Falling back to default naming for head job")
            
            logger.debug(f"Created DefaultHeadJob with name: {default_head.name}")
            job_instances[default_head.name] = default_head
            graph_definition[default_head.name] = {"next": head_jobs}
            head_job_name = default_head.name
            
            # Add the default head job to nodes dictionary
            nodes[default_head.name] = default_head
        elif len(head_jobs) == 1:
            head_job_name = head_jobs[0]
        else:
            raise ValueError("No head nodes found in graph definition")

        # 2) Set next_jobs for each node
        for job_name, config in graph_definition.items():
            nodes[job_name].next_jobs = [nodes[next_name] for next_name in config['next']]

        # 3) Set expected_inputs for each node using fully qualified names
        for job_name, input_job_names_set in incoming_edges.items():
            if input_job_names_set:  # if node has incoming edges
                # Transform short names to fully qualified names using the job_instances dictionary
                nodes[job_name].expected_inputs = {job_instances[input_name].name for input_name in input_job_names_set}

        # 4) Set reference to final node in head node -- not needed!
        # Find node with no next jobs
        # final_job_name = next(job_name for job_name, config in graph_definition.items() 
        #                    if not config['next'])
        # nodes[head_job_name].final_node = nodes[final_job_name]

        return nodes[head_job_name]


class ConfigLoader:
    # Directories are searched in order. If a valid jobchain directory is found,
    # the search stops and uses that directory.
    # TODO: Nice to have - Add support for merging configurations from multiple directories
    #       if required in the future.
    directories: List[str] = [
        os.path.join(os.getcwd(), "jobchain"),  # jobchain directory in current working directory
        os.path.join(os.path.expanduser("~"), "jobchain"),  # ~/jobchain
        "/etc/jobchain"
    ]
    _cached_configs: Dict[str, dict] = None

    @classmethod
    def _set_directories(cls, directories):
        """Set the directories and clear the cache"""
        cls.directories = directories
        cls._cached_configs = None

    @classmethod
    def __setattr__(cls, name, value):
        """Clear cache when directories are changed"""
        super().__setattr__(name, value)
        if name == 'directories':
            cls._cached_configs = None

    @classmethod
    def load_configs_from_dirs(
            cls,
            directories: List[str] = [],
            config_bases: List[str] = ['graphs', 'jobs', 'parameters', 'jobchain_all'],
            allowed_extensions: tuple = ('.yaml', '.yml', '.json')
    ) -> Dict[str, dict]:
        """
        Load configuration files from directories. Will search directories in order and stop
        at the first valid jobchain directory found.
        
        Args:
            directories: List of directory paths to search
            config_bases: List of configuration file base names to look for
            allowed_extensions: Tuple of allowed file extensions
        
        Returns:
            Dictionary with config_base as key and loaded config as value
            
        Raises:
            FileNotFoundError: If no valid jobchain directory is found in any of the directories
            ConfigurationError: If configuration files are malformed
        """
        configs: Dict[str, dict] = {}
        config_files: Dict[str, str] = {}  # Track which file each config came from

        # Convert directories to Path objects
        dir_paths = [Path(str(d)) for d in directories]
        logger.info(f"Looking for config files in directories: {dir_paths}")

        found_valid_dir = False
        for dir_path in dir_paths:
            if not dir_path.exists():
                logger.info(f"Directory not found, skipping: {dir_path}")
                continue

            # Check if any config files exist in this directory
            has_configs = False
            for config_base in config_bases:
                for ext in allowed_extensions:
                    if (dir_path / f"{config_base}{ext}").exists():
                        has_configs = True
                        break
                if has_configs:
                    break

            if has_configs:
                found_valid_dir = True
                logger.info(f"Found valid jobchain directory: {dir_path}")
                # Load configs from this directory only
                for config_base in config_bases:
                    for ext in allowed_extensions:
                        config_path = dir_path / f"{config_base}{ext}"
                        if config_path.exists():
                            try:
                                with open(config_path) as f:
                                    configs[config_base] = yaml.safe_load(f)
                                    config_files[config_base] = str(config_path)
                            except yaml.YAMLError as e:
                                error_msg = "Configuration is malformed. Unable to proceed with job execution.\n\n" \
                                        "-------------------------------------------------------\n" \
                                        "          Configuration file malformed - cannot continue\n" \
                                        "-------------------------------------------------------\n" \
                                        f"File: {config_path}\n" \
                                        f"Error details: {str(e)}\n" \
                                        "-------------------------------------------------------"
                                raise ConfigurationError(error_msg) from e
                break  # Stop searching after finding first valid directory

        if not found_valid_dir:
            raise FileNotFoundError(f"No valid jobchain directory found in search paths: {dir_paths}")

        # Store file paths in configs
        configs['__files__'] = config_files
        return configs

    @classmethod
    def _extract_config_section(cls, configs: Dict[str, dict], section_name: str) -> dict:
        """
        Extract a configuration section from either a dedicated file or jobchain_all.
        
        Args:
            configs: Dictionary containing all configurations
            section_name: Name of the section to extract (e.g., 'graphs', 'jobs', 'parameters')
            
        Returns:
            Dictionary containing the configuration section, or empty dict if not found
        """
        # Try to get from dedicated file first
        if section_name in configs:
            return configs[section_name]

        # If not found, try to get from jobchain_all
        if 'jobchain_all' in configs and isinstance(configs['jobchain_all'], dict):
            return configs['jobchain_all'].get(section_name, {})

        # If nothing found, return empty dict
        return {}

    @classmethod
    def _find_parameterized_fields(cls, job_config: dict) -> set:
        """
        Find all parameterized fields in a job configuration.
        A field is parameterized if its value starts with '$'.
        
        Args:
            job_config: Job configuration dictionary
            
        Returns:
            Set of parameterized field names
        """
        params = set()

        def search_dict(d):
            for k, v in d.items():
                if isinstance(v, dict):
                    search_dict(v)
                elif isinstance(v, str) and v.startswith('$'):
                    params.add(v[1:])  # Remove the '$' prefix

        search_dict(job_config.get('properties', {}))
        return params

    @classmethod
    def _validate_graph_structure(cls, graph_def: dict, defined_jobs: set, graph_name: str) -> None:
        """
        Validate the structure of a job graph.
        - Checks for cycles
        - Ensures all referenced jobs exist
        - Validates head/tail nodes
        
        Args:
            graph_def: Graph definition from configuration
            defined_jobs: Set of all defined job names
            graph_name: Name of the graph being validated
            
        Raises:
            ValueError: If validation fails
        """
        # First validate all jobs exist and are properly connected
        for job, job_def in graph_def.items():
            if job not in defined_jobs:
                raise ValueError(f"Job '{job}' in graph '{graph_name}' is not defined")
            for next_job in job_def.get('next', []):
                if next_job not in defined_jobs:
                    raise ValueError(f"Job '{next_job}' referenced in 'next' field of job '{job}' in graph '{graph_name}' is not defined in jobs configuration")
                    
        # Build adjacency list after validating jobs
        adjacency = {job: job_def.get('next', []) for job, job_def in graph_def.items()}
        
        # Check for cycles using DFS
        visited = set()
        rec_stack = set()
        
        def has_cycle(node):
            visited.add(node)
            rec_stack.add(node)
            
            for neighbor in adjacency[node]:
                if neighbor not in visited:
                    if has_cycle(neighbor):
                        return True
                elif neighbor in rec_stack:
                    return True
                    
            rec_stack.remove(node)
            return False
            
        # Run cycle detection from each unvisited node
        for job in adjacency:
            if job not in visited:
                if has_cycle(job):
                    raise ValueError(f"Cycle detected in graph '{graph_name}'")
                    
    @classmethod
    def validate_configs(cls, configs: Dict[str, dict]) -> None:
        """
        Validate that:
        1. All jobs referenced in graphs exist in jobs configuration
        2. All parameterized jobs have corresponding parameter values
        3. Each graph structure is valid (no cycles, proper head/tail nodes, valid references)
        
        Args:
            configs: Dictionary containing all configurations
            
        Raises:
            ValueError: If validation fails
            ConfigurationError: If configuration is malformed (e.g. wrong types, missing required fields)
        """
        try:
            graphs_config = cls._extract_config_section(configs, 'graphs')
            jobs_config = cls._extract_config_section(configs, 'jobs')
            parameters_config = cls._extract_config_section(configs, 'parameters')
            config_files = configs.get('__files__', {})

            if not graphs_config or not jobs_config:
                return

            # First validate that all jobs in graphs exist
            defined_jobs = set(jobs_config.keys())

            # Track which config we're currently validating
            current_config = 'jobs'
            
            # Validate jobs config structure
            for job_name, job_config in jobs_config.items():
                if not isinstance(job_config, dict):
                    raise TypeError(f"Job '{job_name}' configuration must be a dictionary")
                
            current_config = 'graphs'
            for graph_name, graph_definition in graphs_config.items():
                # Validate graph structure (no cycles, etc)
                print(f"\nChecking {graph_name} for cycles...")
                cls._validate_graph_structure(graph_definition, defined_jobs, graph_name)
                print("No cycles detected")
                validate_graph(graph_definition, graph_name)

                # Find all parameterized jobs in this graph
                graph_parameterized_jobs = {}
                for job_name in graph_definition.keys():
                    job_config = jobs_config[job_name]
                    params = cls._find_parameterized_fields(job_config)
                    if params:
                        graph_parameterized_jobs[job_name] = params

                # If graph has parameterized jobs, it must have parameters
                if graph_parameterized_jobs:
                    current_config = 'parameters'
                    if graph_name not in parameters_config:
                        raise ValueError(
                            f"Graph '{graph_name}' contains parameterized jobs {list(graph_parameterized_jobs.keys())} but has no entry in parameters configuration")

                    parameters_for_graph = parameters_config[graph_name]

                    # Validate parameter groups
                    for param_name in parameters_for_graph.keys():
                        if not param_name.startswith('params'):
                            raise ValueError(
                                f"Invalid parameter group name '{param_name}' in graph '{graph_name}'. Parameter groups must start with 'params'")

                    # Validate that all parameters are filled for each group
                    for param_name, parameterized_jobs in parameters_for_graph.items():
                        for job_name, required_params in graph_parameterized_jobs.items():
                            if job_name not in parameterized_jobs:
                                raise ValueError(
                                    f"Job '{job_name}' in graph '{graph_name}' requires parameters {required_params} but has no entry in parameter group '{param_name}'")

                            # Each job should have a list of parameter sets
                            job_param_sets = parameterized_jobs[job_name]
                            if not isinstance(job_param_sets, list):
                                raise ValueError(
                                    f"Parameters for job '{job_name}' in graph '{graph_name}', group '{param_name}' should be a list of parameter sets")

                            # Validate each parameter set
                            for param_set in job_param_sets:
                                missing_params = required_params - set(param_set.keys())
                                if missing_params:
                                    raise ValueError(
                                        f"Parameter set for job '{job_name}' in graph '{graph_name}', group '{param_name}' is missing required parameters: {missing_params}")

                print(f"Graph {graph_name} passed all validations")

        except (AttributeError, TypeError, KeyError) as e:
            # Get the relevant file path based on which config we were validating
            error_file = config_files.get(current_config, 'unknown file')
            error_msg = "Configuration is malformed. Unable to proceed with job execution.\n\n" \
                        "-------------------------------------------------------\n" \
                        "          Configuration file malformed - cannot continue\n" \
                        "-------------------------------------------------------\n" \
                        f"File: {error_file}\n" \
                        f"Error details: {str(e)}\n" \
                        "-------------------------------------------------------"
            raise ConfigurationError(error_msg) from e

    @classmethod
    def load_all_configs(cls) -> Dict[str, dict]:
        """Load all configurations and validate them"""
        if cls._cached_configs is None:
            cls._cached_configs = cls.load_configs_from_dirs(cls.directories)
            cls.validate_configs(cls._cached_configs)
        return cls._cached_configs

    @classmethod
    def reload_configs(cls) -> Dict[str, dict]:
        """Force a reload of all configurations."""
        logger.info("Reloading configs...")
        cls._cached_configs = None
        return cls.load_all_configs()

    @classmethod
    def get_graphs_config(cls) -> dict:
        """
        Get graphs configuration from either dedicated graphs file or jobchain_all.
        Returns empty dict if no configuration is found.
        """
        configs = cls.load_all_configs()
        return cls._extract_config_section(configs, 'graphs')

    @classmethod
    def get_jobs_config(cls) -> dict:
        """
        Get jobs configuration from either dedicated jobs file or jobchain_all.
        Returns empty dict if no configuration is found.
        """
        configs = cls.load_all_configs()
        return cls._extract_config_section(configs, 'jobs')

    @classmethod
    def get_parameters_config(cls) -> dict:
        """
        Get parameters configuration from either dedicated parameters file or jobchain_all.
        Returns empty dict if no configuration is found.
        """
        configs = cls.load_all_configs()
        return cls._extract_config_section(configs, 'parameters')

    @classmethod
    def is_parameterized_job(cls, raw_job_def):
        """
        Check if a job definition contains parameterized fields.
        
        Args:
            raw_job_def: Raw job definition from jobs.yaml
            
        Returns:
            bool: True if job has parameterized fields, False otherwise
        """
        if not isinstance(raw_job_def, dict):
            return False
            
        # Use existing method to find parameterized fields
        params = cls._find_parameterized_fields(raw_job_def)
        return len(params) > 0

    @classmethod
    def fill_job_with_parameters(cls, job_config: dict, graph_name: str, parameter_name: str) -> dict:
        """
        Fill a job configuration with parameters from parameters.yaml.
        
        Args:
            job_config: Raw job configuration from jobs.yaml
            graph_name: Name of the graph containing the job
            parameter_name: Name of the parameter group to use
            
        Returns:
            dict: Job configuration with parameters filled in
        """
        # Deep copy the job config to avoid modifying the original
        import copy
        filled_config = copy.deepcopy(job_config)
        
        # Get parameters for this job from parameters.yaml
        params_config = cls.get_parameters_config()
        if graph_name not in params_config or parameter_name not in params_config[graph_name]:
            raise ValueError(f"No parameters found for graph '{graph_name}' and parameter group '{parameter_name}'")
            
        # Get the job name by finding which job in the parameters matches this config
        job_name = None
        for job in params_config[graph_name][parameter_name].keys():
            if job_config == cls.get_jobs_config()[job]:
                job_name = job
                break
                
        if job_name is None:
            raise ValueError(f"Could not find job in parameters for graph '{graph_name}' and group '{parameter_name}'")
            
        # Get parameter values for this job
        param_sets = params_config[graph_name][parameter_name][job_name]
        if not param_sets or not isinstance(param_sets, list):
            raise ValueError(f"Invalid parameter sets for job '{job_name}' in graph '{graph_name}', group '{parameter_name}'")
            
        # Use the first parameter set (as defined in the spec)
        param_values = param_sets[0]
        
        def replace_params(obj, params):
            if isinstance(obj, dict):
                return {k: replace_params(v, params) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [replace_params(item, params) for item in obj]
            elif isinstance(obj, str) and obj.startswith('$'):
                param_name = obj[1:]  # Remove '$' prefix
                if param_name not in params:
                    raise ValueError(f"Parameter '{param_name}' not found in parameter set")
                return params[param_name]
            return obj
            
        # Replace all parameterized values in the config
        filled_config = replace_params(filled_config, param_values)
        return filled_config


================================================
File: src/jobchain/taskmanager.py
================================================
import asyncio
import threading
from collections import deque

import jobchain.jc_logging as logging

from .job_loader import JobFactory


class TaskManager:
    _instance = None
    _lock = threading.Lock()  # Class-level lock for singleton creation

    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance.__initialized = False
        return cls._instance

    def __init__(self):
        if not hasattr(self, '_TaskManager__initialized') or not self.__initialized:
            with self._lock:
                if not hasattr(self, '_TaskManager__initialized') or not self.__initialized:
                    self._initialize()
                    self.__initialized = True

    def _initialize(self):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.loop = asyncio.new_event_loop()
        self.thread = threading.Thread(target=self._run_loop, daemon=True)
        self.thread.start()
        self.head_jobs = JobFactory.get_head_jobs_from_config()

        self.submitted_count = 0
        self.completed_count = 0
        self.error_count = 0
        self.completed_results = deque()
        self.error_results = deque()

        self._data_lock = threading.Lock()

    def _run_loop(self):
        asyncio.set_event_loop(self.loop)
        self.loop.run_forever()

    def submit(self, func, *args, **kwargs):
        with self._data_lock:
            self.submitted_count += 1

        try:
            coro = func(*args, **kwargs)
        except Exception as e:
            self.logger.error(f"Error processing result: {e}")
            self.logger.info("Detailed stack trace:", exc_info=True)
            with self._data_lock:
                self.error_count += 1
                self.error_results.append(
                    (e, {'func': func, 'args': args, 'kwargs': kwargs})
                )
            return

        future = asyncio.run_coroutine_threadsafe(coro, self.loop)
        future.add_done_callback(
            lambda f: self._handle_completion(f, func, args, kwargs)
        )

    def _handle_completion(self, future, func, args, kwargs):
        try:
            result = future.result()
            with self._data_lock:
                self.completed_count += 1
                self.completed_results.append(
                    (result, {'func': func, 'args': args, 'kwargs': kwargs})
                )
        except Exception as e:
            self.logger.error(f"Error processing result: {e}")
            self.logger.info("Detailed stack trace:", exc_info=True)
            with self._data_lock:
                self.error_count += 1
                self.error_results.append(
                    (e, {'func': func, 'args': args, 'kwargs': kwargs})
                )

    def get_counts(self):
        with self._data_lock:
            return {
                'submitted': self.submitted_count,
                'completed': self.completed_count,
                'errors': self.error_count
            }

    def pop_results(self):
        with self._data_lock:
            completed = list(self.completed_results)
            errors = list(self.error_results)
            self.completed_results.clear()
            self.error_results.clear()
            return {
                'completed': completed,
                'errors': errors
            }



================================================
File: src/jobchain/jobs/__init__.py
================================================
from jobchain.jobs.llm_jobs import OpenAIJob

__all__ = ['OpenAIJob']

================================================
File: src/jobchain/jobs/default_jobs.py
================================================
from typing import Any, Dict, Union

from jobchain import jc_logging as logging
from jobchain.job import JobABC, Task

logger = logging.getLogger(__name__)


class DefaultHeadJob(JobABC):
    """A Job implementation that provides a simple default behavior."""
    
    async def run(self, task: Union[Dict[str, Any], Task]) -> Dict[str, Any]:
        """Run a simple job that logs and returns the task."""
        logger.info(f"Default head JOB for {task}")
        return {}

================================================
File: src/jobchain/jobs/llm_jobs.py
================================================
import os
from typing import Any, Dict, Optional, Union

from aiolimiter import AsyncLimiter
from dotenv import load_dotenv
from openai import AsyncOpenAI

import jobchain.jc_logging as logging
from jobchain.job import JobABC
from jobchain.job_loader import JobFactory
from jobchain.utils.llm_utils import check_response_errors, clean_prompt

logger = logging.getLogger("OpenAIJob")

class OpenAIClient:
    """
    Singleton class for AsyncOpenAI client.
    """
    _client = None

    @classmethod
    def get_client(cls, params: Dict[str, Any] = None):
        if cls._client is None:
            # Load environment variables from api.env file
            load_dotenv("api.env")

            # Initialize params if None
            params = params or {}

            # Handle special parameters
            api_key = os.getenv(params.pop("api_key", None)) if "api_key" in params else os.getenv('OPENAI_API_KEY')
            logger.info(f"Resolved API Key exists: {bool(api_key)}")
            
            # Optional: Check if the API key is not set and raise an error 
            if not api_key:
                raise ValueError("API key is not set. Please provide an API key.")
            
            # Create client with remaining params
            cls._client = AsyncOpenAI(api_key=api_key, **params)
            logger.info(f"Created client with base_url: {params.get('base_url', 'default')}")
        return cls._client

class OpenAIJob(JobABC):

    # Shared AsyncLimiter for all jobs, default to 5,000 requests per minute
    default_rate_limit = {"max_rate": 5000, "time_period": 60}

    def __init__(self, name: Optional[str] = None, properties: Dict[str, Any] = {}):
        """
        Call JobChain.submit_task({"prompt": prompt}), when submitting a task to the JobChain.
        Initialize an OpenAIJob instance with a properties dict containing three top-level keys, client, api, and rate_limit.
        All properties are optional.

        Args:
            name (Optional[str], optional): 
                A unique identifier for this job within the context of a JobChain.
                The name must be unique among all jobs in the same JobChain to ensure proper job identification 
                and dependency resolution. If not provided, a unique name will be auto-generated.

            properties (Dict[str, Any], optional): Optional properties for the job. A dictionary containing the following keys:

            {
                rate_limit: {
                    max_rate: Allow up to max_rate / time_period acquisitions before blocking.
                    time_period: duration of the time period in which to limit the rate. Note that up to max_rate acquisitions are allowed within this time period in a burst
                },
                client: {
                    api_key: str | None = None,
                    organization: str | None = None,
                    project: str | None = None,
                    base_url: str | URL | None = None,
                    websocket_base_url: str | URL | None = None,
                    timeout: float | Timeout | NotGiven | None = NOT_GIVEN,
                    max_retries: int = DEFAULT_MAX_RETRIES,
                    default_headers: Mapping[str, str] | None = None,
                    default_query: Mapping[str, object] | None = None,
                    http_client: AsyncClient | None = None,
                    _strict_response_validation: bool = False
                },
                api: {
                    messages: Iterable[ChatCompletionMessageParam],
                    model: ChatModel | str,
                    audio: ChatCompletionAudioParam | NotGiven | None = NOT_GIVEN,
                    frequency_penalty: float | NotGiven | None = NOT_GIVEN,
                    function_call: FunctionCall | NotGiven = NOT_GIVEN,
                    functions: Iterable[Function] | NotGiven = NOT_GIVEN,
                    logit_bias: Dict[str, int] | NotGiven | None = NOT_GIVEN,
                    logprobs: bool | NotGiven | None = NOT_GIVEN,
                    max_completion_tokens: int | NotGiven | None = NOT_GIVEN,
                    max_tokens: int | NotGiven | None = NOT_GIVEN,
                    metadata: Dict[str, str] | NotGiven | None = NOT_GIVEN,
                    modalities: List[ChatCompletionModality] | NotGiven | None = NOT_GIVEN,
                    n: int | NotGiven | None = NOT_GIVEN,
                    parallel_tool_calls: bool | NotGiven = NOT_GIVEN,
                    prediction: ChatCompletionPredictionContentParam | NotGiven | None = NOT_GIVEN,
                    presence_penalty: float | NotGiven | None = NOT_GIVEN,
                    reasoning_effort: ChatCompletionReasoningEffort | NotGiven = NOT_GIVEN,
                    response_format: ResponseFormat | NotGiven | None = NOT_GIVEN,
                    seed: int | NotGiven | None = NOT_GIVEN,
                    service_tier: NotGiven | Literal['auto', 'default'] | None = NOT_GIVEN,
                    stop: str | List[str] | NotGiven | None = NOT_GIVEN,
                    store: bool | NotGiven | None = NOT_GIVEN,
                    stream: NotGiven | Literal[False] | None = NOT_GIVEN,
                    stream_options: ChatCompletionStreamOptionsParam | NotGiven | None = NOT_GIVEN,
                    temperature: float | NotGiven | None = NOT_GIVEN,
                    tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven = NOT_GIVEN,
                    tools: Iterable[ChatCompletionToolParam] | NotGiven = NOT_GIVEN,
                    top_logprobs: int | NotGiven | None = NOT_GIVEN,
                    top_p: float | NotGiven | None = NOT_GIVEN,
                    user: str | NotGiven = NOT_GIVEN,
                    extra_headers: Headers | None = None,
                    extra_query: Query | None = None,
                    extra_body: Body | None = None,
                    timeout: float | Timeout | NotGiven | None = NOT_GIVEN
                }
            }
        """
        super().__init__(name, properties)
        
        # Initialize OpenAI client with properties
        self.client = OpenAIClient.get_client(self.properties.get("client", {}))
        
        # Rate limiter configuration
        rate_limit_config = self.properties.get("rate_limit", self.default_rate_limit)
        self.limiter = AsyncLimiter(**rate_limit_config)

        # Extract other relevant properties for OpenAI client
        self.api_properties = self.properties.get("api", {})

    async def run(self, task: Union[Dict[str, Any], Any]) -> Dict[str, Any]:
        """
        Perform an OpenAI API call while adhering to rate limits.
        
        Args:
            task: A dictionary containing either:
                - prompt: str - The prompt to send to the model
                - messages: list - Direct message format for the API
                Or any other valid parameters for the chat.completions.create API
        """
        # Start with default properties
        request_properties = {
            "model": "gpt-4o",
            "temperature": 0.7,
            "messages": [
                {"role": "system", "content": "You are a helpful assistant"},
                {"role": "user", "content": "You are a helpful assistant."}
            ]
        }
        
        # Add API properties from initialization
        request_properties.update(self.api_properties)

        # Check if response_format is a string and replace with the Pydantic class
        if "response_format" in request_properties and isinstance(request_properties["response_format"], str):
            try:
                response_format_name = request_properties["response_format"]
                request_properties["response_format"] = JobFactory.get_pydantic_class(response_format_name)
                logger.info(f"Successfully replaced response_format string with Pydantic class: {response_format_name}")
            except ValueError as e:
                logger.error(f"Could not find Pydantic class for response_format: {request_properties['response_format']}. Error: {e}")
            except Exception as e:
                logger.error(f"An unexpected error occurred while trying to get the Pydantic class: {e}")

        self.create_prompt(request_properties, task)

        # Acquire the rate limiter before making the request
        async with self.limiter:
            try:
                logger.info(f"{self.name} is making an OpenAI API call.")
                if "response_format" in request_properties:
                    response = await self.client.beta.chat.completions.parse(**request_properties)
                else:
                    response = await self.client.chat.completions.create(**request_properties)
                logger.info(f"{self.name} received a response.")
                
                # Handle the response
                if hasattr(response, 'choices') and response.choices:
                    if "response_format" in request_properties:
                        return response.choices[0].message.parsed
                    else:
                        return {"response": response.choices[0].message.content}
                else:
                    return {"error": "No valid response content found"}
            except Exception as e:
                logger.error(f"Error in {self.name}: {e}")
                return {"error": str(e)}

    def create_prompt(self, request_properties, task):
        # Handle the task input
        if isinstance(task, dict):
            # If task has a prompt, convert it to messages format
            if "prompt" in task:
                prompt = clean_prompt(task["prompt"])
                request_properties["messages"] = [
                    {"role": "system", "content": "You are a helpful assistant"},
                    {"role": "user", "content": prompt}
                ]
            # If task already has messages, use those
            elif "messages" in task:
                request_properties["messages"] = task["messages"]

            # Add any other valid API parameters from task
            # request_properties.update({k: v for k, v in task.items() if k not in ["prompt", "messages"]})
        elif task:  # If task is not empty and not a dict
            # If task is not a dict, treat it as the prompt
            prompt = clean_prompt(str(task))
            request_properties["messages"] = [
                {"role": "system", "content": "You are a helpful assistant"},
                {"role": "user", "content": prompt}
            ]

================================================
File: src/jobchain/resources/otel_config.yaml
================================================
exporter: file  # Default exporter is file; can be overridden by OTEL_TRACES_EXPORTER env variable.
service_name: MyService  # Can be overridden by OTEL_SERVICE_NAME env variable.
batch_processor:
  max_queue_size: 1000  # Batch processor will handle up to 1000 spans in queue.
  schedule_delay_millis: 1000  # 1-second timeout for exporting spans.
file_exporter:
  path: "~/.JobChain/otel_trace.json"  # Default path for trace export
  max_size_bytes: 5242880  # 5MB (5 * 1024 * 1024 bytes)
  rotation_time_days: 1  # Rotate daily


================================================
File: src/jobchain/utils/__init__.py
================================================
# Make utils a Python package
from .otel_wrapper import TracerFactory, trace_function
from .timing import timing_decorator

__all__ = ['TracerFactory', 'trace_function', 'timing_decorator']


================================================
File: src/jobchain/utils/llm_utils.py
================================================
import re
import string

import jobchain.jc_logging as logging

logger = logging.getLogger(__name__)

def clean_prompt(text):
    # Keep only printable characters
    return ''.join(char for char in text if char in string.printable)


def clean_prompt(text):
    if not isinstance(text, str):
        logger.error("Input must be a string")
        raise ValueError("Input must be a string")
    
    # Remove control characters but keep normal whitespace
    cleaned_1 = ''.join(char for char in text if ord(char) >= 32 or char in '\n\r\t')
    cleaned = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F]', '', cleaned_1)
    # Optional: Check if the text was modified
    if cleaned != text:
        logger.info("Characters were cleaned from the prompt")
    
    # Optional: Ensure the text isn't empty after cleaning
    if not cleaned.strip():
        logger.error("Prompt is empty after cleaning")
        raise ValueError("Prompt is empty after cleaning")
    
    return cleaned

def check_response_errors(response:dict):
    if response.get("error"):
        logger.error(f"Response has an error: {response}")
        raise ValueError("Response has an error")
    elif response.get("status"):
        status = response.get("status")
        if status == "error":
            logger.error(f"Response has an error: {response}")
            raise ValueError("Response has an error")


================================================
File: src/jobchain/utils/monitor_utils.py
================================================
"""Utilities for monitoring and logging task progress."""
import asyncio

NO_CHANGE_LOG_INTERVAL = 1.0

def should_log_task_stats(monitor_fn, tasks_created: int, tasks_completed: int) -> bool:
    """Check if task stats should be logged based on changes or time elapsed.
    
    Args:
        monitor_fn: The monitoring function to store state on
        tasks_created: Current count of created tasks
        tasks_completed: Current count of completed tasks
        
    Returns:
        bool: True if stats should be logged
    """
    if not hasattr(monitor_fn, '_last_log_time'):
        monitor_fn._last_log_time = 0
        monitor_fn._last_tasks_created = -1
        monitor_fn._last_tasks_completed = -1
    
    current_time = asyncio.get_event_loop().time()
    counts_changed = (tasks_created != monitor_fn._last_tasks_created or 
                     tasks_completed != monitor_fn._last_tasks_completed)
    
    should_log = counts_changed or (current_time - monitor_fn._last_log_time) >= NO_CHANGE_LOG_INTERVAL
    
    if should_log:
        monitor_fn._last_log_time = current_time
        monitor_fn._last_tasks_created = tasks_created
        monitor_fn._last_tasks_completed = tasks_completed
        
    return should_log


================================================
File: src/jobchain/utils/otel_wrapper.py
================================================
import inspect
import json
import os
from functools import wraps
from importlib import resources
from threading import Lock
from typing import Any, Dict, Optional, Sequence

import yaml
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import \
    OTLPSpanExporter
from opentelemetry.sdk.trace import ReadableSpan, TracerProvider
from opentelemetry.sdk.trace.export import (BatchSpanProcessor,
                                            ConsoleSpanExporter, SpanExporter,
                                            SpanExportResult)

# Explicitly define exports
__all__ = ['TracerFactory', 'trace_function', 'AsyncFileExporter']
DEFAULT_OTEL_CONFIG = "otel_config.yaml"

class AsyncFileExporter(SpanExporter):
    """Asynchronous file exporter for OpenTelemetry spans with log rotation support."""
    
    def __init__(self, filepath: str, max_size_bytes: int = None, rotation_time_days: int = None):
        """Initialize the exporter with the target file path and rotation settings.
        
        Args:
            filepath: Path to the file where spans will be exported
            max_size_bytes: Maximum file size in bytes before rotation
            rotation_time_days: Number of days before rotating file
        """
        self.filepath = os.path.expanduser(filepath)
        self.max_size_bytes = max_size_bytes
        self.rotation_time_days = rotation_time_days
        self.last_rotation_time = None
        
        # Ensure directory exists
        os.makedirs(os.path.dirname(self.filepath), exist_ok=True)
        self._export_lock = Lock()
        
        # Initialize file with empty array if it doesn't exist
        if not os.path.exists(self.filepath):
            with open(self.filepath, 'w') as f:
                json.dump([], f)
                
        # Record initial rotation time
        if self.rotation_time_days:
            self.last_rotation_time = os.path.getmtime(self.filepath)
    
    def _should_rotate(self, additional_size: int = 0) -> bool:
        """Check if file should be rotated based on size or time.
        
        Args:
            additional_size: Additional size in bytes that will be added
        """
        if not os.path.exists(self.filepath):
            return False
            
        should_rotate = False
        
        # Check size-based rotation
        if self.max_size_bytes:
            current_size = os.path.getsize(self.filepath)
            if (current_size + additional_size) >= self.max_size_bytes:
                should_rotate = True
                
        # Check time-based rotation
        if self.rotation_time_days and self.last_rotation_time:
            current_time = os.path.getmtime(self.filepath)
            days_elapsed = (current_time - self.last_rotation_time) / (24 * 3600)
            if days_elapsed >= self.rotation_time_days:
                should_rotate = True
                
        return should_rotate
    
    def _rotate_file(self):
        """Rotate the current file if it exists."""
        if not os.path.exists(self.filepath):
            return
            
        # Generate rotation suffix based on timestamp
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        rotated_path = f"{self.filepath}.{timestamp}"
        
        # Rotate the file
        os.rename(self.filepath, rotated_path)
        
        # Create new empty file
        with open(self.filepath, 'w') as f:
            json.dump([], f)
            
        # Update rotation time
        self.last_rotation_time = os.path.getmtime(self.filepath)
    
    def _serialize_span(self, span: ReadableSpan) -> dict:
        """Convert a span to a JSON-serializable dictionary.
        
        Args:
            span: The span to serialize
        Returns:
            dict: JSON-serializable representation of the span
        """
        return {
            'name': span.name,
            'context': {
                'trace_id': format(span.context.trace_id, '032x'),
                'span_id': format(span.context.span_id, '016x'),
            },
            'parent_id': format(span.parent.span_id, '016x') if span.parent else None,
            'start_time': span.start_time,
            'end_time': span.end_time,
            'attributes': dict(span.attributes),
            'events': [
                {
                    'name': event.name,
                    'timestamp': event.timestamp,
                    'attributes': dict(event.attributes)
                }
                for event in span.events
            ],
            'status': {
                'status_code': str(span.status.status_code),
                'description': span.status.description
            }
        }

    def export(self, spans: Sequence[ReadableSpan]) -> SpanExportResult:
        """Export spans to file with rotation support.
        
        Args:
            spans: Sequence of spans to export
        Returns:
            SpanExportResult indicating success or failure
        """
        try:
            with self._export_lock:
                # Create serializable span data
                span_data = [self._serialize_span(span) for span in spans]
                
                # Read existing spans
                try:
                    with open(self.filepath, 'r') as f:
                        try:
                            existing_spans = json.load(f)
                        except json.JSONDecodeError:
                            existing_spans = []
                except FileNotFoundError:
                    existing_spans = []
                
                # Calculate size of new data
                new_data = existing_spans + span_data
                new_data_str = json.dumps(new_data, indent=2)
                additional_size = len(new_data_str.encode('utf-8'))
                
                # Check rotation after calculating new size
                if self._should_rotate(additional_size - os.path.getsize(self.filepath) if os.path.exists(self.filepath) else 0):
                    self._rotate_file()
                    existing_spans = []
                
                # Append new spans
                existing_spans.extend(span_data)
                
                # Write all spans back to file
                temp_file = f"{self.filepath}.tmp"
                try:
                    with open(temp_file, 'w') as f:
                        json.dump(existing_spans, f, indent=2)
                    # Atomic replace
                    os.replace(temp_file, self.filepath)
                finally:
                    if os.path.exists(temp_file):
                        os.unlink(temp_file)
                
            return SpanExportResult.SUCCESS
        except Exception as e:
            print(f"Error exporting spans to file: {e}")
            return SpanExportResult.FAILURE

    def shutdown(self) -> None:
        """Shutdown the exporter."""
        pass

class TestTracerProvider(TracerProvider):
    _instance = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(TestTracerProvider, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if not hasattr(self, '_initialized'):
            super().__init__()
            self._initialized = True
            
    def get_tracer(
        self,
        instrumenting_module_name: str,
        instrumenting_library_version: str = None,
        schema_url: str = None,
        attributes: dict = None,
    ) -> trace.Tracer:
        """Get a tracer for use in tests.
        
        Args:
            instrumenting_module_name: The name of the instrumenting module
            instrumenting_library_version: Optional version of the instrumenting module
            schema_url: Optional URL of the OpenTelemetry schema
            attributes: Optional attributes to add to the tracer
            
        Returns:
            A tracer instance for use in tests
        """
        return super().get_tracer(
            instrumenting_module_name,
            instrumenting_library_version,
            schema_url,
            attributes,
        )

# Singleton TracerFactory
class TracerFactory:
    _instance = None
    _config = None
    _lock = Lock()
    _is_test_mode = False
    
    @classmethod
    def set_test_mode(cls, enabled: bool = True):
        """Enable or disable test mode.
        
        Args:
            enabled: Whether to enable test mode
        """
        cls._is_test_mode = enabled
        cls._instance = None  # Reset instance to force recreation with new provider
    
    @classmethod
    def _load_config(cls, yaml_file=None):
        """Load configuration from YAML file.
        
        Args:
            yaml_file: Optional path override for the YAML configuration file
        Returns:
            dict: Configuration dictionary
        """
       # First try yaml_file parameter
        config_path = yaml_file
        if not config_path:
            # Then try environment variable
            config_path = os.environ.get('JOBCHAIN_OT_CONFIG', "")
        
        if not config_path:
            # Finally use default path from package resources
            try:
                with resources.path('jobchain.resources', DEFAULT_OTEL_CONFIG) as path:
                    config_path = str(path)
            except Exception as e:
                raise RuntimeError(f"Could not find {DEFAULT_OTEL_CONFIG} in package resources: {e}")
        
        with open(config_path, 'r') as file:
                cls._config = yaml.safe_load(file)
        return cls._config
    
    @classmethod
    def get_tracer(cls, config=None):
        """Get or create the tracer instance.
        
        Args:
            config: Optional configuration override. If not provided, loads from file.
        Returns:
            Tracer instance
        """
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    # Use provided config or load from file
                    cfg = config if config is not None else cls._load_config()
                    
                    # Use TestTracerProvider in test mode
                    provider = TestTracerProvider() if cls._is_test_mode else TracerProvider()
                    
                    # Configure main exporter
                    main_exporter = cls._configure_exporter(cfg['exporter'])
                    batch_processor = BatchSpanProcessor(
                        main_exporter,
                        max_queue_size=cfg['batch_processor']['max_queue_size'],
                        schedule_delay_millis=cfg['batch_processor']['schedule_delay_millis']
                    )
                    provider.add_span_processor(batch_processor)
                    
                    trace.set_tracer_provider(provider)
                    cls._instance = trace.get_tracer(cfg["service_name"])
        return cls._instance

    @staticmethod
    def _configure_exporter(exporter_type):
        """Configure the appropriate exporter based on type.
        
        Args:
            exporter_type: Type of exporter to configure
        Returns:
            Configured exporter instance
        """
        if exporter_type == "otlp":
            return OTLPSpanExporter()  # OTEL_EXPORTER_OTLP_... environment variables apply here
        elif exporter_type == "console":
            return ConsoleSpanExporter()  # OTEL_EXPORTER_CONSOLE_... environment variables apply here
        elif exporter_type == "file":
            # Load config to get file path
            config = TracerFactory._load_config()
            file_path = config.get('file_exporter', {}).get('path', "~/.JobChain/otel_trace.json")
            max_size_bytes = config.get('file_exporter', {}).get('max_size_bytes')
            rotation_time_days = config.get('file_exporter', {}).get('rotation_time_days')
            return AsyncFileExporter(file_path, max_size_bytes, rotation_time_days)
        else:
            raise ValueError("Unsupported exporter type")

    @classmethod
    def trace(cls, message: str, detailed_trace: bool = False, attributes: Optional[Dict[str, Any]] = None):
        """Trace a message with OpenTelemetry tracing.
        
        Args:
            message: The message to trace
            detailed_trace: Whether to include detailed tracing information (args, kwargs, object fields)
            attributes: Optional dictionary of additional attributes to add to the span
        """
        tracer = cls.get_tracer()
        
        # Get the calling frame
        frame = inspect.currentframe()
        if frame:
            caller_frame = frame.f_back
            if caller_frame:
                # Get function info
                func_name = caller_frame.f_code.co_name
                module_name = inspect.getmodule(caller_frame).__name__ if inspect.getmodule(caller_frame) else "__main__"
                
                # Get local variables including 'self' if it exists
                local_vars = caller_frame.f_locals
                args = []
                kwargs = {}
                
                # If this is a method call (has 'self')
                if 'self' in local_vars:
                    args.append(local_vars['self'])
                    # Add other arguments if they exist
                    if len(local_vars) > 1:
                        # Filter out 'self' and get remaining arguments
                        args.extend([v for k, v in local_vars.items() if k != 'self'])
                
                span_name = f"{module_name}.{func_name}"
                with tracer.start_as_current_span(span_name) as span:
                    span.set_attribute("trace.message", message)
                    if detailed_trace:
                        span.set_attribute("function.args", str(tuple(args)))
                        span.set_attribute("function.kwargs", str(kwargs))
                        if args and hasattr(args[0], "__dict__"):
                            span.set_attribute("object.fields", str(vars(args[0])))
                    if attributes:
                        for key, value in attributes.items():
                            span.set_attribute(key, str(value))
                    print(message)
                
                # Clean up
                del frame
                del caller_frame
                return
        
        # Fallback if not in a function context
        with tracer.start_as_current_span("trace_message") as span:
            span.set_attribute("trace.message", message)
            if attributes:
                for key, value in attributes.items():
                    span.set_attribute(key, str(value))
            print(message)

# Decorator for OpenTelemetry tracing
def trace_function(func=None, *, detailed_trace: bool = False, attributes: Optional[Dict[str, Any]] = None):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            tracer = TracerFactory.get_tracer()
            span_name = f"{func.__module__}.{func.__name__}"
            with tracer.start_as_current_span(span_name) as span:
                # Record function arguments only if detailed_trace is True
                if detailed_trace:
                    span.set_attribute("function.args", str(args))
                    span.set_attribute("function.kwargs", str(kwargs))
                    if args and hasattr(args[0], "__dict__"):
                        span.set_attribute("object.fields", str(vars(args[0])))
                if attributes:
                    for key, value in attributes.items():
                        span.set_attribute(key, str(value))
                try:
                    result = func(*args, **kwargs)
                    return result
                except Exception as e:
                    span.record_exception(e)
                    raise
        return wrapper
    
    if func is None:
        return decorator
    return decorator(func)


================================================
File: src/jobchain/utils/print_utils.py
================================================
from .. import jc_logging as logging


def printh(text):
    """
    Log the given text surrounded by asterisks.
    """
    logger = logging.getLogger('PrintUtils')
    logger.info("*** " + text + " ***")


================================================
File: src/jobchain/utils/timing.py
================================================
import time
from functools import wraps

def timing_decorator(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.perf_counter()
        result = func(*args, **kwargs)
        end_time = time.perf_counter()
        elapsed_time = end_time - start_time
        print(f"Elapsed time: {elapsed_time:.6f} seconds")
        return result
    return wrapper


================================================
File: tests/README.md
================================================
# Test Suite Documentation

## Running Tests

### Running All Tests
To run all tests:
```bash
python3 -m pytest
```
Note: By default, performance-intensive tests (like those in `test_parallel_load.py`) will be skipped to ensure quick test execution during normal development.

### Running All Tests Including Performance Tests
To run all tests including performance tests:
```bash
python3 -m pytest --full-suite
```
This will execute all tests, including the normally skipped performance and parallel load tests.

### Running Specific Tests
When running specific test files or individual tests that are normally skipped (like parallel load tests), you must include the `--full-suite` flag:

```bash
# Running a specific test file with skipped tests
python3 -m pytest tests/test_parallel_load.py -v --full-suite

# Running a specific test function
python3 -m pytest tests/test_parallel_load.py::test_maximum_parallel_file_trace -v --full-suite
```

The `--full-suite` flag is only required when running tests that are normally skipped. Regular tests can be run individually without this flag:

```bash
# Running a regular test file
python3 -m pytest tests/test_job_loader.py -v

# Running a specific regular test
python3 -m pytest tests/test_job_loader.py::test_load_job -v
```

## Test Categories

### Regular Tests
All the test modules run by default except test_parallel_load.py which takes a little more time than the other tests.

### Performance Tests (Requires --full-suite)
These tests are skipped by default as they are time and resource-intensive:
- test_parallel_load.py

These tests are valuable for verifying system performance and stability but are separated to maintain fast test execution during normal development.

To add more files to the skipped tests list, update the following section in `conftest.py`:
```python
def pytest_collection_modifyitems(config, items):
    if not config.getoption("--full-suite"):
        # Skip load tests by default
        for item in items:
            if "test_parallel_load.py" in str(item.fspath):
                item.add_marker(pytest.mark.skip(reason="Load test - use --full-suite to include"))
```
Additional test files can be added to the skip condition using the same pattern.


================================================
File: tests/conftest.py
================================================
import pytest

from jobchain.utils.otel_wrapper import TracerFactory

# set the TracerFactory up with the TestTracerProvider so the TracerProvider
#  can be overridden by code which normally isn't possible.
TracerFactory.set_test_mode(True)

def pytest_configure(config):
    config.addinivalue_line(
        "markers", "isolated: marks tests to run in isolation"
    )

def pytest_addoption(parser):
    parser.addoption(
        "--full-suite",
        action="store_true",
        default=False,
        help="run full test suite including load tests"
    )
    parser.addoption(
        "--isolated",
        action="store_true",
        default=False,
        help="run only tests marked as isolated"
    )

def pytest_collection_modifyitems(config, items):
    # Handle isolated tests
    # if config.getoption("--isolated"):
    #     selected = []
    #     deselected = []
    #     for item in items:
    #         if item.get_closest_marker("isolated"):
    #             selected.append(item)
    #         else:
    #             deselected.append(item)
    #     config.hook.pytest_deselected(items=deselected)
    #     items[:] = selected
    #     return
    # else:
    #     # Skip isolated tests when not running with --isolated
    #     for item in items:
    #         if item.get_closest_marker("isolated"):
    #             item.add_marker(pytest.mark.skip(reason="Isolated test - use --isolated to run"))

    # Handle full suite option
    if not config.getoption("--full-suite"):
        # Skip load tests by default
        for item in items:
            if "test_parallel_load.py" in str(item.fspath):
                item.add_marker(pytest.mark.skip(reason="Load test - use --full-suite to include"))


================================================
File: tests/test_aa.py
================================================
from jobchain.job import JobABC
from jobchain.job_chain import JobChain
from tests.test_utils.simple_job import SimpleJob


class AsyncTestJob(JobABC):
    """Job to confirm the basics of async functionality are working: """
    def __init__(self):
        super().__init__(name="AsyncTestJob")
    
    async def run(self, inputs):
        task = inputs[self.name]  # Get the task from inputs dict
        if isinstance(task, dict) and task.get('fail'):
            raise ValueError("Simulated task failure")
        if isinstance(task, dict) and task.get('delay'):
            await asyncio.sleep(task['delay'])
        return {'task': task, 'completed': True}

def test_imports_are_working():
  job = SimpleJob("Test Job")
  job_chain = JobChain(job)
  testvar="test"
  assert testvar == "test"


================================================
File: tests/test_async_functionality.py
================================================
"""
    Simple Tests to confirm the basics of async functionality are working:
    
            - Tests concurrent task execution and verification
            - Tests async task cancellation and cleanup
            - Tests event loop handling and lifecycle
            - Tests async exception propagation
            - Tests parallel task limits and scaling  
"""

import asyncio

import pytest

from jobchain.job import JobABC
from jobchain.job_chain import JobChain


class AsyncTestJob(JobABC):
    """Job to confirm the basics of async functionality are working: """
    def __init__(self):
        super().__init__(name="AsyncTestJob")
    
    async def run(self, task):
        task = task[self.name]  # Get the task from inputs dict
        if isinstance(task, dict) and task.get('fail'):
            raise ValueError("Simulated task failure")
        if isinstance(task, dict) and task.get('delay'):
            await asyncio.sleep(task['delay'])
        return {'task': task, 'completed': True}


@pytest.mark.asyncio
async def test_concurrent_task_execution():
    """Test that tasks are truly executed concurrently"""
    results = []
    
    def collect_result(result):
        results.append(result)
    
    job_chain = JobChain(AsyncTestJob(), collect_result, serial_processing=True)
    
    # Submit tasks with different delays
    tasks = [
        {'AsyncTestJob': {'task_id': 1, 'delay': 0.2}},
        {'AsyncTestJob': {'task_id': 2, 'delay': 0.1}},
        {'AsyncTestJob': {'task_id': 3, 'delay': 0.3}}
    ]
    
    for task in tasks:
        job_chain.submit_task(task)
    
    job_chain.mark_input_completed()
    
    # Verify all tasks completed
    assert len(results) == 3
    # Verify task completion order (should still be in delay order due to async execution)
    task_ids = [r['task']['task_id'] for r in results]
    assert 2 in task_ids  # Task 2 should be completed (not necessarily first in serial mode)
    assert 1 in task_ids
    assert 3 in task_ids

@pytest.mark.asyncio
async def test_async_task_cancellation():
    """Test proper cleanup of cancelled async tasks"""
    job_chain = JobChain(AsyncTestJob())
    
    # Submit a long-running task
    job_chain.submit_task({'AsyncTestJob': {'delay': 1.0}})
    
    # Force cleanup before completion
    job_chain._cleanup()
    
    # Verify process termination
    assert not job_chain.job_executor_process.is_alive()
    assert job_chain._task_queue._closed

@pytest.mark.asyncio
async def test_event_loop_handling():
    """Test proper event loop creation and cleanup"""
    job_chain = JobChain(AsyncTestJob())
    
    # Submit a simple task
    job_chain.submit_task({'AsyncTestJob': {'task_id': 1}})
    job_chain.mark_input_completed()
    
    # Verify process cleanup
    assert not job_chain.job_executor_process.is_alive()

@pytest.mark.asyncio
async def test_async_exception_handling():
    """Test that async exceptions are properly caught and handled"""
    results = []
    
    def collect_result(result):
        results.append(result)
    
    job_chain = JobChain(AsyncTestJob(), collect_result, serial_processing=True)
    
    # Submit mix of successful and failing tasks
    tasks = [
        {'AsyncTestJob': {'task_id': 1}},
        {'AsyncTestJob': {'task_id': 2, 'fail': True}},
        {'AsyncTestJob': {'task_id': 3}}
    ]
    
    for task in tasks:
        job_chain.submit_task(task)
    
    job_chain.mark_input_completed()
    
    # Only successful tasks should have results
    assert len(results) == 2
    assert all(r['completed'] for r in results)

@pytest.mark.asyncio
async def test_parallel_task_limit():
    """Test handling of many concurrent tasks"""
    results = []
    
    def collect_result(result):
        results.append(result)
    
    job_chain = JobChain(AsyncTestJob(), collect_result, serial_processing=True)
    
    # Submit many quick tasks
    num_tasks = 100
    for i in range(num_tasks):
        job_chain.submit_task({'AsyncTestJob': {'task_id': i, 'delay': 0.01}})
    
    job_chain.mark_input_completed()
    
    # All tasks should complete
    assert len(results) == num_tasks
    # Results should maintain task integrity
    task_ids = {r['task']['task_id'] for r in results}
    assert len(task_ids) == num_tasks

if __name__ == '__main__':
    pytest.main(['-v', 'test_async_functionality.py'])


================================================
File: tests/test_concurrency.py
================================================
import asyncio
import contextvars
import multiprocessing as mp
import os
from functools import partial
from typing import Any, Dict

import pytest

import jobchain.jc_logging as logging
from jobchain.job import JobABC, Task, job_graph_context_manager
from jobchain.job_chain import JobChain
from jobchain.job_loader import ConfigLoader, JobFactory


def returns_collector(shared_results, result):
    shared_results.append(result)

#@pytest.mark.skip("Skipping test due to working yet")
@pytest.mark.asyncio
async def test_concurrency_by_expected_returns():
    # Create a manager for sharing the results list between processes
    manager = mp.Manager()
    shared_results = manager.list()
    
    # Create a partial function with our shared results list
    collector = partial(returns_collector, shared_results)
    
    # Set config directory for test
    config_dir = os.path.join(os.path.dirname(__file__), "test_configs/test_concurrency_by_returns")
    ConfigLoader._set_directories([config_dir])
    
    # Create JobChain with parallel processing
    job_chain = JobChain(result_processing_function=collector)
    logging.info(f"Names of jobs in head job: {job_chain.get_job_graph_mapping()}")

    def submit_task(range_val:int):
        for i in range(range_val):
            job_chain.submit_task({'task': f'{i}'})

    def check_results():
        for result in shared_results:
            #logging.info(f"Result: {result}")
            assert result['result'] == 'A.A.B.C.E.A.D.F.G'

        shared_results[:] = []  # Clear the shared_results using slice assignment
    

    submit_task(300)

    job_chain.mark_input_completed() # this waits for all results to be returned

    check_results()



class A(JobABC):
  async def run(self, inputs: Dict[str, Any]) -> Any:
    print(f"\nA expected inputs: {self.expected_inputs}")
    print(f"A data inputs: {inputs}")
    dataA:dict = {
        'dataA1': {},
        'dataA2': {}
    }
    print(f"A returned: {dataA}")
    return dataA

class B(JobABC):
  async def run(self, inputs: Dict[str, Any]) -> Any:
    print(f"\nB expected inputs: {self.expected_inputs}")
    print(f"B data inputs: {inputs}")
    dataB:dict = {
        'dataB1': {},
        'dataB2': {}
    }
    print(f"B returned: {dataB}")
    return dataB

class C(JobABC):
  async def run(self, inputs: Dict[str, Any]) -> Any:
    print(f"\nC expected inputs: {self.expected_inputs}")
    print(f"C data inputs: {inputs}")
    dataC:dict = {
        'dataC1': {},
        'dataC2': {}
    } 
    print(f"C returned: {dataC}")
    return dataC

class D(JobABC):
  async def run(self, inputs: Dict[str, Any]) -> Any:
    print(f"\nD expected inputs: {self.expected_inputs}")
    print(f"D data inputs: {inputs}")
    dataD:dict = {
        'dataD1': {},
        'dataD2': {}
    } 
    print(f"D returned: {dataD}")
    return dataD

jobs = {
    'A': A('A'),
    'B': B('B'),
    'C': C('C'),
    'D': D('D')
}

graph_definition1 = {
    'A': {'next': ['B', 'C']},
    'B': {'next': ['C', 'D']},
    'C': {'next': ['D']},
    'D': {'next': []}
} 



@pytest.mark.asyncio
async def test_simple_graph():
    head_job:JobABC = JobFactory.create_job_graph(graph_definition1, jobs)
    job_set = JobABC.job_set(head_job)
    # Create 50 tasks to run concurrently
    tasks = []
    for _ in range(50):
      async with job_graph_context_manager(job_set):
        task = asyncio.create_task(head_job._execute(Task({'1': {},'2': {}})))
        tasks.append(task)
    
    # Run all tasks concurrently and gather results
    results = await asyncio.gather(*tasks)
    
    # Verify each result matches the expected final output from job D
    for final_result in results:
        # Extract just the job result data, ignoring task_pass_through
        result_data = {k: v for k, v in final_result.items() if k not in ['task_pass_through', 'RETURN_JOB']}
        assert result_data == {
                'dataD1': {},
                'dataD2': {}
            }


================================================
File: tests/test_error_conditions.py
================================================
"""
    Test resilience under error conditions:

        - Tests basic error handling and propagation
        - Tests timeout scenarios
        - Tests process termination handling
        - Tests invalid input handling
        - Tests resource cleanup
        - Tests result processing errors
        - Tests memory error handling
        - Tests unpicklable result scenarios
"""

import asyncio

import pytest

from jobchain.job import JobABC
from jobchain.job_chain import JobChain


class ErrorTestJob(JobABC):
    """Job implementation for testing error conditions"""
    def __init__(self):
        super().__init__(name="ErrorTestJob")
    
    async def run(self, inputs):
        inputs = inputs[self.name]  # Get the task from inputs dict
        if inputs.get('raise_error'):
            raise Exception(inputs.get('error_message', 'Simulated error'))
        if inputs.get('timeout'):
            await asyncio.sleep(float(inputs['timeout']))
            return {'task': inputs, 'status': 'timeout_completed'}
        if inputs.get('memory_error'):
            # Explicitly raise MemoryError instead of trying to create a large list
            raise MemoryError("Simulated memory error")
        if inputs.get('invalid_result'):
            # Return an unpicklable object
            return lambda x: x  # Functions can't be pickled
        return {'task': inputs, 'status': 'completed'}


def test_basic_error_handling():
    """Test handling of basic exceptions during task execution"""
    results = []
    errors = []
    
    def collect_result(result):
        if isinstance(result, Exception):
            errors.append(result)
        else:
            results.append(result)
    
    job_chain = JobChain(ErrorTestJob(), collect_result, serial_processing=True)
    
    # Submit mix of successful and failing tasks
    tasks = [
        {'ErrorTestJob': {'task_id': 1}},
        {'ErrorTestJob': {'task_id': 2, 'raise_error': True, 'error_message': 'Task 2 error'}},
        {'ErrorTestJob': {'task_id': 3}},
        {'ErrorTestJob': {'task_id': 4, 'raise_error': True, 'error_message': 'Task 4 error'}}
    ]
    
    for task in tasks:
        job_chain.submit_task(task)
    
    job_chain.mark_input_completed()
    
    # Verify successful tasks completed
    assert len(results) == 2
    assert all(r['status'] == 'completed' for r in results)
    
    # Verify errors were captured
    assert len(errors) == 0  # Errors should be logged, not passed to result processor

def test_timeout_handling():
    """Test handling of task timeouts"""
    results = []
    def collect_result(result):
        results.append(result)
    
    job_chain = JobChain(ErrorTestJob(), collect_result, serial_processing=True)
    
    # Submit tasks with varying timeouts
    tasks = [
        {'ErrorTestJob': {'task_id': 1, 'timeout': 0.3}},
        {'ErrorTestJob': {'task_id': 2, 'timeout': 0.2}},
        {'ErrorTestJob': {'task_id': 3, 'timeout': 0.1}}
    ]
    
    for task in tasks:
        job_chain.submit_task(task)
    
    job_chain.mark_input_completed()
    
    # Verify all tasks eventually completed
    assert len(results) == 3
    assert all(r['status'] == 'timeout_completed' for r in results)
    
    # Verify tasks completed in order of timeout
    task_ids = [r['task']['task_id'] for r in results]
    assert task_ids == [3, 2, 1]

def test_process_termination():
    """Test handling of process termination"""
    job_chain = JobChain(ErrorTestJob())
    
    # Submit a long-running task
    job_chain.submit_task({'ErrorTestJob': {'task_id': 1, 'timeout': 1.0}})
    
    # Force terminate the process
    job_chain.job_executor_process.terminate()
    
    # Verify cleanup handles terminated process
    job_chain._cleanup()
    assert not job_chain.job_executor_process.is_alive()

def test_invalid_input():
    """Test handling of invalid input data"""
    job_chain = JobChain(ErrorTestJob())
    
    # Test various invalid inputs
    invalid_inputs = [
        None,
        "",
        {},
        {'task_id': None},
        {'task_id': object()},  # Unpicklable object
        []
    ]
    
    for invalid_input in invalid_inputs:
        job_chain.submit_task(invalid_input)
    
    job_chain.mark_input_completed()
    # Should complete without raising exceptions

def test_resource_cleanup():
    """Test proper cleanup of resources"""
    job_chain = JobChain(ErrorTestJob())
    
    # Submit some tasks
    for i in range(5):
        job_chain.submit_task({'ErrorTestJob': {'task_id': i}})
    
    # Get queue references
    task_queue = job_chain._task_queue
    result_queue = job_chain._result_queue
    
    # Cleanup
    job_chain._cleanup()
    
    # Verify queues are closed
    assert task_queue._closed
    assert result_queue._closed
    
    # Verify processes are terminated
    assert not job_chain.job_executor_process.is_alive()
    if job_chain.result_processor_process:
        assert not job_chain.result_processor_process.is_alive()

def test_error_in_result_processing():
    """Test handling of errors in result processing function"""
    def failing_processor(result):
        raise Exception("Result processing error")
    
    job_chain = JobChain(ErrorTestJob(), failing_processor, serial_processing=True)
    
    # Submit tasks
    for i in range(3):
        job_chain.submit_task({'ErrorTestJob': {'task_id': i}})
    
    job_chain.mark_input_completed()
    # Should complete without hanging or crashing

def test_memory_error_handling():
    """Test handling of memory errors"""
    results = []
    def collect_result(result):
        results.append(result)
    
    job_chain = JobChain(ErrorTestJob(), collect_result, serial_processing=True)
    
    # Submit task that will cause memory error
    job_chain.submit_task({'ErrorTestJob': {'task_id': 1, 'memory_error': True}})
    
    job_chain.mark_input_completed()
    
    # Process should handle the memory error gracefully
    assert len(results) == 0  # No results should be processed

def test_unpicklable_result():
    """Test handling of unpicklable results"""
    results = []
    def collect_result(result):
        results.append(result)
    
    job_chain = JobChain(ErrorTestJob(), collect_result, serial_processing=True)
    
    # Submit task that returns unpicklable result
    job_chain.submit_task({'ErrorTestJob': {'task_id': 1, 'invalid_result': True}})
    
    job_chain.mark_input_completed()
    
    # Process should handle the pickling error gracefully
    assert len(results) == 0  # No results should be processed

if __name__ == '__main__':
    pytest.main(['-v', 'test_error_conditions.py'])


================================================
File: tests/test_graph_config_parsing.py
================================================
from jobchain.jc_graph import (add_edge_anywhere, check_graph_for_cycles, print_graph,
                      validate_graph_references, find_head_nodes, find_tail_nodes,
                      validate_graph)
import pytest

graph1: dict = {
    'A': {'next': ['B', 'C']},
    'B': {'next': ['C', 'D']},
    'C': {'next': ['D']},
    'D': {'next': []}
}

graph2: dict = {
    'A': {'next': ['B', 'C']},
    'B': {'next': ['C', 'D']},
    'C': {
        'next': ['D'],
        'subgraph': {  # Attributed subgraph
            'V': {'next': ['W']},
            'W': {'next': ['X', 'Y']},
            'X': {'next': ['Z']},
            'Y': {'next': ['Z']},
            'Z': {'next': []}
        }
    },
    'D': {'next': []}
}

graph3: dict = {
    'A': {'next': ['B', 'C']},
    'B': {'next': ['C', 'D']},
    'C': {
        'next': ['D'],
        'subgraph': {  # Attributed subgraph
            'V': {'next': ['W']},
            'W': {
                'next': ['X', 'Y'],
                'subgraph': {  # Attributed subgraph
                    'alpha': {'next': ['beta']},
                    'beta': {'next': ['sigma', 'pi']},
                    'sigma': {'next': ['zeta']},
                    'pi': {'next': ['zeta']},
                    'zeta': {'next': []}
                }
            },
            'X': {'next': ['Z']},
            'Y': {'next': ['Z']},
            'Z': {'next': []}
        }
    },
    'D': {'next': []}
}

def test_head_graph():
    """
    Test that graphs have exactly one head node (node with no incoming edges).
    Tests include:
    1. Valid graphs with single head node (graph1, graph2, graph3)
    2. Invalid graph with no head nodes (cycle)
    3. Invalid graph with multiple head nodes
    4. Invalid graph with no nodes
    """
    # Test valid graphs (should have exactly one head node)
    assert find_head_nodes(graph1) == {'A'}, "graph1 should have exactly one head node 'A'"
    assert find_head_nodes(graph2) == {'A'}, "graph2 should have exactly one head node 'A'"
    assert find_head_nodes(graph3) == {'A'}, "graph3 should have exactly one head node 'A'"
    
    # Test graph with no head nodes (cycle)
    cycle_graph = {
        'X': {'next': ['Y']},
        'Y': {'next': ['Z']},
        'Z': {'next': ['X']}
    }
    assert len(find_head_nodes(cycle_graph)) == 0, "Cycle graph should have no head nodes"
    
    # Test graph with multiple head nodes
    multi_head_graph = {
        'A': {'next': ['C']},
        'B': {'next': ['C']},
        'C': {'next': []}
    }
    heads = find_head_nodes(multi_head_graph)
    assert len(heads) > 1, f"Multi-head graph should have multiple head nodes, found: {heads}"
    
    # Test empty graph
    empty_graph = {}
    assert len(find_head_nodes(empty_graph)) == 0, "Empty graph should have no head nodes"

def test_tail_graph():
    """
    Test that graphs have exactly one tail node (node with no outgoing edges).
    Tests include:
    1. Valid graphs with single tail node (graph1, graph2, graph3)
    2. Invalid graph with no tail nodes (cycle)
    3. Invalid graph with multiple tail nodes
    4. Invalid graph with no nodes
    """
    # Test valid graphs (should have exactly one tail node)
    assert find_tail_nodes(graph1) == {'D'}, "graph1 should have exactly one tail node 'D'"
    assert find_tail_nodes(graph2) == {'D'}, "graph2 should have exactly one tail node 'D'"
    assert find_tail_nodes(graph3) == {'D'}, "graph3 should have exactly one tail node 'D'"
    
    # Test graph with no tail nodes (cycle)
    cycle_graph = {
        'X': {'next': ['Y']},
        'Y': {'next': ['Z']},
        'Z': {'next': ['X']}
    }
    assert len(find_tail_nodes(cycle_graph)) == 0, "Cycle graph should have no tail nodes"
    
    # Test graph with multiple tail nodes
    multi_tail_graph = {
        'A': {'next': []},
        'B': {'next': []},
        'C': {'next': ['A', 'B']}
    }
    tails = find_tail_nodes(multi_tail_graph)
    assert len(tails) > 1, f"Multi-tail graph should have multiple tail nodes, found: {tails}"
    
    # Test empty graph
    empty_graph = {}
    assert len(find_tail_nodes(empty_graph)) == 0, "Empty graph should have no tail nodes"
    
    # Test graph with subgraph tail nodes
    subgraph_tail_graph = {
        'A': {'next': ['B']},
        'B': {
            'next': [],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {'next': []},
            }
        }
    }
    assert find_tail_nodes(subgraph_tail_graph) == {'Y'}, "Should find tail node in subgraph"

def test_print_format():
    """
    Test the print formatting of different graph structures.
    Tests include:
    1. Simple graph with multiple paths
    2. Graph with single subgraph
    3. Graph with nested subgraphs
    """
    print("--------")       
    print("Graph 1:")
    print("--------")
    print_graph(graph1)

    print("--------")
    print("\nGraph 2:")
    print("--------")
    print_graph(graph2)

    print("--------")
    print("\nGraph 3:")
    print("--------")
    print_graph(graph3)

def test_simple_cross_graph_cycle_detection():
    """
    Test cycle detection and edge addition rules for simple graphs and cross-graph scenarios.
    Tests include:
    1. Basic cycle detection on predefined graphs
    2. Edge addition with cycle prevention
    3. Cross-graph edge addition rules with subgraphs
    """
    # Check each graph for cycles
    assert check_graph_for_cycles(graph1, "Graph 1") == False, "Graph 1 should not have cycles"
    assert check_graph_for_cycles(graph2, "Graph 2") == False, "Graph 2 should not have cycles"
    assert check_graph_for_cycles(graph3, "Graph 3") == False, "Graph 3 should not have cycles"

    # Create a test graph
    test_graph = {
        'A': {'next': ['B']},
        'B': {'next': ['C']},
        'C': {'next': ['D']},
        'D': {'next': []}
    }
    original_graph = test_graph.copy()

    # Try to add edges that would create cycles - should be prevented
    result = add_edge_anywhere(test_graph, 'D', 'A')
    assert result == False, "Adding edge D->A should fail as it creates a cycle"
    assert test_graph == original_graph, "Graph should remain unchanged after failed edge addition"

    # Add valid shortcut paths
    result = add_edge_anywhere(test_graph, 'A', 'C')
    assert result == True, "Adding edge A->C should succeed as it's a valid shortcut"
    assert 'C' in test_graph['A']['next'], "Edge A->C should be added to graph"

    result = add_edge_anywhere(test_graph, 'B', 'D')
    assert result == True, "Adding edge B->D should succeed as it's a valid shortcut"
    assert 'D' in test_graph['B']['next'], "Edge B->D should be added to graph"

    # Test subgraph edge addition
    test_graph_with_sub = {
        'A': {'next': ['B']},
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {'next': ['Z']},
                'Z': {'next': []}
            }
        },
        'C': {'next': []}
    }
    original_subgraph = test_graph_with_sub.copy()

    # Test cases for edge addition rules
    # 1. Adding edge within same subgraph
    result = add_edge_anywhere(test_graph_with_sub, 'X', 'Z')
    assert result == True, "Adding edge X->Z within subgraph should succeed"
    assert 'Z' in test_graph_with_sub['B']['subgraph']['X']['next'], "Edge X->Z should be added to subgraph"

    # 2. Adding edge that would create cycle in subgraph
    result = add_edge_anywhere(test_graph_with_sub, 'Z', 'X')
    assert result == False, "Adding edge Z->X should fail as it creates a cycle"

    # 3. Adding cross-graph edge from main to subgraph
    result = add_edge_anywhere(test_graph_with_sub, 'A', 'X')
    assert result == False, "Adding edge A->X should fail as cross-graph references are not allowed"

    # 4. Adding cross-graph edge from subgraph to main
    result = add_edge_anywhere(test_graph_with_sub, 'Z', 'C')
    assert result == False, "Adding edge Z->C should fail as cross-graph references are not allowed"

    # 5. Adding edge in main graph
    result = add_edge_anywhere(test_graph_with_sub, 'A', 'C')
    assert result == True, "Adding edge A->C in main graph should succeed"
    assert 'C' in test_graph_with_sub['A']['next'], "Edge A->C should be added to main graph"

def test_comprehensive_cycle_detection():
    """
    Comprehensive tests for cycle detection in graphs.
    Tests include:
    1. Simple cycles in main graph
    2. Complex cycles in main graph
    3. Cycles in subgraphs
    4. Cycles across nested subgraphs
    5. Valid acyclic graphs with complex paths
    """
    # Test Case 1: Simple cycle in main graph
    simple_cycle = {
        'A': {'next': ['B']},
        'B': {'next': ['C']},
        'C': {'next': ['A']}  # Creates cycle A -> B -> C -> A
    }
    assert check_graph_for_cycles(simple_cycle, "Simple cycle") == True, "Should detect cycle in simple graph A->B->C->A"

    # Test Case 2: Complex cycle in main graph
    complex_cycle = {
        'A': {'next': ['B', 'C']},
        'B': {'next': ['D']},
        'C': {'next': ['E']},
        'D': {'next': ['F']},
        'E': {'next': ['F']},
        'F': {'next': ['B']}  # Creates cycle B -> D -> F -> B
    }
    assert check_graph_for_cycles(complex_cycle, "Complex cycle") == True, "Should detect cycle in complex graph B->D->F->B"

    # Test Case 3: Cycle in subgraph
    subgraph_cycle = {
        'A': {'next': ['B']},
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {'next': ['Z']},
                'Z': {'next': ['X']}  # Creates cycle X -> Y -> Z -> X
            }
        },
        'C': {'next': []}
    }
    assert check_graph_for_cycles(subgraph_cycle, "Subgraph cycle") == True, "Should detect cycle in subgraph X->Y->Z->X"

    # Test Case 4: Valid acyclic graph with multiple paths
    valid_complex = {
        'A': {'next': ['B', 'C', 'D']},
        'B': {'next': ['E']},
        'C': {'next': ['E']},
        'D': {'next': ['E']},
        'E': {'next': []}
    }
    assert check_graph_for_cycles(valid_complex, "Valid complex") == False, "Should not detect cycles in valid complex graph"

    # Test Case 5: Valid acyclic graph with nested subgraphs
    valid_nested = {
        'A': {'next': ['B']},
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {
                    'next': ['Z'],
                    'subgraph': {
                        'alpha': {'next': ['beta']},
                        'beta': {'next': ['gamma']},
                        'gamma': {'next': []}
                    }
                },
                'Z': {'next': []}
            }
        },
        'C': {'next': []}
    }
    assert check_graph_for_cycles(valid_nested, "Valid nested") == False, "Should not detect cycles in valid nested graph"

    # Test Case 6: Multiple paths to same node (diamond pattern)
    diamond_pattern = {
        'A': {'next': ['B', 'C']},
        'B': {'next': ['D']},
        'C': {'next': ['D']},
        'D': {'next': []}
    }
    assert check_graph_for_cycles(diamond_pattern, "Diamond pattern") == False, "Should not detect cycles in diamond pattern"

    # Test Case 7: Complex nested subgraph with cycle
    nested_cycle = {
        'A': {'next': ['B']},
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {
                    'next': ['Z'],
                    'subgraph': {
                        'alpha': {'next': ['beta']},
                        'beta': {'next': ['gamma']},
                        'gamma': {'next': ['alpha']}  # Creates cycle in nested subgraph
                    }
                },
                'Z': {'next': []}
            }
        },
        'C': {'next': []}
    }
    assert check_graph_for_cycles(nested_cycle, "Nested cycle") == True, "Should detect cycle in deeply nested subgraph alpha->beta->gamma->alpha"

def test_cross_graph_references():
    """
    Test validation of graph references to ensure nodes only reference other nodes
    within their own graph level (main graph or subgraph).
    
    Tests include:
    1. Valid graph with proper references
    2. Invalid graph with main graph referencing subgraph node
    3. Invalid graph with subgraph referencing main graph node
    """
    # Test Case 1: Valid graph with proper references
    valid_graph = {
        'A': {'next': ['B']},
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {'next': ['Z']},
                'Z': {'next': []}
            }
        },
        'C': {'next': []}
    }
    is_valid, violations = validate_graph_references(valid_graph)
    assert is_valid, "Valid graph should be valid"
    assert len(violations) == 0, "Valid graph should have no reference violations"

    # Test Case 2: Invalid - main graph referencing subgraph node
    invalid_main_ref = {
        'A': {'next': ['B', 'X']},  # 'X' is in subgraph, can't be referenced from main
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {'next': ['Z']},
                'Z': {'next': []}
            }
        },
        'C': {'next': []}
    }
    is_valid, violations = validate_graph_references(invalid_main_ref)
    assert not is_valid, "Graph with main->subgraph reference should be invalid"
    assert len(violations) == 1, "Should detect one violation for main graph referencing subgraph node"
    assert "Node 'A' references 'X' which is not in the same graph level" in violations[0], \
        "Should detect invalid reference from main graph to subgraph node"

    # Test Case 3: Invalid - subgraph referencing main graph node
    invalid_sub_ref = {
        'A': {'next': ['B']},
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {'next': ['C']},  # 'C' is in main graph, can't be referenced from subgraph
                'Z': {'next': []}
            }
        },
        'C': {'next': []}
    }
    is_valid, violations = validate_graph_references(invalid_sub_ref)
    assert not is_valid, "Graph with subgraph->main reference should be invalid"
    assert len(violations) == 1, "Should detect one violation for subgraph referencing main graph node"
    assert "Node 'B -> Y' references 'C' which is not in the same graph level" in violations[0], \
        "Should detect invalid reference from subgraph to main graph node"

    # Test Case 4: Invalid - multiple cross-graph references
    invalid_multiple_refs = {
        'A': {'next': ['B', 'X', 'Y']},  # Multiple invalid references
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y', 'C']},  # Invalid reference to main graph
                'Y': {'next': ['Z', 'A']},  # Invalid reference to main graph
                'Z': {'next': []}
            }
        },
        'C': {'next': []}
    }
    is_valid, violations = validate_graph_references(invalid_multiple_refs)
    assert not is_valid, "Graph with multiple cross-graph references should be invalid"
    assert len(violations) == 4, "Should detect all cross-graph reference violations"
    violation_texts = ' '.join(violations)
    assert "Node 'A' references 'X' which is not in the same graph level" in violation_texts, "Should detect A->X violation"
    assert "Node 'A' references 'Y' which is not in the same graph level" in violation_texts, "Should detect A->Y violation"
    assert "Node 'B -> X' references 'C' which is not in the same graph level" in violation_texts, "Should detect X->C violation"
    assert "Node 'B -> Y' references 'A' which is not in the same graph level" in violation_texts, "Should detect Y->A violation"

    # Test Case 5: Valid - nested subgraphs with proper references
    valid_nested = {
        'A': {'next': ['B']},
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['Y']},
                'Y': {
                    'next': ['Z'],
                    'subgraph': {
                        'alpha': {'next': ['beta']},
                        'beta': {'next': ['gamma']},
                        'gamma': {'next': []}
                    }
                },
                'Z': {'next': []}
            }
        },
        'C': {'next': []}
    }
    is_valid, violations = validate_graph_references(valid_nested)
    assert is_valid, "Valid nested subgraphs should be valid"
    assert len(violations) == 0, "Valid nested subgraphs should have no reference violations"

def test_validate_graph(caplog):
    """
    Test comprehensive graph validation.
    Tests include:
    1. Valid graphs (graph1, graph2, graph3)
    2. Graph with cycles
    3. Graph with cross-graph reference violations
    4. Graph with no head node
    5. Graph with multiple head nodes
    6. Graph with no tail node
    7. Graph with multiple tail nodes
    """
    # Test valid graphs
    validate_graph(graph1, "graph1")  # Should pass
    validate_graph(graph2, "graph2")  # Should pass
    validate_graph(graph3, "graph3")  # Should pass
    
    # Test graph with cycles
    cycle_graph = {
        'X': {'next': ['Y']},
        'Y': {'next': ['Z']},
        'Z': {'next': ['X']}
    }
    with pytest.raises(ValueError, match="contains cycles"):
        validate_graph(cycle_graph, "cycle_graph")
    
    # Test graph with cross-graph reference violations
    invalid_ref_graph = {
        'A': {'next': ['B']},
        'B': {
            'next': ['C'],
            'subgraph': {
                'X': {'next': ['C']}  # Invalid: subgraph node references main graph node
            }
        },
        'C': {'next': []}
    }
    with pytest.raises(ValueError, match="invalid cross-graph references"):
        validate_graph(invalid_ref_graph, "invalid_ref_graph")
    
    # Test graph with no head node (all nodes have incoming edges)
    no_head_graph = {
        'A': {'next': ['B']},
        'B': {'next': ['A']}  # Cycle creates no head nodes
    }
    with pytest.raises(ValueError, match="no head nodes"):
        validate_graph(no_head_graph, "no_head_graph")
    
    # Test graph with multiple head nodes
    multi_head_graph = {
        'A': {'next': ['C']},
        'B': {'next': ['C']},
        'C': {'next': []}
    }
    validate_graph(multi_head_graph, "multi_head_graph")
    assert "multiple head nodes" in caplog.text
    assert "Exactly one head node is required" in caplog.text
    
    # Test graph with no tail node (all nodes have outgoing edges)
    no_tail_graph = {
        'A': {'next': ['B']},
        'B': {'next': ['A']}  # Cycle creates no tail nodes
    }
    with pytest.raises(ValueError, match="no tail nodes"):
        validate_graph(no_tail_graph, "no_tail_graph")
    
    # Test graph with multiple tail nodes
    multi_tail_graph = {
        'A': {'next': []},
        'B': {'next': []},
        'C': {'next': ['A', 'B']}
    }
    validate_graph(multi_tail_graph, "multi_tail_graph")
    assert "multiple tail nodes" in caplog.text
    assert "Exactly one tail node is required" in caplog.text


================================================
File: tests/test_job_graph.py
================================================
import asyncio
import time
from typing import Any, Dict

from jobchain.job import JobABC, Task, job_graph_context_manager
from jobchain.job_chain import JobChain
from jobchain.job_loader import JobFactory
from jobchain.jobs.default_jobs import DefaultHeadJob

class MockJob(JobABC):
    def run(self):
        pass

class MockJobSubclass(MockJob):
    pass

class DelayedMockJob(MockJob):
    """A mock job that introduces a configurable delay in processing."""
    def __init__(self, name: str, delay: float):
        super().__init__(name=name)
        self.delay = delay

    async def run(self, inputs):
        inputs = inputs[self.name]  # Get the task from inputs dict
        await asyncio.sleep(self.delay)
        return {'input': inputs, 'output': f'processed by {self.name}'}

def test_job_name_always_present():
    # Test with explicit name
    job1 = MockJob(name="explicit_name")
    assert job1.name == "explicit_name"
    
    # Test with auto-generated name
    job2 = MockJob()
    assert job2.name is not None
    assert isinstance(job2.name, str)
    assert len(job2.name) > 0

    # Test subclass with explicit name
    job3 = MockJobSubclass(name="subclass_name")
    assert job3.name == "subclass_name"
    
    # Test subclass with auto-generated name
    job4 = MockJobSubclass()
    assert job4.name is not None
    assert isinstance(job4.name, str)
    assert len(job4.name) > 0

def test_auto_generated_names_are_unique():
    # Create multiple jobs without explicit names
    num_jobs = 100  # Test with a significant number of jobs
    jobs = [MockJob() for _ in range(num_jobs)]
    
    # Collect all names in a set
    names = {job.name for job in jobs}
    
    # If all names are unique, the set should have the same length as the list
    assert len(names) == num_jobs
    
    # Test uniqueness across different subclass instances
    mixed_jobs = [
        MockJob() if i % 2 == 0 else MockJobSubclass()
        for i in range(num_jobs)
    ]
    mixed_names = {job.name for job in mixed_jobs}
    assert len(mixed_names) == num_jobs

def test_parallel_execution_multiple_jobs():
    """Test parallel execution with multiple jobs in a job graph."""
    # Test with both 1s and 2s delays
    for delay in [1.0, 2.0]:
        # Create 5 jobs with the same delay
        jobs = [DelayedMockJob(f'job_{i}', delay) for i in range(5)]
        
        # Create tasks for each job
        tasks = []
        for i in range(5):
            for j in range(4):  # 4 tasks per job = 20 total tasks
                tasks.append({f'job_{i}': {'task': f'task_{i}_{j}', 'job_name': f'job_{i}'}})

        # Time the execution
        start_time = time.time()
        
        # Run the job chain with all jobs and collect results
        results = []
        def result_collector(result):
            results.append(result)
        job_chain = JobChain(jobs, result_collector, serial_processing=True)
        for task in tasks:
            job_name = next(iter(task.keys()))  # Get the job name from the task dict
            job_chain.submit_task(task, job_name=job_name)
        job_chain.mark_input_completed()
        
        end_time = time.time()
        execution_time = end_time - start_time

        # Verify results
        assert len(results) == 20  # Should have 20 results
        
        # Check that each task was processed by the correct job
        for result in results:
            input_task = result['input']
            job_name = input_task['job_name']
            assert result['output'] == f'processed by {job_name}'
        
        # Verify parallel execution - should take ~4 * delay seconds (4 batches of tasks)
        # Add some buffer time for overhead
        assert execution_time < (4 * delay + 1), f"Execution took {execution_time} seconds, expected less than {4 * delay + 1} seconds"

def collect_result(results):
    """Create a picklable result collector function that appends to the given list."""
    def collector(result):
        results.append(result)
    return collector


class A(JobABC):
  async def run(self, inputs: Dict[str, Any]) -> Any:
    print(f"\nA expected inputs: {self.expected_inputs}")
    print(f"A data inputs: {inputs}")
    dataA:dict = {
        'dataA1': {},
        'dataA2': {}
    }
    print(f"A returned: {dataA}")
    return dataA

class B(JobABC):
  async def run(self, inputs: Dict[str, Any]) -> Any:
    print(f"\nB expected inputs: {self.expected_inputs}")
    print(f"B data inputs: {inputs}")
    dataB:dict = {
        'dataB1': {},
        'dataB2': {}
    }
    print(f"B returned: {dataB}")
    return dataB

class C(JobABC):
  async def run(self, inputs: Dict[str, Any]) -> Any:
    print(f"\nC expected inputs: {self.expected_inputs}")
    print(f"C data inputs: {inputs}")
    dataC:dict = {
        'dataC1': {},
        'dataC2': {}
    } 
    print(f"C returned: {dataC}")
    return dataC

class D(JobABC):
  async def run(self, inputs: Dict[str, Any]) -> Any:
    print(f"\nD expected inputs: {self.expected_inputs}")
    print(f"D data inputs: {inputs}")
    dataD:dict = {
        'dataD1': {},
        'dataD2': {}
    } 
    print(f"D returned: {dataD}")
    return dataD

class E(MockJob):
    async def run(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        return {'result': f'processed by {self.name}'}

class F(MockJob):
    async def run(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        return {'result': f'processed by {self.name}'}

class G(MockJob):
    async def run(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        return {'result': f'processed by {self.name}'}

class H(MockJob):
    async def run(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        return {'result': f'processed by {self.name}'}

class I(MockJob):
    async def run(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        return {'result': f'processed by {self.name}'}

class J(MockJob):
    async def run(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        return {'result': f'processed by {self.name}'}

jobs = {
    'A': A('A'),
    'B': B('B'),
    'C': C('C'),
    'D': D('D')
}
jobs.update({
    'E': E('E'),
    'F': F('F'),
    'G': G('G'),
    'H': H('H'),
    'I': I('I'),
    'J': J('J'),
})

data:dict = {
    '1': {},
    '2': {}
}

graph_definition1 = {
    'A': {'next': ['B', 'C']},
    'B': {'next': ['C', 'D']},
    'C': {'next': ['D']},
    'D': {'next': []}
} 

graph_definition_complex = {
    'A': {'next': ['B', 'C', 'E']},  # Head job with 3 branches
    'B': {'next': ['D', 'F']},       # Branch 1
    'C': {'next': ['F', 'G']},       # Branch 2
    'D': {'next': ['H']},            # Merge point 1
    'E': {'next': ['G', 'I']},       # Branch 3
    'F': {'next': ['H']},            # Merge point 2
    'G': {'next': ['I']},            # Merge point 3
    'H': {'next': ['J']},            # Pre-final merge
    'I': {'next': ['J']},            # Pre-final merge
    'J': {'next': []}                # Tail job
}

async def execute_graph(graph_definition: dict, jobs: dict, data: dict) -> Any:
    head_job = JobFactory.create_job_graph(graph_definition, jobs)
    job_set = JobABC.job_set(head_job)
    async with job_graph_context_manager(job_set):
        final_result = await head_job._execute(Task(data))
    return final_result

def test_execute_graph1():
    final_result1 = asyncio.run(execute_graph(graph_definition1, jobs, data))
    # Extract just the job result data, ignoring task_pass_through
    result_data = {k: v for k, v in final_result1.items() if k not in ['task_pass_through', 'RETURN_JOB']}
    assert result_data == {
            'dataD1': {},
            'dataD2': {}
        }

def test_job_set():
    head_job = JobFactory.create_job_graph(graph_definition1, jobs)
    job_name_set = head_job.job_set_str()
    assert job_name_set == {'A', 'B', 'C', 'D'}

def test_execute_graph2():
    final_result2 = asyncio.run(execute_graph(graph_definition2, jobs, data))
    # Extract just the job result data, ignoring task_pass_through
    result_data = {k: v for k, v in final_result2.items() if k not in ['task_pass_through', 'RETURN_JOB']}
    assert result_data == {
            'dataC1': {},
            'dataC2': {}
        }
    

graph_definition2 = {
    'A': {'next': ['B', 'C']},
    'B': {'next': ['C']},
    'C': {'next': []},
} 

def test_complex_job_set():
    """
    Test job_set() with a complex graph structure containing:
    - Multiple paths from head to tail
    - Diamond patterns (multiple paths converging)
    - Multiple levels of job dependencies
    Graph structure:
           A
        /  |  \
       B   C   E
      /\  / \  /\
     D  F    G  I
      \ /     \ /
       H       I
        \     /
          J
    """
    head_job = JobFactory.create_job_graph(graph_definition_complex, jobs)
    job_set = head_job.job_set_str()
    expected_jobs = {'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'}
    assert job_set == expected_jobs, f"Expected {expected_jobs}, but got {job_set}"

def test_complex_job_set_instances():
    """
    Test job_set() with a complex graph structure to verify it returns the actual job instances.
    Uses the same graph structure as test_complex_job_set:
           A
        /  |  \
       B   C   E
      /\  / \  /\
     D  F    G  I
      \ /     \ /
       H       I
        \     /
          J
    """
    head_job = JobFactory.create_job_graph(graph_definition_complex, jobs)
    job_instances = JobABC.job_set(head_job)
    
    # Verify we got the correct number of instances
    expected_count = 10  # A through J
    assert len(job_instances) == expected_count, f"Expected {expected_count} job instances, but got {len(job_instances)}"
    
    # Verify all instances are JobABC types
    assert all(isinstance(job, JobABC) for job in job_instances), "All items in job_set should be JobABC instances"
    
    # Verify the job names match what we expect
    job_names = {job.name for job in job_instances}
    expected_names = {'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'}
    assert job_names == expected_names, f"Expected jobs named {expected_names}, but got {job_names}"

def test_multiple_head_nodes():
    """Test that multiple head nodes are handled correctly by creating a DefaultHeadJob."""
    # Test graph with two independent heads
    graph_definition = {
        "A": {"next": ["C"]},
        "B": {"next": ["C"]},
        "C": {"next": []}
    }
    
    job_instances = {
        "A": A("A"),
        "B": B("B"), 
        "C": C("C")
    }
    
    # Create job graph
    head_job = JobFactory.create_job_graph(graph_definition, job_instances)
    
    # Verify default head was created and is correct type
    assert isinstance(head_job, DefaultHeadJob)
    assert head_job.name in job_instances
    
    # Verify graph was updated correctly
    assert head_job.name in graph_definition
    assert set(graph_definition[head_job.name]["next"]) == {"A", "B"}
    
    # Verify next_jobs were set correctly
    assert len(head_job.next_jobs) == 2
    next_job_names = {job.name for job in head_job.next_jobs}
    assert next_job_names == {"A", "B"}

def test_execute_multiple_head_nodes():
    """Test execution of a graph with multiple head nodes."""
    # Create a graph with multiple head nodes
    graph_definition = {
        "A": {"next": ["D"]},
        "B": {"next": ["D"]},
        "C": {"next": ["D"]},
        "D": {"next": []}
    }
    
    # Create job instances
    job_instances = {
        "A": A("A"),
        "B": B("B"),
        "C": C("C"),
        "D": D("D")
    }
    
    # Execute the graph
    data = {"input": "test"}
    final_result = asyncio.run(execute_graph(graph_definition, job_instances, data))
    
    # Extract just the job result data, ignoring task_pass_through
    result_data = {k: v for k, v in final_result.items() if k not in ['task_pass_through', 'RETURN_JOB']}
    
    # Verify final job data is returned (only D's data is in the final result)
    assert result_data == {
        'dataD1': {},
        'dataD2': {}
    }
    
    # Verify the RETURN_JOB is D
    assert final_result['RETURN_JOB'] == 'D'


================================================
File: tests/test_job_loading.py
================================================
import asyncio
import os

import pytest
import yaml

import jobchain.jc_logging as logging
from jobchain.jc_graph import validate_graph
from jobchain.job import JobABC, Task, job_graph_context_manager
from jobchain.job_chain import JobChain  # Import JobChain
from jobchain.job_loader import ConfigLoader, ConfigurationError, JobFactory
from jobchain.jobs.llm_jobs import OpenAIJob

# Test configuration
TEST_CONFIG_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config"))

@pytest.fixture
def job_factory() -> JobFactory:
    factory = JobFactory()
    # Load both the test jobs and the real jobs
    #  the real jobs are always loaded by the factory
    factory.load_python_into_registries([TEST_CONFIG_DIR])
    return factory

def test_job_type_registration(job_factory: JobFactory):
    """Test that all expected job types are registered"""
    # Get all registered job types
    job_types = job_factory._job_types_registry
    
    # Expected job types from test directory
    assert "MockJob" in job_types, "MockJob should be registered"
    assert "MockFileReadJob" in job_types, "MockFileReadJob should be registered"
    assert "MockDatabaseWriteJob" in job_types, "MockDatabaseWriteJob should be registered"
    assert "DummyJob" in job_types, "DummyJob should be registered"
    
    # Expected job type from real jobs directory
    assert "OpenAIJob" in job_types, "OpenAIJob should be registered"


def test_pydantic_type_registration(job_factory: JobFactory):
    """Test that all expected pydantic models are registered"""
    # Configure JobFactory to use test_pydantic_config directory
    test_config_dir = os.path.join(os.path.dirname(__file__), "test_configs/test_pydantic_config")
    JobFactory.load_python_into_registries([test_config_dir])
    
    # Get all registered pydantic types
    pydantic_types = job_factory._pydantic_types_registry
    
    # Expected pydantic models from test directory
    assert "UserProfile" in pydantic_types, "UserProfile model should be registered"
    assert "JobMetadata" in pydantic_types, "JobMetadata model should be registered"
    assert "TaskConfig" in pydantic_types, "TaskConfig model should be registered"
    
    # Verify the registered models are actually pydantic BaseModel subclasses
    from pydantic import BaseModel
    assert issubclass(pydantic_types["UserProfile"], BaseModel)
    assert issubclass(pydantic_types["JobMetadata"], BaseModel)
    assert issubclass(pydantic_types["TaskConfig"], BaseModel)

@pytest.mark.asyncio
async def test_job_instantiation_and_execution(job_factory: JobFactory):
    """Test that jobs can be instantiated and run"""
    # Create a mock job instance
    mock_job = job_factory.create_job(
        name="test_mock_job",
        job_type="MockJob",
        job_def={"properties": {"test_param": "test_value"}}
    )
    
    # Verify job creation
    assert mock_job is not None
    assert mock_job.name == "test_mock_job"
    
    # Run the job with required inputs
    result = await mock_job.run(inputs={"test_input": "test_value"})
    assert result is not None
    assert mock_job.name in result

@pytest.mark.asyncio
async def test_openai_job_instantiation_and_execution(job_factory: JobFactory):
    """Test that OpenAIJob can be instantiated and run"""
    # Get the OpenAIJob class from the registry
    assert "OpenAIJob" in job_factory._job_types_registry, "OpenAIJob should be registered"
    OpenAIJobClass = job_factory._job_types_registry["OpenAIJob"]
    
    openai_job = job_factory.create_job(
        name="test_openai_job",
        job_type="OpenAIJob",
        job_def={
            "properties": {
                "api": {
                    "model": "gpt-4",
                    "temperature": 0.7
                },
                "rate_limit": {
                    "max_rate": 1,
                    "time_period": 4
                }
            }
        }
    )
    
    assert openai_job is not None
    assert openai_job.name == "test_openai_job"
    assert isinstance(openai_job, OpenAIJobClass), "Job should be an instance of OpenAIJob"
    
    # Run the job
    result = await openai_job.run({})
    assert result is not None
    assert "response" in result
    assert isinstance(result["response"], str)
    assert len(result["response"]) > 0

def test_config_loader_separate():
    """Test loading configurations from separate files"""
    # Get absolute paths
    test_config_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config"))
    logging.info(f"\nTest config dir: {test_config_dir}")
    logging.info(f"Directory exists: {os.path.exists(test_config_dir)}")
    logging.info(f"Directory contents: {os.listdir(test_config_dir)}")
    
    # Reset ConfigLoader state and set directories
    ConfigLoader._cached_configs = None  # Reset cached configs
    ConfigLoader._set_directories([str(test_config_dir)])  # Convert to string for anyconfig
    logging.info(f"ConfigLoader directories: {ConfigLoader.directories}")
    
    # Test graphs config
    graphs_config = ConfigLoader.get_graphs_config()
    logging.info(f"Graphs config: {graphs_config}")
    assert graphs_config is not None
    with open(os.path.join(test_config_dir, "graphs.yaml"), 'r') as f:
        expected_graphs = yaml.safe_load(f)
    logging.info(f"Expected graphs: {expected_graphs}")
    assert graphs_config == expected_graphs
    
    # Test jobs config
    jobs_config = ConfigLoader.get_jobs_config()
    assert jobs_config is not None
    with open(os.path.join(test_config_dir, "jobs.yaml"), 'r') as f:
        expected_jobs = yaml.safe_load(f)
    assert jobs_config == expected_jobs
    
    # Test parameters config
    params_config = ConfigLoader.get_parameters_config()
    assert params_config is not None
    with open(os.path.join(test_config_dir, "parameters.yaml"), 'r') as f:
        expected_params = yaml.safe_load(f)
    assert params_config == expected_params
    
    # Validate each graph separately
    for graph_name, graph in graphs_config.items():
        validate_graph(graph, graph_name)

def test_config_loader_all():
    """Test loading configurations from a single combined file"""
    # Get absolute paths
    test_config_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config_all"))
    logging.info(f"\nTest config dir: {test_config_dir}")
    logging.info(f"Directory exists: {os.path.exists(test_config_dir)}")
    logging.info(f"Directory contents: {os.listdir(test_config_dir)}")
    
    # Reset ConfigLoader state and set directories
    ConfigLoader._cached_configs = None  # Reset cached configs
    ConfigLoader._set_directories([str(test_config_dir)])  # Convert to string for anyconfig
    logging.info(f"ConfigLoader directories: {ConfigLoader.directories}")
    
    # Load the combined config file for comparison
    with open(os.path.join(test_config_dir, "jobchain_all.yaml"), 'r') as f:
        all_config = yaml.safe_load(f)
    logging.info(f"All config: {all_config}")
    
    # Test graphs config
    graphs_config = ConfigLoader.get_graphs_config()
    logging.info(f"Graphs config: {graphs_config}")
    assert graphs_config is not None
    assert graphs_config == all_config.get('graphs', {})
    
    # Test jobs config
    jobs_config = ConfigLoader.get_jobs_config()
    logging.info(f"Jobs config: {jobs_config}")
    assert jobs_config is not None
    assert jobs_config == all_config.get('jobs', {})
    
    # Test parameters config
    params_config = ConfigLoader.get_parameters_config()
    logging.info(f"Parameters config: {params_config}")
    assert params_config is not None
    assert params_config == all_config.get('parameters', {})
    
    # Validate each graph separately
    for graph_name, graph in graphs_config.items():
        validate_graph(graph, graph_name)

def test_create_head_jobs_from_config(job_factory: JobFactory):
    """Test that create_head_jobs_from_config creates the correct number of graphs with correct structure"""
    # Set up test config directory
    test_config_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config"))
    logging.info(f"\nTest config dir: {test_config_dir}")
    logging.info(f"Directory exists: {os.path.exists(test_config_dir)}")
    logging.info(f"Directory contents: {os.listdir(test_config_dir)}")
    
    # Reset ConfigLoader state and set directories
    ConfigLoader._cached_configs = None  # Reset cached configs
    ConfigLoader._set_directories([str(test_config_dir)])  # Convert to string for anyconfig

    # Create head jobs
    head_jobs = JobFactory.get_head_jobs_from_config()
    
    # Should create 4 graphs:
    # - 2 from four_stage_parameterized (params1 and params2)
    # - 1 from three_stage (params1)
    # - 1 from three_stage_reasoning (no params)
    assert len(head_jobs) == 4, f"Expected 4 head jobs, got {len(head_jobs)}"
    
    # Get graph definitions for validation
    graphs_config = ConfigLoader.get_graphs_config()
    
    # Validate each head job's structure matches its graph definition
    for i, head_job in enumerate(head_jobs):
        print(f"\nJob Graph {i + 1}:")
        
        # Print all jobs in this graph using DFS
        visited = set()
        def print_job_graph(job):
            if job in visited:
                return
            visited.add(job)
            print(str(job))
            for child in job.next_jobs:
                print_job_graph(child)
        
        print_job_graph(head_job)
        print("----------------------")
        
        # Extract graph name and param group from job name
        job_parts = head_job.name.split("_")
        if len(job_parts) >= 3 and job_parts[0] in graphs_config:
            graph_name = job_parts[0]
            param_group = job_parts[1] if job_parts[1].startswith("params") else None
            
            # Get graph definition
            graph_def = graphs_config[graph_name]
            
            # Validate job structure matches graph definition
            def validate_job_structure(job, graph_def):
                # Get job's base name (without graph and param prefixes)
                base_job_name = "_".join(job.name.split("_")[2:]) if param_group else "_".join(job.name.split("_")[1:])
                
                # Check that next_jobs match graph definition
                expected_next = set(graph_def[base_job_name].get("next", []))
                actual_next = {next_job.name.split("_")[-1] for next_job in job.next_jobs}
                assert expected_next == actual_next, \
                    f"Mismatch in next_jobs for {job.name}. Expected: {expected_next}, Got: {actual_next}"
                
                # Recursively validate next jobs
                for next_job in job.next_jobs:
                    validate_job_structure(next_job, graph_def)
            
            validate_job_structure(head_job, graph_def)

def test_validate_all_jobs_in_graph():
    """Test that validation catches jobs referenced in graphs but not defined in jobs"""
    # Test with invalid configuration
    invalid_config_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config_invalid"))
    ConfigLoader._set_directories([invalid_config_dir])
    
    with pytest.raises(ValueError) as exc_info:
        ConfigLoader.load_all_configs()
    assert "Job 'nonexistent_job' referenced in 'next' field of job 'read_file' in graph 'four_stage_parameterized'" in str(exc_info.value)
    
    # Test with valid configuration
    valid_config_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config"))
    ConfigLoader._set_directories([valid_config_dir])
    
    try:
        ConfigLoader.load_all_configs()
    except ValueError as e:
        pytest.fail(f"Validation failed for valid configuration: {str(e)}")

def test_validate_all_parameters_filled():
    """Test that validation catches missing or invalid parameter configurations"""
    # Test with invalid parameter configuration
    invalid_config_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config_invalid_parameters"))
    ConfigLoader._set_directories([invalid_config_dir])
    
    with pytest.raises(ValueError) as exc_info:
        ConfigLoader.load_all_configs()
    
    error_msg = str(exc_info.value)
    # Should catch missing parameters for read_file in params1
    assert "Job 'read_file' in graph 'four_stage_parameterized' requires parameters {'filepath'} but has no entry in parameter group 'params1'" in error_msg
    
    # Test with valid configuration
    valid_config_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config"))
    ConfigLoader._set_directories([valid_config_dir])
    
    try:
        ConfigLoader.load_all_configs()
    except ValueError as e:
        pytest.fail(f"Validation failed for valid configuration: {str(e)}")



@pytest.mark.asyncio
async def test_job_execution_chain(caplog):
    """Test that all jobs in a graph are executed when _execute is called on the head job."""
    caplog.set_level('DEBUG')  # Set the logging level
    # Load custom job types
    JobFactory.load_python_into_registries([TEST_CONFIG_DIR])

    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config")])
    ConfigLoader.reload_configs()

    # Get head jobs from config
    head_jobs = JobFactory.get_head_jobs_from_config()

    # Get the first head job from four_stage_parameterized_params1
    head_job = [job for job in head_jobs if 'four_stage_parameterized$$params1$$read_file$$' in job.name][0]

    # Execute the head job
    job_set = JobABC.job_set(head_job)
    async with job_graph_context_manager(job_set):
        await asyncio.create_task(head_job._execute(Task({"task": "Test task"})))

    # Expected job names in the graph
    expected_jobs = {
        'four_stage_parameterized$$params1$$read_file$$',
        'four_stage_parameterized$$params1$$ask_llm$$',
        'four_stage_parameterized$$params1$$save_to_db$$',
        'four_stage_parameterized$$params1$$summarize$$'
    }

    # Check that all jobs were executed by verifying their presence in the log output
    for job_name in expected_jobs:
        assert f"{job_name} finished running" in caplog.text, f"Job {job_name} was not executed"

   

@pytest.mark.asyncio
async def test_head_jobs_in_jobchain_serial():
    """Test that head jobs from config can be executed in JobChain with serial processing"""
    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config")])
    
    # Initialize results tracking
    results = []
    
    def result_processor(result):
        results.append(result)
        logging.info(f"Processed result: {result}")
    
    # Create JobChain with serial processing to ensure deterministic results
    job_chain = JobChain(job=None, result_processing_function=result_processor, serial_processing=True)
    
    # Get head jobs from config to know their names
    head_jobs = job_chain.get_job_names()
    
    # Submit tasks for each job
    for job in head_jobs:
        job_chain.submit_task({"task": "Test task"}, job_name=job)
    
    # Mark input as completed and wait for all tasks to finish
    job_chain.mark_input_completed()
    
    # Convert shared list to regular list for sorting
    results_list = list(results)
    
    # Verify results
    # We expect one result per head job
    assert len(results_list) == len(head_jobs), f"Expected {len(head_jobs)} results, got {len(results_list)}"
    
    # Sort results by job name to ensure deterministic ordering
    results_list.sort(key=lambda x: next(iter(x.keys())))
    
    # Each result should be a dictionary with job results
    for result in results_list:
        assert isinstance(result, dict), f"Expected dict result, got {type(result)}"
        
        # Verify parameter substitution for each graph type
        if 'four_stage_parameterized_params1_summarize' in result:
            result_str = str(result['four_stage_parameterized_params1_summarize'])
            # Verify save_to_db parameters
            assert 'postgres://user1:pass1@db1/mydb' in result_str, "Database URL not correctly substituted"
            assert 'table_a' in result_str, "Table name not correctly substituted"
            # Verify read_file parameters
            assert './file1.txt' in result_str, "Filepath not correctly substituted"
            
        elif 'three_stage_params1_summarize' in result:
            result_str = str(result['three_stage_params1_summarize'])
            # Verify save_to_db2 parameters are from the job definition
            assert 'sqlite://user2:pass2@db2/mydb' in result_str, "Database URL not correctly set"
            assert 'table_b' in result_str, "Table name not correctly set"


def process_result(result):
    """Process a result by appending it to the global results list and logging to file"""
    print(f"Got result: {result}")
    # Extract just the job-specific data, excluding task_pass_through and RETURN_JOB
    job_result = {k: v for k, v in result.items() if k not in ['task_pass_through', 'RETURN_JOB']}
    with open("count_parallel_results", "a") as f:
        f.write(str(job_result) + "\n")


@pytest.mark.asyncio
async def test_head_jobs_in_jobchain_parallel():
    """Test that head jobs from config can be executed in JobChain with parallel processing"""
    # Clean up any existing results file
    if os.path.exists("count_parallel_results"):
        os.remove("count_parallel_results")
        
    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config")])
    
    # Create JobChain with parallel processing (default)
    job_chain = JobChain(job=None, result_processing_function=process_result)
    
    # Get head jobs from config to know their names
    head_jobs = job_chain.get_job_names()
    
    # Submit tasks for each job
    for job in head_jobs:
        job_chain.submit_task({"task": "Test task"}, job_name=job)
    
    # Mark input as completed and wait for all processing to finish
    job_chain.mark_input_completed()
    
    # Read results file and verify contents
    with open("count_parallel_results", "r") as f:
        results_list = [eval(line.strip()) for line in f.readlines()]
        assert len(results_list) == 4, f"Expected 4 results but got {len(results_list)}"
        
        # Each result should be a dictionary with job results
        for result in results_list:
            assert isinstance(result, dict), f"Expected dict result, got {type(result)}"
            
            # Verify parameter substitution for each graph type
            if 'four_stage_parameterized_params1_summarize' in result:
                result_str = str(result['four_stage_parameterized_params1_summarize'])
                # Verify save_to_db parameters
                assert 'postgres://user1:pass1@db1/mydb' in result_str, "Database URL not correctly substituted"
                assert 'table_a' in result_str, "Table name not correctly substituted"
                # Verify read_file parameters
                assert './file1.txt' in result_str, "Filepath not correctly substituted"
                
            elif 'four_stage_parameterized_params2_summarize' in result:
                result_str = str(result['four_stage_parameterized_params2_summarize'])
                # Verify save_to_db parameters
                assert 'sqlite://user2:pass2@db2/mydb' in result_str, "Database URL not correctly substituted"
                assert 'table_b' in result_str, "Table name not correctly substituted"
                # Verify read_file parameters
                assert './file2.txt' in result_str, "Filepath not correctly substituted"
                
            elif 'three_stage_params1_summarize' in result:
                result_str = str(result['three_stage_params1_summarize'])
                # Verify save_to_db2 parameters are from the job definition
                assert 'sqlite://user2:pass2@db2/mydb' in result_str, "Database URL not correctly set"
                assert 'table_b' in result_str, "Table name not correctly set"
                
            elif 'three_stage_reasoning__summarize' in result:
                result_str = str(result['three_stage_reasoning__summarize'])
                # Verify save_to_db2 parameters are from the job definition
                assert 'sqlite://user2:pass2@db2/mydb' in result_str, "Database URL not correctly set"
                assert 'table_b' in result_str, "Table name not correctly set"
    
    # Clean up results file
    os.remove("count_parallel_results")

def process_prompts(result):
    """Process a result by appending it to the global results list and logging to file"""
    print(f"Got result: {result}")

@pytest.mark.asyncio
async def test_single_job_multiple_prompts():
    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_single_job")])
    
    # Create JobChain with parallel processing (default)
    job_chain = JobChain(result_processing_function=process_prompts)

    prompts = ["what is the capital of france",
    "what is the capital of germany",
    "what is the capital of the UK",
    "what is the capital of the USA"
    ]

    for prompt in prompts:
        job_chain.submit_task({"prompt": prompt})
    
    # Mark input as completed and wait for all processing to finish
    job_chain.mark_input_completed()

@pytest.mark.asyncio
async def test_malformed_configuration():
    """Test that a malformed configuration file raises a clear error."""
    
    # Test malformed graphs config
    test_config_dir = os.path.join(os.path.dirname(__file__), "test_configs/test_malformed_config")
    ConfigLoader._set_directories([test_config_dir])
    
    with pytest.raises(ConfigurationError) as excinfo:
        ConfigLoader.load_all_configs()
        
    error_msg = str(excinfo.value)
    assert "Configuration is malformed" in error_msg
    assert "test_malformed_config/graphs.yaml" in error_msg
    
    # Test malformed jobs config
    test_config_dir = os.path.join(os.path.dirname(__file__), "test_configs/test_malformed_config_jobs")
    ConfigLoader._set_directories([test_config_dir])
    ConfigLoader._cached_configs = None  # Reset cached configs
    
    with pytest.raises(ConfigurationError) as excinfo:
        ConfigLoader.load_all_configs()
        
    error_msg = str(excinfo.value)
    assert "Configuration is malformed" in error_msg
    assert "test_malformed_config_jobs/jobs.yaml" in error_msg
    
    # Test malformed parameters config
    test_config_dir = os.path.join(os.path.dirname(__file__), "test_configs/test_malformed_config_params")
    ConfigLoader._set_directories([test_config_dir])
    ConfigLoader._cached_configs = None  # Reset cached configs
    
    with pytest.raises(ConfigurationError) as excinfo:
        ConfigLoader.load_all_configs()
        
    error_msg = str(excinfo.value)
    assert "Configuration is malformed" in error_msg
    assert "test_malformed_config_params/parameters.yaml" in error_msg

#@pytest.mark.skip("Skipping test due to working yet")
@pytest.mark.asyncio
async def test_pydantic_jobs_in_jobchain_serial():
    """Test that head jobs from config can be executed in JobChain with serial processing"""
    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_pydantic_config")])

   
    results = []
    
    
    def result_processor(result):
        results.append(result)
        logging.info(f"Processed result: {result}")
    
    # Create JobChain with serial processing to ensure deterministic results
    job_chain = JobChain(job=None, result_processing_function=result_processor, serial_processing=True)
    
    # Get head jobs from config to know their names
    head_jobs = job_chain.get_job_names()
    
    # Submit tasks for each job
    for job in head_jobs:
        job_chain.submit_task({"prompt": "Create a male user."}, job_name=job)
        job_chain.submit_task({"prompt": "Create a female user."}, job_name=job)
    
    # Mark input as completed and wait for all tasks to finish
    job_chain.mark_input_completed()
    
    # Convert shared list to regular list for sorting
    results_list = list(results)
    
    # Verify results
    # We expect one result per head job
    assert len(results_list) == len(head_jobs)*2, f"Expected {len(head_jobs)} results, got {len(results_list)}"
    
    # Sort results by job name to ensure deterministic ordering
    results_list.sort(key=lambda x: next(iter(x.keys())))
    
    # Each result should be a dictionary with job results
    for result in results_list:
        assert isinstance(result, dict), f"Expected dict result, got {type(result)}"
        logging.info(f"Result: {result}")
        

@pytest.mark.asyncio
async def test_multiple_head_jobs_in_jobchain_serial(caplog):
    """Test that multiple head jobs from config can be executed in JobChain with serial processing"""
    # Enable debug logging
    caplog.set_level('DEBUG')
    
    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_multiple_heads")])
    ConfigLoader.reload_configs()
    # Initialize results tracking
    results = []
    
    def result_processor(result):
        results.append(result)
        logging.info(f"Processed result: {result}")
    
    # Create JobChain with serial processing to ensure deterministic results
    job_chain = JobChain(job=None, result_processing_function=result_processor, serial_processing=True)
    
    # Get head jobs from config to know their names
    head_jobs = job_chain.get_job_names()
    logging.info(f"Identified head jobs: {head_jobs}")
    
    # Print head job name for debugging
    if head_jobs:
        logging.info(f"DEBUG - Head job name: {head_jobs[0]}")
    
    # Verify there is exactly one head job (the DefaultHeadJob)
    assert len(head_jobs) == 1, f"Expected exactly one head job, got {len(head_jobs)}: {head_jobs}"
    
    # Get the head job name and verify it's correctly formatted
    head_job_name = head_jobs[0]
    logging.debug(f"Head job name: {head_job_name}")
    parsed_name = JobABC.parse_job_name(head_job_name)
    assert parsed_name == "DefaultHeadJob", \
        f"Parsed name mismatch. Expected 'DefaultHeadJob' got {parsed_name}"
    
    # Submit tasks for each job
    for job in head_jobs:
        logging.info(f"Submitting task for job: {job}")
        job_chain.submit_task({"task": "Multi-head test task"}, job_name=job)
    
    # Mark input as completed and wait for all tasks to finish
    logging.info("Marking input as completed")
    job_chain.mark_input_completed()
    # JobChain automatically waits for completion when mark_input_completed is called
    logging.info("Job chain completed")
    
    # Log the results for debugging
    logging.info(f"Results count: {len(results)}")
    for i, result in enumerate(results):
        logging.info(f"Result {i}: {result}")
    
    # Check if we have any results
    if len(results) == 0:
        logging.error("No results received from job execution")
        assert False, "No results received from job execution"
    
    # Verify that multiple head nodes were detected
    assert "multiple head nodes" in caplog.text.lower(), "Multiple head nodes not detected in log"
    
    # Verify we got at least one result
    assert len(results) > 0, f"Expected at least one result, got {len(results)}"
    
    # Verify the result contains the expected data
    assert "storage_url" in results[0], "Result missing storage_url field"
    assert "status" in results[0], "Result missing status field"
    assert results[0]["status"] == "success", f"Expected status 'success', got '{results[0].get('status')}'"


================================================
File: tests/test_job_name_parsing.py
================================================
import pytest

from jobchain.job import JobABC


def test_parse_graph_name():
    # Valid cases
    assert JobABC.parse_graph_name("four_stage_parameterized$$params1$$read_file$$") == "four_stage_parameterized"
    assert JobABC.parse_graph_name("three_stage_reasoning$$$$ask_llm_reasoning$$") == "three_stage_reasoning"
    
    # Invalid cases
    assert JobABC.parse_graph_name("invalid_name") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_graph_name("$$$$$$") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_graph_name("name$$param$$job") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_graph_name("") == "UNSUPPORTED NAME FORMAT"

def test_parse_param_name():
    # Valid cases
    assert JobABC.parse_param_name("four_stage_parameterized$$params1$$read_file$$") == "params1"
    assert JobABC.parse_param_name("three_stage_reasoning$$$$ask_llm_reasoning$$") == ""
    
    # Invalid cases
    assert JobABC.parse_param_name("invalid_name") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_param_name("$$$$$$") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_param_name("name$$param$$job") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_param_name("") == "UNSUPPORTED NAME FORMAT"

def test_parse_job_name():
    # Valid cases
    assert JobABC.parse_job_name("four_stage_parameterized$$params1$$read_file$$") == "read_file"
    assert JobABC.parse_job_name("three_stage_reasoning$$$$ask_llm_reasoning$$") == "ask_llm_reasoning"
    
    # Invalid cases
    assert JobABC.parse_job_name("invalid_name") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_job_name("$$$$$$") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_job_name("name$$param$$job") == "UNSUPPORTED NAME FORMAT"
    assert JobABC.parse_job_name("") == "UNSUPPORTED NAME FORMAT"

def test_parse_job_loader_name():
    # Valid cases with parameters
    result1 = JobABC.parse_job_loader_name("four_stage_parameterized$$params1$$read_file$$")
    assert result1 == {
        "graph_name": "four_stage_parameterized",
        "param_name": "params1",
        "job_name": "read_file"
    }
    
    # Valid cases without parameters
    result2 = JobABC.parse_job_loader_name("three_stage_reasoning$$$$ask_llm_reasoning$$")
    assert result2 == {
        "graph_name": "three_stage_reasoning",
        "param_name": "",
        "job_name": "ask_llm_reasoning"
    }
    
    # Invalid cases
    invalid_cases = [
        "invalid_name",
        "$$$$$$",
        "name$$param$$job",
        "",
        "name$$param$$job$$extra$$",
        "name$$param$$job$"
    ]
    
    for invalid_case in invalid_cases:
        result = JobABC.parse_job_loader_name(invalid_case)
        assert result == {"parsing_message": "UNSUPPORTED NAME FORMAT"}, f"Failed for case: {invalid_case}"

def test_get_input_from():
    # Create mock input data with realistic job names
    mock_inputs = {
        'four_stage_parameterized$$params1$$read_file$$': {'file_content': 'data1'},
        'four_stage_parameterized$$params1$$ask_llm$$': {'llm_response': 'answer1'},
        'four_stage_parameterized$$params1$$save_to_db$$': {'db_status': 'saved'},
        'four_stage_parameterized$$params1$$summarize$$': {'summary': 'text1'},
        'three_stage$$params1$$ask_llm_mini$$': {'mini_response': 'short_answer'},
        'three_stage_reasoning$$$$ask_llm_reasoning$$': {'reasoning': 'explanation'}
    }

    # Test getting input by short job name
    assert JobABC.get_input_from(mock_inputs, 'read_file') == {'file_content': 'data1'}
    assert JobABC.get_input_from(mock_inputs, 'ask_llm') == {'llm_response': 'answer1'}
    assert JobABC.get_input_from(mock_inputs, 'save_to_db') == {'db_status': 'saved'}
    assert JobABC.get_input_from(mock_inputs, 'summarize') == {'summary': 'text1'}
    assert JobABC.get_input_from(mock_inputs, 'ask_llm_mini') == {'mini_response': 'short_answer'}
    assert JobABC.get_input_from(mock_inputs, 'ask_llm_reasoning') == {'reasoning': 'explanation'}

    # Test non-existent job name returns empty dict
    assert JobABC.get_input_from(mock_inputs, 'non_existent_job') == {}

    # Test with empty inputs dict
    assert JobABC.get_input_from({}, 'read_file') == {}

    # Test that we get the first matching job when multiple jobs have same short name
    # In this case, read_file appears in both params1 and params2
    assert JobABC.get_input_from(mock_inputs, 'read_file') == {'file_content': 'data1'}


================================================
File: tests/test_job_tracing.py
================================================
import inspect
from typing import Any, Dict

from jobchain.job import JobABC, _has_own_traced_execute, _is_traced


class Level1Job(JobABC):
    """First level in the inheritance hierarchy."""
    async def run(self, inputs) -> Dict[str, Any]:
        return {"task": inputs, "level": 1}


class Level2Job(Level1Job):
    """Second level in the inheritance hierarchy."""
    async def run(self, inputs) -> Dict[str, Any]:
        return {"task": inputs, "level": 2}


class Level3Job(Level2Job):
    """Third level in the inheritance hierarchy."""
    async def run(self, inputs) -> Dict[str, Any]:
        return {"task": inputs, "level": 3}


def test_deep_hierarchy_tracing():
    """Test that only JobABC._execute is traced, not its subclasses."""
    # Create instances of each level
    level1_job = Level1Job("Level 1")
    level2_job = Level2Job("Level 2")
    level3_job = Level3Job("Level 3")

    # Count how many classes have their own traced _execute
    def count_own_traced_execute(job):
        count = 0
        cls = job.__class__
        while cls != object:
            if _has_own_traced_execute(cls):
                count += 1
            cls = cls.__base__
        return count

    # Each instance should only have one class with its own traced _execute
    # (JobABC)
    assert count_own_traced_execute(level1_job) == 1, "Should only have one class with own traced _execute"
    assert count_own_traced_execute(level2_job) == 1, "Should only have one class with own traced _execute"
    assert count_own_traced_execute(level3_job) == 1, "Should only have one class with own traced _execute"


class SimpleTestJob(JobABC):
    """A simple Job implementation for testing."""
    async def run(self, inputs) -> Dict[str, Any]:
        return {"task": inputs, "status": "complete"}


def test_abstractjob_execute_is_traced():
    """Test that JobABC's execute method is traced"""
    # Create a test job instance
    job = SimpleTestJob("Test Job")
    
    # Get the actual JobABC class
    abstract_job_cls = JobABC
    
    # Verify JobABC's _execute is traced
    assert _has_own_traced_execute(abstract_job_cls), "JobABC should have its own traced _execute"
    
    # Verify the subclass doesn't have its own traced _execute
    assert not _has_own_traced_execute(job.__class__), "Subclass should not have its own traced _execute"


def test_job_execute_no_trace_available():
    """Test that JobABC subclasses have access to untraced execute via executeNoTrace"""
    job = SimpleTestJob("Test Job")
    assert hasattr(job, 'executeNoTrace'), "Job should have executeNoTrace method"
    assert not _is_traced(job.__class__.executeNoTrace), "executeNoTrace should not be traced"


def test_subclass_execute_not_traced():
    """Test that JobABC subclasses do not have their own traced execute methods"""
    class CustomJob(JobABC):
        async def run(self, inputs) -> Dict[str, Any]:
            return {"task": inputs, "status": "success"}
    
    job = CustomJob("Test Job")
    assert not _has_own_traced_execute(job.__class__), "Subclass should not have its own traced _execute"
    assert hasattr(job, 'executeNoTrace'), "Should still have executeNoTrace method"


def test_decorator_preserves_method_signature():
    """Test that the traced execute method preserves the original signature"""
    class TestJob(JobABC):
        async def run(self, inputs) -> Dict[str, Any]:
            return {"task": inputs, "status": "complete"}
    
    # Get the signatures of the traced and untraced versions
    untraced_sig = inspect.signature(TestJob.executeNoTrace)
    traced_sig = inspect.signature(JobABC._execute)
    
    # Compare parameters and return annotation
    assert str(untraced_sig.parameters) == str(traced_sig.parameters), (
        "The traced execute method should preserve the parameter signature"
    )
    assert untraced_sig.return_annotation == traced_sig.return_annotation, (
        "The traced execute method should preserve the return type annotation"
    )


def test_job_factory_returns_untraced_jobs():
    """Test that JobFactory returns properly untraced Job instances"""
    from tests.test_utils.simple_job import SimpleJobFactory

    # Test file-based job loading
    file_job = SimpleJobFactory.load_job({"type": "file", "params": {}})
    assert isinstance(file_job, JobABC)
    assert not _has_own_traced_execute(file_job.__class__), "Factory-created jobs should not have their own traced _execute"
    assert hasattr(file_job, 'executeNoTrace')
    
    # Test datastore-based job loading
    datastore_job = SimpleJobFactory.load_job({"type": "datastore", "params": {}})
    assert isinstance(datastore_job, JobABC)
    assert not _has_own_traced_execute(datastore_job.__class__), "Factory-created jobs should not have their own traced _execute"
    assert hasattr(datastore_job, 'executeNoTrace')


def test_job_implementations_not_traced():
    """Test that Job implementations do not have their own traced execute methods"""
    def get_all_subclasses(cls):
        """Recursively get all subclasses of a class"""
        subclasses = set()
        for subclass in cls.__subclasses__():
            subclasses.add(subclass)
            subclasses.update(get_all_subclasses(subclass))
        return subclasses
    
    # Get all JobABC subclasses (excluding our test classes)
    job_subclasses = {cls for cls in get_all_subclasses(JobABC)
                     if not cls.__module__.startswith('test_job_tracing')}
    
    # Check each subclass
    traced_classes = []
    for cls in job_subclasses:
        if _has_own_traced_execute(cls):
            traced_classes.append(f"{cls.__module__}.{cls.__name__}")
    
    assert not traced_classes, (
        f"Found Job subclasses with their own traced execute method: "
        f"{', '.join(traced_classes)}\n"
        "Job subclasses should not have their own traced execute method.\n"
        "Only JobABC should have tracing."
    )


def test_execute_no_trace_matches_original():
    """Test that executeNoTrace matches the original implementation"""
    class TestJob(JobABC):
        async def run(self, inputs) -> Dict[str, Any]:
            return {"task": inputs, "result": "success"}
    
    job = TestJob("Test Job")
    
    # Get the source code of both methods
    execute_source = inspect.getsource(job.__class__.executeNoTrace)
    
    # The source code should contain key implementation details
    assert "async def" in execute_source
    assert "_execute(self, inputs" in execute_source  # More flexible check that works with type hints


def test_subclass_execute_not_traced():
    """Test that JobABC subclasses do not have their own traced execute methods"""
    class CustomJob(JobABC):
        async def run(self, inputs) -> Dict[str, Any]:
            return {"task": inputs, "status": "success"}
    
    job = CustomJob("Test Job")
    assert not _has_own_traced_execute(job.__class__), "Subclass should not have its own traced _execute"
    assert hasattr(job, 'executeNoTrace'), "Should still have executeNoTrace method"


def test_decorator_preserves_method_signature():
    """Test that the traced execute method preserves the original signature"""
    class TestJob(JobABC):
        async def run(self, inputs) -> Dict[str, Any]:
            return {"task": inputs, "status": "complete"}
    
    # Get the signatures of the traced and untraced versions
    untraced_sig = inspect.signature(TestJob.executeNoTrace)
    traced_sig = inspect.signature(JobABC._execute)
    
    # Compare parameters and return annotation
    assert str(untraced_sig.parameters) == str(traced_sig.parameters), (
        "The traced execute method should preserve the parameter signature"
    )
    assert untraced_sig.return_annotation == traced_sig.return_annotation, (
        "The traced execute method should preserve the return type annotation"
    )


def test_job_factory_returns_untraced_jobs():
    """Test that JobFactory returns properly untraced Job instances"""
    from tests.test_utils.simple_job import SimpleJobFactory

    # Test file-based job loading
    file_job = SimpleJobFactory.load_job({"type": "file", "params": {}})
    assert isinstance(file_job, JobABC)
    assert not _has_own_traced_execute(file_job.__class__), "Factory-created jobs should not have their own traced _execute"
    assert hasattr(file_job, 'executeNoTrace')
    
    # Test datastore-based job loading
    datastore_job = SimpleJobFactory.load_job({"type": "datastore", "params": {}})
    assert isinstance(datastore_job, JobABC)
    assert not _has_own_traced_execute(datastore_job.__class__), "Factory-created jobs should not have their own traced _execute"
    assert hasattr(datastore_job, 'executeNoTrace')


def test_job_implementations_not_traced():
    """Test that Job implementations do not have their own traced execute methods"""
    def get_all_subclasses(cls):
        """Recursively get all subclasses of a class"""
        subclasses = set()
        for subclass in cls.__subclasses__():
            subclasses.add(subclass)
            subclasses.update(get_all_subclasses(subclass))
        return subclasses
    
    # Get all JobABC subclasses (excluding our test classes)
    job_subclasses = {cls for cls in get_all_subclasses(JobABC)
                     if not cls.__module__.startswith('test_job_tracing')}
    
    # Check each subclass
    traced_classes = []
    for cls in job_subclasses:
        if _has_own_traced_execute(cls):
            traced_classes.append(f"{cls.__module__}.{cls.__name__}")
    
    assert not traced_classes, (
        f"Found Job subclasses with their own traced execute method: "
        f"{', '.join(traced_classes)}\n"
        "Job subclasses should not have their own traced execute method.\n"
        "Only JobABC should have tracing."
    )


def test_execute_no_trace_matches_original():
    """Test that executeNoTrace matches the original implementation"""
    class TestJob(JobABC):
        async def run(self, inputs) -> Dict[str, Any]:
            return {"task": inputs, "result": "success"}
    
    job = TestJob("Test Job")
    
    # Get the source code of both methods
    execute_source = inspect.getsource(job.__class__.executeNoTrace)
    
    # The source code should contain key implementation details
    assert "async def" in execute_source
    assert "_execute(self, task" in execute_source  # More flexible check that works with type hints


================================================
File: tests/test_jobchain_factory.py
================================================
"""
Tests for JobChainFactory class functionality.

This module contains tests that verify the JobChainFactory wrapper over JobChain,
focusing on key functionality from other test modules.
"""

import asyncio
import os
import time
from typing import List

import pytest

import jobchain.jc_logging as logging
from jobchain.job import JobABC
from jobchain.job_chain import JobChainFactory
from jobchain.job_loader import ConfigLoader

# Global results list for picklable result processing
RESULTS: List[dict] = []


def picklable_result_processor(result):
    """A picklable result processor function"""
    RESULTS.append(result)
    logging.info(f"Processing result: {result}")


class AsyncTestJob(JobABC):
    """Job to test async functionality with JobChainFactory"""
    def __init__(self):
        super().__init__(name="AsyncTestJob")
    
    async def run(self, inputs):
        inputs = inputs[self.name]
        if isinstance(inputs, dict) and inputs.get('delay'):
            await asyncio.sleep(inputs['delay'])
        return {'task': inputs, 'completed': True}


class BasicTestJob(JobABC):
    """Simple job for testing basic functionality"""
    def __init__(self, name="BasicTestJob"):
        super().__init__(name=name)
    
    async def run(self, inputs):
        return {self.name: "completed"}


class DelayedJob(JobABC):
    """Job that introduces a configurable delay"""
    def __init__(self, delay: float):
        super().__init__(name="DelayedJob")
        self.delay = delay
    
    async def run(self, inputs):
        logging.info(f"Executing DelayedJob for {inputs} with delay {self.delay}")
        await asyncio.sleep(self.delay)
        return {"task": inputs, "status": "complete"}


class ResultTimingJob(JobABC):
    """Job that records execution timing"""
    def __init__(self):
        super().__init__("ResultTimingJob")
        self.executed_tasks = set()
    
    async def run(self, inputs):
        # Extract the actual task
        actual_task = inputs.get(self.name, inputs)
        # Record task execution
        task_str = str(actual_task)
        self.executed_tasks.add(task_str)
        # Simulate work
        await asyncio.sleep(0.1)
        current_time = time.time()
        logging.info(f"Executing task {actual_task} at {current_time}")
        return {"task": actual_task, "timestamp": current_time}


class UnpicklableState:
    """Class with unpicklable state (file handle)"""
    def __init__(self):
        self.log_file = open('temp.log', 'w')
    
    def __del__(self):
        try:
            self.log_file.close()
            if os.path.exists('temp.log'):
                os.remove('temp.log')
        except:
            pass


@pytest.fixture(autouse=True)
def reset_factory():
    """Reset JobChainFactory between tests"""
    JobChainFactory._instance = None
    JobChainFactory._job_chain = None
    RESULTS.clear()  # Clear global results
    # Clean up temp file if exists
    if os.path.exists('temp.log'):
        os.remove('temp.log')
    yield
    # Clean up after test
    if os.path.exists('temp.log'):
        os.remove('temp.log')


def test_empty_initialization():
    """Test that JobChainFactory is initialized correctly"""
    # Set config directory for test
    ConfigLoader._set_directories([os.path.join(os.path.dirname(__file__), "test_configs/test_jc_config")])
    
    # Create JobChain with serial processing to ensure deterministic results
    JobChainFactory()
    
    # Get head jobs from config to know their names
    head_jobs = sorted(JobChainFactory.get_instance().get_job_names())
    
    # Verify head jobs are loaded - these are the entry point jobs from all graphs and parameter sets
    expected_jobs = sorted([
        'four_stage_parameterized$$params1$$read_file$$',
        'four_stage_parameterized$$params2$$read_file$$',
        'three_stage$$params1$$ask_llm_mini$$',
        'three_stage_reasoning$$$$ask_llm_reasoning$$'
    ])
    
    assert head_jobs == expected_jobs, "JobChain config not loaded correctly"

    JobChainFactory.get_instance().mark_input_completed()


@pytest.mark.asyncio
async def test_concurrent_task_execution():
    """Test that tasks are truly executed concurrently using JobChainFactory"""
    results = []
    
    def collect_result(result):
        results.append(result)
    
    # Initialize factory with a new JobChain
    JobChainFactory(AsyncTestJob(), collect_result, serial_processing=True)
    job_chain = JobChainFactory.get_instance()
    
    # Submit tasks with different delays
    tasks = [
        {'AsyncTestJob': {'task_id': 1, 'delay': 0.2}},
        {'AsyncTestJob': {'task_id': 2, 'delay': 0.1}},
        {'AsyncTestJob': {'task_id': 3, 'delay': 0.3}}
    ]
    
    for task in tasks:
        job_chain.submit_task(task)
    
    job_chain.mark_input_completed()
    
    # Verify all tasks completed
    assert len(results) == 3
    
    # Verify task completion order
    task_ids = [r['task']['task_id'] for r in results]
    assert 2 in task_ids  # Task 2 should be completed
    assert 1 in task_ids
    assert 3 in task_ids


@pytest.mark.asyncio
async def test_job_instantiation_and_execution():
    """Test basic job creation and execution using JobChainFactory"""
    results = []
    
    def collect_result(result):
        results.append(result)
    
    # Create a job chain through the factory
    JobChainFactory(BasicTestJob(), collect_result, serial_processing=True)
    job_chain = JobChainFactory.get_instance()
    
    # Submit a simple task
    job_chain.submit_task({'BasicTestJob': {}})
    job_chain.mark_input_completed()
    
    # Verify job execution
    assert len(results) == 1
    assert results[0]['BasicTestJob'] == "completed"
    
    # Verify we can get the same instance again
    same_job_chain = JobChainFactory.get_instance()
    assert same_job_chain is job_chain


def test_parallel_execution():
    """Test true parallel execution performance using JobChainFactory"""
    async def run_job_chain(delay: float) -> float:
        """Run job chain with specified delay and return execution time"""
        start_time = time.perf_counter()
        
        # Create job chain through factory with picklable result processor
        JobChainFactory(DelayedJob(delay), picklable_result_processor)
        job_chain = JobChainFactory.get_instance()
        
        # Feed 10 tasks with a delay between each to simulate data gathering
        for i in range(10):
            job_chain.submit_task(f"Task {i}")
            await asyncio.sleep(0.2)  # Simulate time taken to gather data
            
        job_chain.mark_input_completed()
        
        execution_time = time.perf_counter() - start_time
        logging.info(f"Execution time for delay {delay}s: {execution_time:.2f}s")
        return execution_time
    
    # Test with 1 second delay
    time_1s = asyncio.run(run_job_chain(1.0))
    
    # Reset factory for second test
    JobChainFactory._instance = None
    JobChainFactory._job_chain = None
    RESULTS.clear()
    
    # Test with 2 second delay
    time_2s = asyncio.run(run_job_chain(2.0))
    
    # Calculate the ratio of execution times
    time_ratio = time_2s / time_1s
    logging.info(f"Time with 1s delay: {time_1s:.2f}s")
    logging.info(f"Time with 2s delay: {time_2s:.2f}s")
    logging.info(f"Ratio: {time_ratio:.2f}x")
    
    # Verify parallel execution
    assert time_1s <= 3.8, (
        f"Expected tasks to complete in ~3.8s (including data gathering + overhead), took {time_1s:.2f}s. "
        "This suggests tasks are running sequentially"
    )
    
    assert time_2s <= 4.8, (
        f"Expected tasks to complete in ~4.8s (including data gathering + overhead), took {time_2s:.2f}s. "
        "This suggests tasks are running sequentially"
    )
    
    assert time_ratio <= 1.5, (
        f"Expected time ratio <= 1.5, got {time_ratio:.2f}. "
        "This suggests tasks are running sequentially instead of in parallel"
    )


def test_serial_result_processor_with_unpicklable():
    """Test serial processing with unpicklable state using JobChainFactory"""
    # Create unpicklable state
    unpicklable = UnpicklableState()
    
    def unpicklable_processor(result):
        """A result processor that uses unpicklable state"""
        unpicklable.log_file.write(f"Processing: {result}\n")
        unpicklable.log_file.flush()
        logging.info(f"Logged result: {result}")
    
    # Test parallel mode (should fail)
    with pytest.raises(TypeError) as exc_info:
        JobChainFactory(ResultTimingJob(), unpicklable_processor, serial_processing=False)
        job_chain = JobChainFactory.get_instance()
        job_chain.submit_task("Task 1")
        job_chain.mark_input_completed()
    assert "pickle" in str(exc_info.value).lower()
    
    # Reset factory for serial mode test
    JobChainFactory._instance = None
    JobChainFactory._job_chain = None
    
    # Test serial mode (should work)
    JobChainFactory(ResultTimingJob(), unpicklable_processor, serial_processing=True)
    job_chain = JobChainFactory.get_instance()
    
    # Submit tasks
    for i in range(3):
        job_chain.submit_task({"ResultTimingJob": f"Task {i}"})
        time.sleep(0.1)
    
    job_chain.mark_input_completed()
    
    # Verify results were logged
    with open('temp.log', 'r') as f:
        log_contents = f.read()
    
    # Check that all tasks were processed
    assert "Task 0" in log_contents
    assert "Task 1" in log_contents
    assert "Task 2" in log_contents


================================================
File: tests/test_jobs.py
================================================
import asyncio
import os
from typing import Any, Dict

import pytest

from jobchain import jc_logging as logging
from jobchain.jobs import OpenAIJob


@pytest.mark.asyncio
async def test_openai_job_async_calls():
    """Test that OpenAIJob can make multiple asynchronous calls to GPT-4o-mini."""
    # Create an OpenAIJob instance
    job = OpenAIJob(properties={
        "api": {
            "model": "gpt-4o-mini",
            "temperature": 0.7
        }
    })
    
    # Test with prompt format
    prompt_tasks = [
        {"prompt": "What is 2+2?"},
        {"prompt": "What is the capital of France?"},
        {"prompt": "What is the color of the sky?"}
    ]
    
    # Record start time
    start_time = asyncio.get_event_loop().time()
    logging.info("Starting async OpenAI calls with prompts")
    
    # Create and gather all tasks
    async_tasks: Dict[str, Any] = []
    for task in prompt_tasks:
        async_tasks.append(asyncio.create_task(job.run(task)))
    
    # Wait for all tasks to complete
    results = await asyncio.gather(*async_tasks)
    
    # Calculate elapsed time
    elapsed_time = asyncio.get_event_loop().time() - start_time
    logging.info(f"All OpenAI prompt calls completed in {elapsed_time:.2f} seconds")
    
    # Verify prompt results
    assert len(results) == 3
    for result in results:
        assert isinstance(result, dict)
        assert ("response" in result) or ("error" in result), "Expected either 'response' or 'error' in result"
        if "response" in result:
            assert isinstance(result["response"], str)
            assert len(result["response"]) > 0

    # Test with messages format
    message_tasks = [
        {"messages": [{"role": "user", "content": "What is 2+2?"}]},
        {"messages": [{"role": "user", "content": "What is the capital of France?"}]},
        {"messages": [{"role": "user", "content": "What is the color of the sky?"}]}
    ]
    
    # Record start time for messages test
    start_time = asyncio.get_event_loop().time()
    logging.info("Starting async OpenAI calls with messages")
    
    # Create and gather all tasks
    async_tasks = []
    for task in message_tasks:
        async_tasks.append(asyncio.create_task(job.run(task)))
    
    # Wait for all tasks to complete
    results = await asyncio.gather(*async_tasks)
    
    # Calculate elapsed time
    elapsed_time = asyncio.get_event_loop().time() - start_time
    logging.info(f"All OpenAI message calls completed in {elapsed_time:.2f} seconds")
    
    # Verify message results
    assert len(results) == 3
    for result in results:
        assert isinstance(result, dict)
        assert ("response" in result) or ("error" in result), "Expected either 'response' or 'error' in result"
        if "response" in result:
            assert isinstance(result["response"], str)
            assert len(result["response"]) > 0


@pytest.mark.asyncio
async def test_rate_limiting():
    """Test that rate limiting configuration is respected."""
    # Create an OpenAIJob instance with strict rate limiting
    job = OpenAIJob(properties={
        "api": {
            "model": "gpt-4o-mini",
            "temperature": 0.7
        },
        "rate_limit": {
            "max_rate": 1,
            "time_period": 4
        }
    })
    
    # Prepare multiple tasks
    tasks = [
        {"prompt": "Count to 1"},
        {"prompt": "Count to 2"},
        {"prompt": "Count to 3"}
    ]
    
    # Record start time
    start_time = asyncio.get_event_loop().time()
    
    # Create and gather all tasks
    async_tasks: Dict[str, Any] = []
    for task in tasks:
        async_tasks.append(asyncio.create_task(job.run(task)))
    
    # Wait for all tasks to complete
    results = await asyncio.gather(*async_tasks)
    
    # Calculate elapsed time
    elapsed_time = asyncio.get_event_loop().time() - start_time
    logging.info(f"Rate limited calls completed in {elapsed_time:.2f} seconds")
    
    # Verify timing - with rate limit of 1 request per 4 seconds, 3 requests should take at least 8 seconds
    # (first request at t=0, second at t=4, third at t=8)
    assert elapsed_time >= 8.0, f"Expected at least 8 seconds, but took {elapsed_time} seconds"
    
    # Verify we got responses (either success or error) for all requests
    assert len(results) == 3
    for result in results:
        assert isinstance(result, dict)
        assert ("response" in result) or ("error" in result), "Expected either 'response' or 'error' in result"


@pytest.mark.asyncio
async def test_openrouter_api():
    """Test that OpenAIJob can make multiple asynchronous calls using OpenRouter API with deepseek model."""
    # Create an OpenAIJob instance with OpenRouter configuration
    job = OpenAIJob(properties={
        "client": {
            "base_url": "https://openrouter.ai/api/v1",
            "api_key": "OPENROUTER_API_KEY"  
        },
        "api": {
            "model": "deepseek/deepseek-chat",
            "temperature": 0.7
        }
    })
    
    # Prepare multiple tasks with proper message format
    tasks = [
        {"messages": [{"role": "user", "content": "What is 2+2?"}]},
        {"messages": [{"role": "user", "content": "What is the capital of France?"}]},
        {"messages": [{"role": "user", "content": "What is the color of the sky?"}]},
        {"prompt": "Count to 1"},
        {"prompt": "Count to 2"},
        {"prompt": "Count to 3"}
    ]
    
    # Record start time
    start_time = asyncio.get_event_loop().time()
    logging.info("Starting async OpenRouter calls")
    
    # Create and gather all tasks
    async_tasks: Dict[str, Any] = []
    for task in tasks:
        async_tasks.append(asyncio.create_task(job.run(task)))
    
    # Wait for all tasks to complete
    results = await asyncio.gather(*async_tasks)
    
    # Calculate elapsed time
    elapsed_time = asyncio.get_event_loop().time() - start_time
    logging.info(f"All OpenRouter calls completed in {elapsed_time:.2f} seconds")
    
    # Verify results
    assert len(results) == 6
    for result in results:
        assert isinstance(result, dict)
        assert ("response" in result) or ("error" in result), "Expected either 'response' or 'error' in result"


================================================
File: tests/test_logging_config.py
================================================
import asyncio
import os

import pytest

from jobchain import jc_logging as logging
from jobchain.job import JobABC, Task
from jobchain.job_chain import JobChain


class DebugDelayedJob(JobABC):
    def __init__(self, name: str, delay: float):
        super().__init__(name)
        self.delay = delay
        self.logger = logging.getLogger(self.__class__.__name__)

    async def run(self, task: Task) -> dict:
        """Execute a delayed job with both debug and info logging."""
        task_str = task.get('task', str(task))  # Get task string or full dict
        self.logger.debug(f"Starting task {task_str} with delay {self.delay}")
        self.logger.info(f"Processing task {task_str}")
        await asyncio.sleep(self.delay)
        self.logger.debug(f"Completed task {task_str}")
        return {"task": dict(task), "status": "complete"}

@pytest.fixture
def clear_log_file():
    """Clear the log file before each test."""
    if os.path.exists('jobchain.log'):
        os.remove('jobchain.log')
    yield
    if os.path.exists('jobchain.log'):
        os.remove('jobchain.log')
    # Clear any environment variables that might affect logging
    os.environ.pop('JOBCHAIN_LOG_HANDLERS', None)
    os.environ.pop('JOBCHAIN_LOG_LEVEL', None)

def test_logging_handlers_default(clear_log_file):
    """Test that by default, logs are only written to console and not to file."""
    logging.setup_logging()
    logger = logging.getLogger('test')
    logger.info('This is a test message')
    
    with open('jobchain.log', 'r') as f:
        lines = f.readlines()
    # Should only contain the header comment
    assert len(lines) == 1, "Log file should only contain header comment"
    assert lines[0].startswith('# JobChain log file'), "Log file should only contain header comment"

def test_logging_handlers_console_explicit(clear_log_file):
    """Test that when JOBCHAIN_LOG_HANDLERS='console', logs are not written to file."""
    os.environ['JOBCHAIN_LOG_HANDLERS'] = 'console'
    logging.setup_logging()
    logger = logging.getLogger('test')
    logger.info('This is a test message')
    
    with open('jobchain.log', 'r') as f:
        lines = f.readlines()
    # Should only contain the header comment
    assert len(lines) == 1, "Log file should only contain header comment"
    assert lines[0].startswith('# JobChain log file'), "Log file should only contain header comment"

def test_logging_handlers_file(clear_log_file):
    """Test that when JOBCHAIN_LOG_HANDLERS includes 'file', logs are written to file."""
    os.environ['JOBCHAIN_LOG_HANDLERS'] = 'console,file'
    logging.setup_logging()
    logger = logging.getLogger('test')
    test_message = 'This should be in the log file'
    logger.info(test_message)
    
    with open('jobchain.log', 'r') as f:
        lines = f.readlines()
    # Should contain both header and log message
    assert len(lines) > 1, "Log file should contain header and log messages"
    assert lines[0].startswith('# JobChain log file'), "First line should be header comment"
    assert any(test_message in line for line in lines[1:]), "Test message should be in log file"

def test_logging_config_debug(clear_log_file):
    """Test that DEBUG level logging works when JOBCHAIN_LOG_LEVEL is set to DEBUG."""
    os.environ['JOBCHAIN_LOG_LEVEL'] = 'DEBUG'
    os.environ['JOBCHAIN_LOG_HANDLERS'] = 'console,file'  # Enable file logging for this test
    logging.setup_logging()  # Reload config with new log level
    
    # Create a logger and log a debug message
    logger = logging.getLogger('test')
    logger.debug('This is a debug message')
    
    # Check if the debug message appears in the log file
    with open('jobchain.log', 'r') as f:
        log_contents = f.read()
    assert 'This is a debug message' in log_contents

def test_logging_config_info(clear_log_file):
    """Test that DEBUG logs are filtered when JOBCHAIN_LOG_LEVEL is set to INFO."""
    os.environ['JOBCHAIN_LOG_LEVEL'] = 'INFO'
    os.environ['JOBCHAIN_LOG_HANDLERS'] = 'console,file'  # Enable file logging for this test
    logging.setup_logging()  # Reload config with new log level
    
    # Create a logger and log messages at different levels
    logger = logging.getLogger('test')
    logger.debug('This is a debug message')
    logger.info('This is an info message')
    
    # Check that only INFO message appears in the log file
    with open('jobchain.log', 'r') as f:
        log_contents = f.read()
    assert 'This is a debug message' not in log_contents
    assert 'This is an info message' in log_contents

def test_debug_logging_in_job_chain(clear_log_file):
    """Test that both JobChain and Job debug logs are visible when JOBCHAIN_LOG_LEVEL=DEBUG."""
    os.environ['JOBCHAIN_LOG_LEVEL'] = 'DEBUG'
    os.environ['JOBCHAIN_LOG_HANDLERS'] = 'console,file'  # Enable file logging for this test
    logging.setup_logging()  # Reload config with new log level

    # Create and run job chain with debug-enabled job
    job = DebugDelayedJob("Debug Test Job", 0.1)
    job_chain = JobChain(job)

    # Submit tasks
    for i in range(3):
        job_chain.submit_task({'task': f'Task {i}'})  # Changed to use dict format
    job_chain.mark_input_completed()

    # Check log file contents
    with open('jobchain.log', 'r') as f:
        log_contents = f.read()
        log_lines = log_contents.splitlines()

    # Separate JobChain and DebugDelayedJob debug logs
    jobchain_debug_logs = [line for line in log_lines if '[DEBUG]' in line and 'JobChain' in line]
    delayed_job_debug_logs = [line for line in log_lines if '[DEBUG]' in line and 'DebugDelayedJob' in line]

    # Verify JobChain has debug logs
    assert len(jobchain_debug_logs) > 0, "No DEBUG logs found from JobChain"
    print("\nJobChain DEBUG logs:")
    for log in jobchain_debug_logs[:3]:  # Print first 3 for verification
        print(log)

    # Verify DebugDelayedJob has debug logs
    assert len(delayed_job_debug_logs) > 0, "No DEBUG logs found from DebugDelayedJob"
    print("\nDebugDelayedJob DEBUG logs:")
    for log in delayed_job_debug_logs[:3]:  # Print first 3 for verification
        print(log)

    # Verify specific debug messages from DebugDelayedJob
    delayed_job_debug_messages = [line.split('] ')[-1] for line in delayed_job_debug_logs]
    assert any('Starting task Task' in msg for msg in delayed_job_debug_messages)
    assert any('Completed task Task' in msg for msg in delayed_job_debug_messages)

    # Verify info logs are also present
    info_logs = [line for line in log_lines if '[INFO]' in line]
    assert any('Processing task Task' in line for line in info_logs)

def test_info_logging_in_job_chain(clear_log_file):
    """Test that DEBUG logs are filtered when JOBCHAIN_LOG_LEVEL=INFO."""
    os.environ['JOBCHAIN_LOG_LEVEL'] = 'INFO'
    os.environ['JOBCHAIN_LOG_HANDLERS'] = 'console,file'  # Enable file logging for this test
    logging.setup_logging()  # Reload config with new log level

    # Create and run job chain with debug-enabled job
    job = DebugDelayedJob("Info Test Job", 0.1)
    job_chain = JobChain(job)

    # Submit tasks
    for i in range(3):
        job_chain.submit_task({'task': f'Task {i}'})  # Changed to use dict format
    job_chain.mark_input_completed()

    # Check log file contents
    with open('jobchain.log', 'r') as f:
        log_contents = f.read()
        log_lines = log_contents.splitlines()

    # Check for absence of DEBUG logs
    debug_logs = [line for line in log_lines if '[DEBUG]' in line]
    assert len(debug_logs) == 0, f"Found unexpected DEBUG logs:\n" + "\n".join(debug_logs[:3])

    # Verify INFO logs are present for both components
    jobchain_info_logs = [line for line in log_lines if '[INFO]' in line and 'JobChain' in line]
    delayed_job_info_logs = [line for line in log_lines if '[INFO]' in line and 'DebugDelayedJob' in line]

    # Verify JobChain info logs
    assert len(jobchain_info_logs) > 0, "No INFO logs found from JobChain"
    print("\nJobChain INFO logs:")
    for log in jobchain_info_logs[:3]:  # Print first 3 for verification
        print(log)

    # Verify DebugDelayedJob info logs
    assert len(delayed_job_info_logs) > 0, "No INFO logs found from DebugDelayedJob"
    print("\nDebugDelayedJob INFO logs:")
    for log in delayed_job_info_logs[:3]:  # Print first 3 for verification
        print(log)

    # Verify specific info messages from DebugDelayedJob
    delayed_job_info_messages = [line.split('] ')[-1] for line in delayed_job_info_logs]
    assert any('Processing task Task' in msg for msg in delayed_job_info_messages)

if __name__ == '__main__':
    pytest.main([__file__])


================================================
File: tests/test_opentelemetry.py
================================================
import asyncio
import json
import os
import time

import pytest
import yaml
from opentelemetry import trace
from opentelemetry.trace import Status, StatusCode

from jobchain import jc_logging as logging
from jobchain.job import JobABC, Task, job_graph_context_manager
from jobchain.utils.otel_wrapper import TracerFactory, trace_function

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@pytest.fixture
def trace_file():
    """Fixture to provide a temporary trace file path and clean up after tests."""
    temp_file = "tests/temp_otel_trace.json"
    yield temp_file
    
    # Clean up the main trace file
    if os.path.exists(temp_file):
        os.unlink(temp_file)
    
    # Clean up any rotated files
    test_dir = os.path.dirname(temp_file)
    base_name = os.path.basename(temp_file)
    rotated_files = [f for f in os.listdir(test_dir) 
                    if f.startswith(base_name + '.')]
    
    for rotated in rotated_files:
        rotated_path = os.path.join(test_dir, rotated)
        if os.path.exists(rotated_path):
            os.unlink(rotated_path)

@pytest.fixture
def setup_file_exporter(trace_file):
    """Fixture to set up file exporter configuration"""
    config_path = "tests/otel_config.yaml"
    
    # Create and write config file
    config = {
        "exporter": "file",
        "service_name": "MyService",
        "batch_processor": {
            "max_queue_size": 1000,
            "schedule_delay_millis": 1000
        },
        "file_exporter": {
            "path": trace_file
        }
    }
    
    with open(config_path, 'w') as f:
        yaml.dump(config, f)
    
    # Reset TracerFactory state and set config path
    TracerFactory._instance = None
    TracerFactory._config = None
    
    # Set config path in environment
    os.environ['JOBCHAIN_OT_CONFIG'] = config_path
    
    yield
    
    # Cleanup
    if os.path.exists(config_path):
        os.unlink(config_path)
    if 'JOBCHAIN_OT_CONFIG' in os.environ:
        del os.environ['JOBCHAIN_OT_CONFIG']
    TracerFactory._instance = None
    TracerFactory._config = None
    time.sleep(0.1)

def verify_trace(trace_file, expected_name=None, expected_attrs=None, expected_status=None, expected_events=None, check_all_spans=False):
    """Helper function to verify trace output"""
    # Wait longer for async operations and file writing
    time.sleep(2.0)  # Increased from 1.0 to 2.0
    # Try a few times in case the file isn't written immediately
    for _ in range(3):
        try:
            with open(trace_file, 'r') as f:
                trace_data = json.load(f)
                assert isinstance(trace_data, list), "Trace data should be a list"
                if len(trace_data) == 0:
                    time.sleep(1.0)  # Wait a bit more if the file is empty
                    continue
                
                if check_all_spans:
                    # Check all spans for the expected attributes
                    spans_to_check = trace_data
                else:
                    # Get the last span (most recent)
                    spans_to_check = [trace_data[-1]]
                
                for span in spans_to_check:
                    # Verify span structure
                    assert 'name' in span, "Span should have a name"
                    assert 'context' in span, "Span should have context"
                    assert 'trace_id' in span['context'], "Span should have trace_id"
                    assert 'span_id' in span['context'], "Span should have span_id"
                    assert 'attributes' in span, "Span should have attributes"
                    
                    # Verify expected name if provided
                    if expected_name and span['name'] == expected_name:
                        # Verify expected attributes if provided
                        if expected_attrs:
                            for key, value in expected_attrs.items():
                                if value is None:
                                    assert key not in span['attributes'], f"Span should not have {key} attribute"
                                else:
                                    if key == "function.args" and "TestJob object at" in str(value):
                                        # For TestJob object, just verify it contains the expected parts
                                        assert "TestJob object at" in span['attributes'][key], f"Expected TestJob object in {key}, got {span['attributes'][key]}"
                                        if "'test task'" in span['attributes'][key]:
                                            assert "'test task'" in span['attributes'][key], "Expected 'test task' in args"
                                    elif key == "object.fields":
                                        # Convert both strings to dicts for comparison, ignoring logger
                                        actual_fields = eval(span['attributes'][key])
                                        expected_fields = eval(value)
                                        for k, v in expected_fields.items():
                                            assert actual_fields[k] == v, f"Mismatch in object.fields for key {k}"
                                    else:
                                        assert key in span['attributes'], f"Expected attribute {key} not found"
                                        assert span['attributes'][key] == value, f"Expected {key}={value}, got {span['attributes'][key]}"
                        
                        # Verify expected status if provided
                        if expected_status:
                            assert 'status' in span, "Span should have status"
                            assert span['status']['status_code'] == expected_status['status_code'], \
                                f"Expected status code {expected_status['status_code']}, got {span['status']['status_code']}"
                            if 'description' in expected_status:
                                expected_desc = expected_status['description']
                                if 'exception_type' in expected_status:
                                    expected_desc = f"{expected_status['exception_type']}: {expected_desc}"
                                assert span['status']['description'] == expected_desc, \
                                    f"Expected status description {expected_desc}, got {span['status']['description']}"
                        
                        # Verify expected events if provided
                        if expected_events:
                            assert 'events' in span, "Span should have events"
                            for event in expected_events:
                                found = False
                                for actual_event in span['events']:
                                    if actual_event['name'] == event['name']:
                                        found = True
                                        if 'attributes' in event:
                                            for key, value in event['attributes'].items():
                                                assert key in actual_event['attributes'], f"Expected event attribute {key} not found"
                                                assert actual_event['attributes'][key] == value, \
                                                    f"Expected event {key}={value}, got {actual_event['attributes'][key]}"
                                assert found, f"Expected event {event['name']} not found"
                return  # Success, exit the function
        except (json.JSONDecodeError, FileNotFoundError, AssertionError) as e:
            if _ == 2:  # On last attempt, raise the error
                raise AssertionError(f"Trace data verification failed after retries: {str(e)}")
            time.sleep(1.0)  # Wait before retrying

def test_trace_function_detailed_off(trace_file, setup_file_exporter):
    """Test that trace_function decorator without detailed_trace doesn't record args"""
    @trace_function
    def sample_function(x, y):
        TracerFactory.trace("Inside sample_function")  # Add a trace to ensure something is captured
        return x + y
    
    result = sample_function(3, 4)
    assert result == 7
    
    # Add a delay to ensure spans are exported
    time.sleep(2)  # Increased delay to ensure export completes
    
    verify_trace(
        trace_file,
        expected_attrs={
            "trace.message": "Inside sample_function",
            "function.args": None,  # Should not have these attributes
            "function.kwargs": None,
            "object.fields": None
        },
        check_all_spans=True  # Check all spans since we have both function and message traces
    )

def test_trace_function_detailed_on(trace_file, setup_file_exporter):
    """Test that trace_function decorator with detailed_trace records args"""
    @trace_function(detailed_trace=True)
    def sample_function(x, y):
        return x + y
    
    result = sample_function(3, 4)
    assert result == 7
    
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.sample_function",
        expected_attrs={
            "function.args": "(3, 4)",
            "function.kwargs": "{}"
        }
    )

def test_tracer_factory_detailed_off(trace_file, setup_file_exporter):
    """Test that TracerFactory.trace without detailed_trace doesn't record args"""
    test_message = "Test message"
    TracerFactory.trace(test_message)
    
    verify_trace(
        trace_file,
        expected_attrs={
            "trace.message": test_message,
            "function.args": None,  # Should not have these attributes
            "function.kwargs": None,
            "object.fields": None
        }
    )

def test_tracer_factory_detailed_on(trace_file, setup_file_exporter):
    """Test that TracerFactory.trace with detailed_trace records args"""
    test_message = "Test message"
    TracerFactory.trace(test_message, detailed_trace=True)
    
    verify_trace(
        trace_file,
        expected_attrs={
            "trace.message": test_message,
            "function.args": "()",
            "function.kwargs": "{}"
        }
    )

def test_job_metaclass_tracing(trace_file, setup_file_exporter):
    """Test that JobABC metaclass properly applies detailed tracing"""
    class TestJob(JobABC):
        async def run(self, inputs):
            # strings are converted to dicts with {'task':,<the string>}
            # Extract just the task data we need
            task_data = inputs['task'] if isinstance(inputs, dict) else inputs
            return {"result": task_data}
            
    async def run_job(job_set):
        async with job_graph_context_manager(job_set):
            result = await job._execute(task)
            return result
    # Create and execute job
    job = TestJob("test")
    task = Task("test task", job.name)
    job_set = JobABC.job_set(job)

    result = asyncio.run(run_job(job_set))
    # Extract just the result field for comparison
    result_data = result.get("result") if isinstance(result, dict) else result
    assert result_data == "test task"
    
    verify_trace(
        trace_file,
        expected_attrs={
            "function.args": "(<test_opentelemetry.test_job_metaclass_tracing.<locals>.TestJob object at",  # Just verify it starts with this
            "function.kwargs": "{}",
            "object.fields": "{'name': 'test'}"
        }
    )

def test_file_exporter_yaml(trace_file, setup_file_exporter):
    """Test file exporter using a temporary yaml config file"""
    test_message = "Test trace message"
    TracerFactory.trace(test_message)
    
    verify_trace(
        trace_file,
        expected_attrs={"trace.message": test_message}
    )

def test_direct_trace_usage(trace_file, setup_file_exporter):
    """Test direct usage of TracerFactory.trace method"""
    test_message = "Test message"
    TracerFactory.trace(test_message)
    
    verify_trace(
        trace_file,
        expected_attrs={"trace.message": test_message}
    )

def test_trace_function_decorator(trace_file, setup_file_exporter):
    """Test that the trace_function decorator properly wraps a function"""
    @trace_function
    def sample_function(x, y):
        return x + y
    
    result = sample_function(3, 4)
    assert result == 7
    
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.sample_function",
        expected_attrs={"function.args": None, "function.kwargs": None}  # Should not have these attributes
    )

def test_class_methods(trace_file, setup_file_exporter):
    """Test that methods within a class can be traced"""
    class SampleClass:
        def __init__(self, value):
            self.value = value
        
        @trace_function(detailed_trace=True)
        def multiply(self, factor):
            return self.value * factor
        
        @trace_function
        def add(self, value):
            return self.value + value
    
    obj = SampleClass(2)
    mult_result = obj.multiply(3)
    add_result = obj.add(4)
    assert mult_result == 6
    assert add_result == 6
    
    # Verify add trace (last operation)
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.add",
        expected_attrs={
            "function.args": None,  # Should not have these attributes
            "function.kwargs": None
        }
    )

def test_nested_tracing(trace_file, setup_file_exporter):
    """Test nested tracing behavior"""
    @trace_function
    def outer_function():
        return inner_function()
    
    @trace_function
    def inner_function():
        return "result"
    
    result = outer_function()
    assert result == "result"
    
    # Verify outer function trace (last operation)
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.outer_function",
        expected_attrs={
            "function.args": None,  # Should not have these attributes
            "function.kwargs": None
        }
    )

def test_exception_handling(trace_file, setup_file_exporter):
    """Test that exceptions are properly recorded in spans"""
    @trace_function(detailed_trace=True)
    def failing_function():
        raise ValueError("Test error")
    
    with pytest.raises(ValueError) as exc_info:
        failing_function()
    assert str(exc_info.value) == "Test error"
    
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.failing_function",
        expected_attrs={
            "function.args": "()",
            "function.kwargs": "{}"
        },
        expected_status={
            "status_code": "StatusCode.ERROR",
            "description": "Test error",
            "exception_type": "ValueError"
        },
        expected_events=[{
            "name": "exception",
            "attributes": {
                "exception.type": "ValueError",
                "exception.message": "Test error"
            }
        }]
    )

def test_parent_child_functions(trace_file, setup_file_exporter):
    """Test that trace context is properly propagated"""
    @trace_function(detailed_trace=True)
    def parent_function():
        current_span = trace.get_current_span()
        current_span.set_attribute("parent.attr", "parent_value")
        return child_function()
    
    @trace_function
    def child_function():
        current_span = trace.get_current_span()
        current_span.set_attribute("child.attr", "child_value")
        return "success"
    
    result = parent_function()
    assert result == "success"
    
    # Verify parent function trace (last operation)
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.parent_function",
        expected_attrs={
            "function.args": "()",
            "function.kwargs": "{}",
            "parent.attr": "parent_value"
        }
    )

def test_trace_with_status(trace_file, setup_file_exporter):
    """Test that span status can be set"""
    @trace_function(detailed_trace=True)
    def status_function(succeed):
        current_span = trace.get_current_span()
        if not succeed:
            current_span.set_status(Status(StatusCode.ERROR, "Operation failed"))
            raise ValueError("Operation failed")
        return "success"
    
    result = status_function(True)
    assert result == "success"
    
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.status_function",
        expected_attrs={
            "function.args": "(True,)",
            "function.kwargs": "{}"
        },
        expected_status={"status_code": "StatusCode.UNSET"}
    )
    
    with pytest.raises(ValueError):
        status_function(False)
    
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.status_function",
        expected_attrs={
            "function.args": "(False,)",
            "function.kwargs": "{}"
        },
        expected_status={
            "status_code": "StatusCode.ERROR",
            "description": "Operation failed",
            "exception_type": "ValueError"
        },
        expected_events=[{
            "name": "exception",
            "attributes": {
                "exception.type": "ValueError",
                "exception.message": "Operation failed"
            }
        }]
    )

def test_trace_function_with_attributes(trace_file, setup_file_exporter):
    """Test that trace_function decorator properly handles additional attributes"""
    @trace_function(attributes={"custom.attr1": "value1", "custom.attr2": 42})
    def sample_function():
        return "success"
    
    result = sample_function()
    assert result == "success"
    
    verify_trace(
        trace_file,
        expected_name="test_opentelemetry.sample_function",
        expected_attrs={
            "custom.attr1": "value1",
            "custom.attr2": "42"  # Note: all attributes are converted to strings
        }
    )

def test_tracer_factory_with_attributes(trace_file, setup_file_exporter):
    """Test that TracerFactory.trace properly handles additional attributes"""
    test_message = "Test message with attributes"
    TracerFactory.trace(test_message, attributes={
        "environment": "test",
        "version": "1.0.0",
        "priority": 1
    })
    
    verify_trace(
        trace_file,
        expected_attrs={
            "trace.message": test_message,
            "environment": "test",
            "version": "1.0.0",
            "priority": "1"  # Note: all attributes are converted to strings
        }
    )

def test_file_exporter_with_rotation_config(trace_file, setup_file_exporter):
    """Test that file exporter works with rotation configuration."""
    # Create config with rotation settings
    config_path = "tests/otel_config.yaml"
    config = {
        "exporter": "file",
        "service_name": "MyService",
        "batch_processor": {
            "max_queue_size": 1000,
            "schedule_delay_millis": 1000
        },
        "file_exporter": {
            "path": trace_file,
            "max_size_bytes": 1048576,  # 1MB
            "rotation_time_days": 1
        }
    }
    
    with open(config_path, 'w') as f:
        yaml.dump(config, f)
    
    # Reset TracerFactory state
    TracerFactory._instance = None
    TracerFactory._config = None
    
    # Generate a trace
    test_message = "Test with rotation config"
    TracerFactory.trace(test_message)
    
    # Wait for async operations
    time.sleep(2)
    
    # Verify file exists and contains the trace
    assert os.path.exists(trace_file), "Trace file should exist"
    
    # Log the file size
    file_size = os.path.getsize(trace_file)
    logger.info("Trace file size after single trace: %d bytes", file_size)
    
    with open(trace_file, 'r') as f:
        data = json.load(f)
        assert isinstance(data, list), "Trace data should be a list"
        assert len(data) > 0, "Trace data should not be empty"
        # Log size of individual trace
        single_trace = data[0]
        trace_json = json.dumps(single_trace)
        logger.info("Size of single trace JSON: %d bytes", len(trace_json))
        assert any(trace['attributes'].get('trace.message') == test_message for trace in data), \
            "Test message should be in trace data"

def test_file_rotation_with_size_limit(trace_file, setup_file_exporter):
    """Test that file rotation occurs when size limit is exceeded."""
    # Create config with small size limit
    config_path = "tests/otel_config.yaml"
    config = {
        "exporter": "file",
        "service_name": "MyService",
        "batch_processor": {
            "max_queue_size": 1000,
            "schedule_delay_millis": 1000
        },
        "file_exporter": {
            "path": trace_file,
            "max_size_bytes": 700,  # Should rotate after two traces
            "rotation_time_days": 1
        }
    }
    
    with open(config_path, 'w') as f:
        yaml.dump(config, f)
    
    # Reset TracerFactory state
    TracerFactory._instance = None
    TracerFactory._config = None
    
    # Generate first trace
    test_message_1 = "First test trace"
    TracerFactory.trace(test_message_1)
    time.sleep(2)  # Wait for async operations
    
    # Verify first trace file exists and contains data
    assert os.path.exists(trace_file), "First trace file should exist"
    with open(trace_file, 'r') as f:
        data = json.load(f)
        assert isinstance(data, list), "Trace data should be a list"
        assert len(data) > 0, "Trace data should not be empty"
        assert any(trace['attributes'].get('trace.message') == test_message_1 for trace in data), \
            "First test message should be in trace data"
    
    # Log size after first trace
    first_size = os.path.getsize(trace_file)
    logger.info("File size after first trace: %d bytes", first_size)
    
    # Generate second trace
    test_message_2 = "Second test trace"
    TracerFactory.trace(test_message_2)
    time.sleep(2)  # Wait for async operations
    
    # Generate third trace - this should trigger rotation
    test_message_3 = "Third test trace"
    TracerFactory.trace(test_message_3)
    time.sleep(2)  # Wait for async operations
    
    # Find rotated files
    test_dir = os.path.dirname(trace_file)
    base_name = os.path.basename(trace_file)
    rotated_files = sorted([f for f in os.listdir(test_dir) 
                    if f.startswith(base_name + '.') and f != base_name])
    
    assert len(rotated_files) >= 1, f"Expected at least 1 rotated file, found {len(rotated_files)}"
    
    # Get the most recent rotated file
    rotated_file = os.path.join(test_dir, rotated_files[-1])
    
    # Verify files exist and contain correct data
    assert os.path.exists(trace_file), "Original trace file should still exist"
    assert os.path.exists(rotated_file), "Rotated file should exist"
    
    # Check contents of current file (should contain third trace)
    with open(trace_file, 'r') as f:
        current_data = json.load(f)
        assert isinstance(current_data, list), "Current file should contain a list"
        assert len(current_data) > 0, "Current file should not be empty"
        assert any(trace['attributes'].get('trace.message') == test_message_3 for trace in current_data), \
            "Third test message should be in current file"
    
    # Log final sizes
    logger.info("Original file final size: %d bytes", os.path.getsize(trace_file))
    logger.info("Most recent rotated file size: %d bytes", os.path.getsize(rotated_file))
    
    # Log all rotated files for debugging
    for rotated in rotated_files:
        full_path = os.path.join(test_dir, rotated)
        logger.info("Rotated file %s size: %d bytes", rotated, os.path.getsize(full_path))


================================================
File: tests/test_parallel_execution.py
================================================
"""
    Tests parallel workloads with some meaningful load in parallel execution mode:
        - test_parallel_execution: makes sure long running tasks are executed in 
            parallel with short running tasks, and that task results are processed in 
            parallel.
        - run_batch_job_chain: simulates running a Job on several batches of tasks.
        - test_parallel_execution_in_batches: runs batches in parallel
"""
import asyncio
import json
import os
import time

import yaml

from jobchain import jc_logging as logging
from jobchain.job import JobABC
from tests.test_utils.simple_job import SimpleJobFactory
from jobchain.job_chain import JobChain
from jobchain.utils.otel_wrapper import TracerFactory

# Configure logging
logging.basicConfig(level=logging.INFO,
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

class DelayedJob(JobABC):
    def __init__(self, name: str, time_delay: float):
        super().__init__(name)
        self.time_delay = time_delay

    async def run(self, inputs) -> dict:
        """Execute a delayed job with tracing."""
        logger.info(f"Executing DelayedJob for {inputs} with delay {self.time_delay}")
        await asyncio.sleep(self.time_delay)  # Use specified delay
        return {"task": inputs, "status": "complete"}

def create_delayed_job(params: dict) -> JobABC:
    time_delay = params.get('time_delay', 1.0)
    return DelayedJob("Test Job", time_delay)

# Store original load_from_file function
original_load_from_file = SimpleJobFactory._load_from_file

def setup_module(module):
    """Set up test environment"""
    SimpleJobFactory._load_from_file = create_delayed_job

def teardown_module(module):
    """Restore original implementation"""
    SimpleJobFactory._load_from_file = original_load_from_file

def dummy_result_processor(result):
    """Dummy function for processing results in tests"""
    logger.info(f"Processing result: {result}")

async def run_job_chain(time_delay: float, use_direct_job: bool = False) -> float:
    """Run job chain with specified delay and return execution time"""
    start_time = time.perf_counter()
    
    if use_direct_job:
        # Create and pass Job instance directly
        job = DelayedJob("Test Job", time_delay)
        job_chain = JobChain(job, dummy_result_processor)
    else:
        # Use traditional dictionary initialization
        job_chain_context = {
                "type": "file",
                "params": {"time_delay": time_delay}
            }
        loaded_job = SimpleJobFactory.load_job(job_chain_context)
        job_chain = JobChain(loaded_job, dummy_result_processor)

    # Feed 10 tasks with a delay between each to simulate data gathering
    for i in range(10):
        job_chain.submit_task(f"Task {i}")
        await asyncio.sleep(0.2)  # Simulate time taken to gather data
    # Indicate there is no more input data to process to initiate shutdown
    job_chain.mark_input_completed()

    execution_time = time.perf_counter() - start_time
    logger.info(f"Execution time for delay {time_delay}s: {execution_time:.2f}s")
    return execution_time

def test_parallel_execution():
    # Test with 1 second delay
    time_1s = asyncio.run(run_job_chain(1.0))
    
    # Test with 2 second delay
    time_2s = asyncio.run(run_job_chain(2.0))
    
    # Calculate the ratio of execution times
    time_ratio = time_2s / time_1s
    logger.info(f"\nTime with 1s delay: {time_1s:.2f}s")
    logger.info(f"Time with 2s delay: {time_2s:.2f}s")
    logger.info(f"Ratio: {time_ratio:.2f}x")
    
    assert time_1s <= 3.8, (
        f"Expected tasks to complete in ~3.8s (including data gathering + overhead), took {time_1s:.2f}s. "
        "This suggests tasks are running sequentially"
    )
    
    assert time_2s <= 4.8, (
        f"Expected tasks to complete in ~4.8s (including data gathering + overhead), took {time_2s:.2f}s. "
        "This suggests tasks are running sequentially"
    )
    
    assert time_ratio <= 1.5, (
        f"Expected time ratio <= 1.5, got {time_ratio:.2f}. "
        "This suggests tasks are running sequentially instead of in parallel"
    )

def test_direct_job_initialization():
    """Test that direct Job instance initialization works equivalently"""
    # Run with dictionary initialization
    time_dict = asyncio.run(run_job_chain(1.0, use_direct_job=False))
    
    # Run with direct Job instance
    time_direct = asyncio.run(run_job_chain(1.0, use_direct_job=True))
    
    # Calculate the ratio of execution times
    time_ratio = abs(time_direct - time_dict) / time_dict
    logger.info(f"\nTime with dict initialization: {time_dict:.2f}s")
    logger.info(f"Time with direct Job instance: {time_direct:.2f}s")
    logger.info(f"Difference ratio: {time_ratio:.2f}")
    
    # The execution times should be very similar (within 10% of each other)
    assert time_ratio <= 0.1, (
        f"Expected similar execution times, but difference ratio was {time_ratio:.2f}. "
        "This suggests the two initialization methods are not equivalent"
    )

async def run_batch_job_chain() -> float:
    """Run job chain with batches of website analysis jobs"""
    start_time = time.perf_counter()
    
    job_chain_context = {
        "type": "file",
        "params": {"time_delay": 0.70}
    }
    loaded_job = SimpleJobFactory.load_job(job_chain_context)
    job_chain = JobChain(loaded_job, dummy_result_processor)

    # Process 4 batches of 25 links each
    for batch in range(4):
        # Simulate scraping 25 links, 1 second per link
        for link in range(25):
            job_chain.submit_task(f"Batch{batch}_Link{link}")
            await asyncio.sleep(0.10)  # Simulate time to scrape each link
    # Indicate there is no more input data to process to initiate shutdown
    job_chain.mark_input_completed()

    execution_time = time.perf_counter() - start_time
    logger.info(f"\nTotal execution time: {execution_time:.2f}s")
    return execution_time

def test_parallel_execution_in_batches():
    """Test parallel execution of website analysis in batches while scraping continues"""
    execution_time = asyncio.run(run_batch_job_chain())
    
    assert execution_time <= 11.8, (
        f"Expected execution to complete in ~11.8s, took {execution_time:.2f}s. "
        "This suggests analysis jobs are not running in parallel with scraping"
    )
    
    assert execution_time >= 9.5, (
        f"Execution completed too quickly in {execution_time:.2f}s. "
        "Expected ~10s for scraping all links"
    )

async def run_job_chain_without_result_processor() -> bool:
    """Run job chain without a result processing function"""
    try:
        job = DelayedJob("Test Job",  0.1)
        job_chain = JobChain(job)  # Pass no result_processing_function

        # Submit a few tasks
        for i in range(3):
            job_chain.submit_task(f"Task {i}")
        # Indicate there is no more input data to process to initiate shutdown
        job_chain.mark_input_completed()
        return True
    except Exception as e:
        logger.error(f"Error occurred: {e}")
        return False

def test_no_result_processor():
    """Test that JobChain works without setting result_processing_function"""
    success = asyncio.run(run_job_chain_without_result_processor())
    assert success, "JobChain should execute successfully without result_processing_function"

async def run_traced_job_chain(time_delay: float) -> float:
    """Run job chain with specified delay and return execution time"""
    start_time = time.perf_counter()
    
    # Use traditional dictionary initialization
    job_chain_context = {
        "type": "file",
        "params": {"time_delay": time_delay}
    }
    # Load job from context
    loaded_job = SimpleJobFactory.load_job(job_chain_context)
    job_chain = JobChain(loaded_job, dummy_result_processor)

    # Feed 10 tasks with a delay between each to simulate data gathering
    for i in range(10):
        job_chain.submit_task(f"Task {i}")
        await asyncio.sleep(0.2)  # Simulate time taken to gather data
    # Indicate there is no more input data to process to initiate shutdown
    job_chain.mark_input_completed()

    execution_time = time.perf_counter() - start_time
    logger.info(f"Execution time for delay {time_delay}s: {execution_time:.2f}s")
    return execution_time

def test_parallel_execution_with_tracing(tmp_path):
    """Test parallel execution with OpenTelemetry tracing enabled"""
    logger.info("Starting parallel execution with tracing test")
    
    # Set up temporary trace file
    trace_file = str(tmp_path / "temp_otel_trace.json")
    logger.info(f"Setting up trace file at: {trace_file}")
    with open(trace_file, 'w') as f:
        json.dump([], f)

    # Set up temporary config file
    config_path = str(tmp_path / "otel_config.yaml")
    config = {
        "exporter": "file",
        "service_name": "JobChainTest",
        "batch_processor": {
            "max_queue_size": 1000,
            "schedule_delay_millis": 1000
        },
        "file_exporter": {
            "path": trace_file
        }
    }
    
    logger.info(f"Writing config to: {config_path}")
    logger.info(f"Config content: {config}")
    with open(config_path, 'w') as f:
        yaml.dump(config, f)

    try:
        # Set environment variable before creating processes
        os.environ['JOBCHAIN_OT_CONFIG'] = config_path
        logger.info(f"Set JOBCHAIN_OT_CONFIG to: {os.environ.get('JOBCHAIN_OT_CONFIG')}")

        # Reset TracerFactory state
        TracerFactory._instance = None
        TracerFactory._config = None

        # Run the job chain
        logger.info("Starting job chain execution")
        execution_time = asyncio.run(run_traced_job_chain(1.0))
        logger.info("Job chain execution completed")

        # Verify execution time
        assert execution_time <= 3.5, (
            f"Expected tasks to complete in ~3.5s (including data gathering + overhead), took {execution_time:.2f}s. "
            "This suggests tasks are running sequentially"
        )

        # Wait for traces to be exported
        logger.info("Waiting for traces to be exported...")
        time.sleep(2.0)

        # Verify traces
        logger.info(f"Reading trace file: {trace_file}")
        with open(trace_file, 'r') as f:
            trace_data = json.load(f)
        logger.info(f"Raw trace data: {json.dumps(trace_data, indent=2)}")
            
        # Count execute method traces
        execute_traces = [
            span for span in trace_data 
            if span['name'].endswith('._execute')
        ]
        logger.info(f"Found {len(execute_traces)} execute traces")
        if len(execute_traces) > 0:
            logger.info("Sample trace names:")
            for trace in execute_traces[:3]:  # Show first 3 traces as sample
                logger.info(f"  - {trace['name']}")
        
        assert len(execute_traces) == 10, (
            f"Expected 10 execute traces (one per task), but found {len(execute_traces)}. "
            "This suggests not all tasks were traced properly."
        )

    finally:
        # Cleanup
        logger.info("Cleaning up test resources")
        if os.path.exists(trace_file):
            os.unlink(trace_file)
        if os.path.exists(config_path):
            os.unlink(config_path)
        if 'JOBCHAIN_OT_CONFIG' in os.environ:
            del os.environ['JOBCHAIN_OT_CONFIG']
        TracerFactory._instance = None
        TracerFactory._config = None


================================================
File: tests/test_parallel_load.py
================================================
import asyncio
import json
import os
import time

import pytest
import yaml

from jobchain import jc_logging as logging
from jobchain.job import JobABC
from jobchain.job_chain import JobChain
from jobchain.utils.otel_wrapper import TracerFactory
from tests.test_utils.simple_job import SimpleJobFactory

# Configure logging
logging.basicConfig(level=logging.INFO,
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

class DelayedJob(JobABC):
    def __init__(self, name: str, time_delay: float):
        super().__init__(name)
        self.time_delay = time_delay

    async def run(self, inputs) -> dict:
        """Execute a delayed job with tracing."""
        logger.info(f"Executing DelayedJob for {inputs} with delay {self.time_delay}")
        await asyncio.sleep(self.time_delay)  # Use specified delay
        return {"task": inputs, "status": "complete"}

def create_delayed_job(params: dict) -> JobABC:
    time_delay = params.get('time_delay', 1.0)
    return DelayedJob("Test Job", time_delay)

# Store original load_from_file function
original_load_from_file = SimpleJobFactory._load_from_file

def setup_module(module):
    """Set up test environment"""
    SimpleJobFactory._load_from_file = create_delayed_job

def teardown_module(module):
    """Restore original implementation"""
    SimpleJobFactory._load_from_file = original_load_from_file

def dummy_result_processor(result):
    """Dummy function for processing results in tests"""
    logger.info(f"Processing result: {result}")

async def run_parallel_load_test(num_tasks: int) -> float:
    """Run a load test with specified number of parallel tasks
    
    Args:
        num_tasks: Number of tasks to run in parallel
        
    Returns:
        float: Total execution time in seconds
    """
    start_time = time.perf_counter()
    
    job_chain_context = {
            "type": "file",
            "params": {"time_delay": 0.01}  # Very small delay for load testing
        }
    loaded_job = SimpleJobFactory.load_job(job_chain_context)
    job_chain = JobChain(loaded_job, dummy_result_processor)

    # Submit all tasks immediately
    for i in range(num_tasks):
        job_chain.submit_task(f"Task_{i}")
    # Indicate there is no more input data to process to initiate shutdown
    job_chain.mark_input_completed()

    execution_time = time.perf_counter() - start_time
    logger.info(f"\nExecution time for {num_tasks} tasks: {execution_time:.2f}s")
    return execution_time

async def run_sustained_load_test(tasks_per_second: int, duration: int) -> tuple[float, float]:
    """Run a sustained load test with consistent task submission rate
    
    Args:
        tasks_per_second: Number of tasks to submit per second
        duration: Duration of the test in seconds
        
    Returns:
        tuple[float, float]: (average_latency, max_latency) in seconds
    """
    job_chain_context = {
        "type": "file",
        "params": {"time_delay": 0.05}  # 50ms baseline processing time
    }
    loaded_job = SimpleJobFactory.load_job(job_chain_context)
    job_chain = JobChain(loaded_job, dummy_result_processor)
    
    start_time = time.perf_counter()
    latencies = []
    
    # Calculate sleep time between tasks to achieve desired rate
    sleep_time = 1.0 / tasks_per_second
    
    # Submit tasks at the specified rate for the specified duration
    task_count = 0
    while time.perf_counter() - start_time < duration:
        task_start = time.perf_counter()
        job_chain.submit_task(f"SustainedTask_{task_count}")
        task_count += 1
        
        # Record latency
        latency = time.perf_counter() - task_start
        latencies.append(latency)
        
        # Sleep to maintain desired rate
        await asyncio.sleep(sleep_time)
    
    # Mark completion and calculate metrics
    job_chain.mark_input_completed()
    avg_latency = sum(latencies) / len(latencies)
    max_latency = max(latencies)
    
    return avg_latency, max_latency

def test_maximum_parallel_execution():
    """Test the maximum theoretical parallel execution capacity
    
    This test verifies the system's ability to handle increasing loads of parallel tasks,
    from 100 to 10,000 tasks. It ensures that execution times stay within expected
    thresholds as the task count increases, demonstrating effective parallel processing.
    
    The test uses progressively larger task counts and adjusts expected completion
    times based on the load, accounting for system overhead and parallel processing
    capabilities.
    
    To run this test:
    Use pytest's --full-suite option: pytest --full-suite
    """
    
    # Test with increasing number of tasks
    task_counts = [100, 500, 2500, 5000, 7500, 10000]
    
    for count in task_counts:
        execution_time = asyncio.run(run_parallel_load_test(count))
        
        # Scale expected time with task count
        if count <= 100:
            expected_time = 2.0
        elif count <= 500:
            expected_time = 4.0
        elif count <= 2500:
            expected_time = 10.0
        elif count <= 5000:
            expected_time = 15.0
        elif count <= 7500:
            expected_time = 20.0
        else:  # 10000 tasks
            expected_time = 25.0
            
        assert execution_time < expected_time, (
            f"Expected {count} tasks to complete in under {expected_time} seconds with parallel execution, "
            f"took {execution_time:.2f}s"
        )
        
        tasks_per_second = count / execution_time
        logger.info(f"Tasks per second with {count} tasks: {tasks_per_second:.2f}")

def test_sustained_load_performance():
    """Test system performance under sustained load
    
    This test verifies the system's ability to handle a consistent stream of tasks
    over time while maintaining acceptable latency. It submits tasks at a fixed rate
    and measures both average and maximum latency to ensure system stability under
    continuous load.
    
    The test runs for a fixed duration, submitting tasks at a specified rate, and
    ensures that latency stays within acceptable bounds throughout the test period.
    
    To run this test:
    Use pytest's --full-suite option: pytest --full-suite
    """
    # Test with moderate sustained load: 10 tasks per second for 30 seconds
    tasks_per_second = 10
    duration = 30
    
    avg_latency, max_latency = asyncio.run(
        run_sustained_load_test(tasks_per_second, duration)
    )
    
    # Verify latency remains within acceptable bounds
    assert avg_latency < 0.1, (
        f"Average latency ({avg_latency:.3f}s) exceeded threshold of 0.1s "
        "under sustained load"
    )
    
    assert max_latency < 0.6, (
        f"Maximum latency ({max_latency:.3f}s) exceeded threshold of 0.5s "
        "under sustained load"
    )
    
    logger.info(f"Sustained load test metrics:")
    logger.info(f"  Average latency: {avg_latency:.3f}s")
    logger.info(f"  Maximum latency: {max_latency:.3f}s")

@pytest.fixture
def trace_file():
    """Fixture to create and clean up a temporary trace file"""
    trace_file = "tests/temp_parallel_trace.json"
    # Initialize trace file
    os.makedirs("tests", exist_ok=True)
    with open(trace_file, 'w') as f:
        json.dump([], f)
    yield trace_file
    # Cleanup
    if os.path.exists(trace_file):
        os.unlink(trace_file)

@pytest.fixture
def setup_file_exporter(trace_file):
    """Fixture to set up file exporter configuration"""
    config_path = "tests/otel_config.yaml"
    
    # Create and write config file
    config = {
        "exporter": "file",
        "service_name": "ParallelLoadTest",
        "batch_processor": {
            "max_queue_size": 10000,  # Increased for parallel load
            "schedule_delay_millis": 100  # Decreased for faster processing
        },
        "file_exporter": {
            "path": trace_file
        }
    }
    
    with open(config_path, 'w') as f:
        yaml.dump(config, f)
    
    # Reset TracerFactory state
    TracerFactory._instance = None
    TracerFactory._config = None
    
    # Set config path in environment
    os.environ['JOBCHAIN_OT_CONFIG'] = config_path
    
    yield
    
    # Cleanup
    if os.path.exists(config_path):
        os.unlink(config_path)
    if 'JOBCHAIN_OT_CONFIG' in os.environ:
        del os.environ['JOBCHAIN_OT_CONFIG']
    TracerFactory._instance = None
    TracerFactory._config = None
    time.sleep(0.1)

def test_maximum_parallel_file_trace(trace_file, setup_file_exporter):
    """Test the impact of file tracing on parallel execution capacity
    
    This test verifies the system's ability to handle increasing loads of parallel tasks
    while simultaneously writing trace data to a file. It helps identify any potential
    bottlenecks in the AsyncFileExporter when under heavy parallel load.
    
    The test uses progressively larger task counts and measures both execution time
    and trace file integrity to ensure proper operation under load.
    
    To run this test:
    Use pytest's --full-suite option: pytest --full-suite
    """
    # Test with increasing number of tasks
    task_counts = [100, 500, 2500, 5000]  # Reduced counts for trace testing
    total_tasks = 0
    
    for count in task_counts:
        execution_time = asyncio.run(run_parallel_load_test(count))
        total_tasks += count
        
        # Scale expected time with task count, allowing extra time for tracing
        if count <= 100:
            expected_time = 3.0  # Additional second for tracing overhead
        elif count <= 500:
            expected_time = 6.0
        elif count <= 2500:
            expected_time = 15.0
        else:  # 5000 tasks
            expected_time = 25.0
            
        assert execution_time < expected_time, (
            f"Expected {count} tasks to complete in under {expected_time} seconds with parallel execution and tracing, "
            f"took {execution_time:.2f}s"
        )
        
        tasks_per_second = count / execution_time
        logger.info(f"Tasks per second with {count} tasks (with tracing): {tasks_per_second:.2f}")
        
        # Verify trace file integrity
        time.sleep(1.0)  # Allow time for traces to be written
        with open(trace_file, 'r') as f:
            trace_data = json.load(f)
            
            # Verify we have traces
            assert len(trace_data) > 0, "No traces were recorded"
            
            # Verify trace structure for a sample of traces
            sample_size = min(10, len(trace_data))
            for trace in trace_data[:sample_size]:
                assert 'name' in trace, "Trace missing name"
                assert 'context' in trace, "Trace missing context"
                assert 'trace_id' in trace['context'], "Trace missing trace_id"
                assert 'span_id' in trace['context'], "Trace missing span_id"
                assert 'attributes' in trace, "Trace missing attributes"
    
    # After all tests complete, verify total number of traces matches total tasks executed
    time.sleep(2.0)  # Additional time to ensure all traces are written
    with open(trace_file, 'r') as f:
        trace_data = json.load(f)
        trace_count = len(trace_data)
        # We expect at least one trace per task (there might be more due to additional instrumentation)
        assert trace_count >= total_tasks, (
            f"Expected at least {total_tasks} traces for all executed tasks, "
            f"but found only {trace_count} traces"
        )
        logger.info(f"Total traces recorded: {trace_count} for {total_tasks} tasks executed")


================================================
File: tests/test_queue_stress.py
================================================
"""
Tests for stress testing queues:

        - Tests high-volume task processing (10,000 tasks)
        - Tests memory pressure handling
        - Tests queue backpressure with slow consumers
        - Tests CPU-intensive workloads
        - Tests mixed workload scenarios
"""
import os
import time

import psutil
import pytest

from jobchain import jc_logging as logging
from jobchain.job import JobABC
from jobchain.job_chain import JobChain


class StressTestJob(JobABC):
    """Job implementation for stress testing queues"""
    def __init__(self):
        super().__init__(name="StressTestJob")
    
    async def run(self, inputs):
        inputs = inputs[self.name] if isinstance(inputs, dict) and self.name in inputs else inputs
        if isinstance(inputs, dict):
            if inputs.get('memory_intensive'):
                # Create temporary large data
                large_data = [i for i in range(inputs.get('size', 1000000))]
                return {'task': inputs, 'data_size': len(large_data)}
            if inputs.get('cpu_intensive'):
                # Perform CPU-intensive calculation
                result = sum(i * i for i in range(inputs.get('iterations', 1000000)))
                return {'task': inputs, 'result': result}
        return {'task': inputs, 'completed': True}

def get_process_memory(pid):
    """Get memory usage of a specific process"""
    try:
        process = psutil.Process(pid)
        return process.memory_info().rss
    except (psutil.NoSuchProcess, psutil.AccessDenied):
        return 0

def test_queue_high_volume():
    """Test queue with high volume of tasks"""
    results = []
    def collect_result(result):
        results.append(result)
    
    job = StressTestJob()
    job_chain = JobChain(job, collect_result, serial_processing=True)
    
    # Submit a high volume of tasks
    num_tasks = 10000
    for i in range(num_tasks):
        job_chain.submit_task({job.name: {'task_id': i}}, job_name=job.name)
    
    job_chain.mark_input_completed()
    
    assert len(results) == num_tasks
    assert len({r['task']['task_id'] for r in results}) == num_tasks

def test_queue_memory_pressure():
    """Test queue behavior under memory pressure"""
    results = []
    def collect_result(result):
        results.append(result)
    
    job = StressTestJob()
    job_chain = JobChain(job, collect_result, serial_processing=True)
    initial_memory = get_process_memory(os.getpid())
    
    # Submit memory-intensive tasks
    num_tasks = 50
    for i in range(num_tasks):
        job_chain.submit_task({
            job.name: {
                'task_id': i,
                'memory_intensive': True,
                'size': 1000000  # 1M integers
            }
        }, job_name=job.name)
    
    job_chain.mark_input_completed()
    
    # Verify all tasks completed
    assert len(results) == num_tasks
    
    # Check memory was properly released
    final_memory = get_process_memory(os.getpid())
    # Allow for some memory overhead, but shouldn't retain all task data
    assert final_memory < initial_memory * 2

def test_queue_backpressure():
    """Test queue backpressure handling with slow consumer"""
    results = []
    def slow_result_processor(result):
        time.sleep(0.01)  # Simulate slow processing
        results.append(result)
    
    job = StressTestJob()
    job_chain = JobChain(job, slow_result_processor, serial_processing=True)
    
    # Submit tasks faster than they can be processed
    num_tasks = 100
    start_time = time.time()
    
    for i in range(num_tasks):
        job_chain.submit_task({job.name: {'task_id': i}}, job_name=job.name)
        if i % 10 == 0:
            time.sleep(0.001)  # Small delay to prevent overwhelming
    
    job_chain.mark_input_completed()
    
    # Verify all tasks eventually complete
    assert len(results) == num_tasks
    # Verify results maintained order
    task_ids = [r['task']['task_id'] for r in results]
    assert sorted(task_ids) == list(range(num_tasks))

def test_queue_cpu_intensive():
    """Test queue behavior with CPU-intensive tasks"""
    results = []
    def collect_result(result):
        results.append(result)
    
    job_chain = JobChain(StressTestJob(), collect_result, serial_processing=True)
    
    # Submit CPU-intensive tasks
    num_tasks = 4  # Number of CPU cores typically available
    for i in range(num_tasks):
        job_chain.submit_task({
            'task_id': i,
            'cpu_intensive': True,
            'iterations': 5000000
        })
    
    job_chain.mark_input_completed()
    
    assert len(results) == num_tasks
    # Verify all tasks produced valid results
    assert all('result' in r for r in results)

def test_queue_mixed_workload():
    """Test queue handling mixed types of workloads"""
    results = []
    def collect_result(result):
        results.append(result)
    
    job = StressTestJob()
    job_chain = JobChain(job, collect_result, serial_processing=True)
    
    # Submit mix of different task types
    tasks = [
        {'task_id': 1, 'memory_intensive': True, 'size': 500000},
        {'task_id': 2, 'cpu_intensive': True, 'iterations': 1000000},
        {'task_id': 3},  # Simple task
        {'task_id': 4, 'memory_intensive': True, 'size': 100000},
        {'task_id': 5, 'cpu_intensive': True, 'iterations': 500000}
    ]
    
    for task in tasks:
        job_chain.submit_task({job.name: task}, job_name=job.name)
    
    job_chain.mark_input_completed()
    
    assert len(results) == len(tasks)
    # Verify each task type completed correctly
    for result in results:
        task = result['task']
        if task.get('memory_intensive'):
            assert 'data_size' in result
        elif task.get('cpu_intensive'):
            assert 'result' in result

if __name__ == '__main__':
    pytest.main(['-v', 'test_queue_stress.py'])


================================================
File: tests/test_result_processing.py
================================================
"""
    Tests parallel and serial functionality with picklable and non-picklable result
        processing functions.
"""

import asyncio
import os
import time
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Dict

import pytest

from jobchain import jc_logging as logging
from jobchain.job import JobABC
from jobchain.job_chain import JobChain


class ResultTimingJob(JobABC):
    def __init__(self):
        super().__init__("Result Timing Job")
        self.executed_tasks = set()

    async def run(self, inputs) -> dict:
        # Extract the actual task from the wrapped task
        actual_task = inputs.get(self.name, inputs)
        # Record task execution using the task string
        if isinstance(actual_task, dict) and 'task' in actual_task:
            task_str = actual_task['task']
        else:
            task_str = str(actual_task)
        self.executed_tasks.add(task_str)
        # Simulate some work
        await asyncio.sleep(0.1)
        current_time = time.time()
        print(f"Executing task {actual_task} at {current_time}")
        return {"task": actual_task, "timestamp": current_time}

class UnpicklableState:
    def __init__(self):
        # Create an unpicklable attribute (file handle)
        self.log_file = open('temp.log', 'w')
    
    def __del__(self):
        try:
            self.log_file.close()
            if os.path.exists('temp.log'):
                os.remove('temp.log')
        except:
            pass

def parallel_mode():
    # Clean up any existing log file
    if os.path.exists('temp.log'):
        os.remove('temp.log')

    print("\nTesting parallel mode (should fail):")
    try:
        # Create unpicklable state
        unpicklable = UnpicklableState()
        
        def unpicklable_processor(result):
            """A result processor that uses unpicklable state"""
            unpicklable.log_file.write(f"Processing: {result}\n")
            unpicklable.log_file.flush()
            print(f"Logged result: {result}")

        # Create job chain in parallel mode
        job = ResultTimingJob()
        job_chain = JobChain(job, unpicklable_processor, serial_processing=False)

        # Submit some tasks
        for i in range(3):
            job_chain.submit_task(f"Task {i}")
            time.sleep(0.1)
        
        job_chain.mark_input_completed()
        assert False, "Expected parallel mode to fail with unpicklable processor"
    except Exception as e:
        print(f"Parallel mode failed as expected: {e}")
        assert "pickle" in str(e).lower(), "Expected pickling error"

def serial_mode():
    print("\nTesting serial mode (should work):")
    try:
        # Clean up any existing log file
        if os.path.exists('temp.log'):
            os.remove('temp.log')
            
        # Create new unpicklable state
        unpicklable = UnpicklableState()
        
        def unpicklable_processor(result):
            """A result processor that uses unpicklable state"""
            unpicklable.log_file.write(f"Processing: {result}\n")
            unpicklable.log_file.flush()
            print(f"Logged result: {result}")

        # Create job chain in serial mode
        job = ResultTimingJob()
        job_chain = JobChain(job, unpicklable_processor, serial_processing=True)

        # Submit some tasks
        expected_tasks = {f"Task {i}" for i in range(3)}
        for task in expected_tasks:
            job_chain.submit_task({job.name: {'task': task}}, job_name=job.name)
            time.sleep(0.1)
        
        # Process tasks and wait for completion
        job_chain.mark_input_completed()  # Not awaited since it's synchronous
        
        # Give a small delay to ensure all results are processed
        time.sleep(0.5)
        
        # Verify results were written to file
        with open('temp.log', 'r') as f:
            content = f.read()
        
        # Verify all tasks were processed by checking log content
        for task in expected_tasks:
            assert f"'task': {{'task': '{task}'}}" in content, f"Expected task {task} to be processed"
        
        print("All tasks were successfully processed and logged")

    except Exception as e:
        assert False, f"Serial mode should not fail: {e}"


def test_parallel_result_processor_fails_with_unpicklable():
    parallel_mode()
    
def test_serial_result_processor_succeeds_with_unpicklable():
    serial_mode()


================================================
File: tests/test_task_passthrough.py
================================================
import json
import os
import tempfile
from functools import partial
from typing import Any, Dict

import pytest

import jobchain.jc_logging as logging
from jobchain.job_chain import JobChain
from jobchain.job_loader import ConfigLoader

test_tasks = [
        {
            'text': 'hello world',
            'task_id': 'task_0',
            'metadata': {
                'original_text': 'hello world',
                'sequence': 0,
                'type': 'text_processing',
                'priority': 'high',
                'tags': ['greeting', 'simple'],
                'timestamp': '2025-01-14T01:42:04Z',
                'nested': {
                    'source': 'user_input',
                    'language': 'en',
                    'confidence': 0.95,
                    'metrics': {
                        'word_count': 2,
                        'char_count': 11,
                        'complexity_score': 0.1
                    }
                }
            },
            'config': {
                'preserve_case': False,
                'max_length': 100,
                'filters': ['lowercase', 'trim']
            }
        },
        {
            'text': 'testing task passthrough',
            'task_id': 'task_1',
            'metadata': {
                'original_text': 'testing task passthrough',
                'sequence': 1,
                'type': 'text_processing',
                'priority': 'medium',
                'tags': ['test', 'complex'],
                'timestamp': '2025-01-14T01:42:04Z',
                'nested': {
                    'source': 'test_suite',
                    'language': 'en',
                    'confidence': 0.99,
                    'metrics': {
                        'word_count': 3,
                        'char_count': 23,
                        'complexity_score': 0.4
                    }
                }
            },
            'config': {
                'preserve_case': True,
                'max_length': 200,
                'filters': ['punctuation', 'normalize']
            }
        },
        {
            'text': 'verify original data',
            'task_id': 'task_2',
            'metadata': {
                'original_text': 'verify original data',
                'sequence': 2,
                'type': 'text_processing',
                'priority': 'low',
                'tags': ['verification', 'data'],
                'timestamp': '2025-01-14T01:42:04Z',
                'nested': {
                    'source': 'validation',
                    'language': 'en',
                    'confidence': 0.85,
                    'metrics': {
                        'word_count': 3,
                        'char_count': 19,
                        'complexity_score': 0.6
                    }
                }
            },
            'config': {
                'preserve_case': True,
                'max_length': 150,
                'filters': ['whitespace', 'special_chars']
            }
        }
    ]

def result_collector(results_file: str, result: Dict[str, Any]) -> None:
    """Collect results by appending to a JSON file.
    
    Args:
        results_file: Path to the file storing results
        result: Result dictionary to append
    """
    logging.info(f"Result collector received: {result}")
    try:
        # Initialize file with empty list if it doesn't exist
        if not os.path.exists(results_file):
            with open(results_file, 'w') as f:
                json.dump([], f)
        
        # Read existing results
        with open(results_file, 'r') as f:
            try:
                results = json.load(f)
            except json.JSONDecodeError:
                # Handle case where file exists but is empty
                results = []
        
        # Transform result to expected format
        transformed_result = result.copy()
        transformed_result['processed_text'] = transformed_result.pop('text')
        
        # Append new result
        results.append(transformed_result)
        
        # Write back all results
        with open(results_file, 'w') as f:
            json.dump(results, f)
            
        logging.info(f"Current results count: {len(results)}")
    except Exception as e:
        logging.error(f"Error collecting result: {e}")
        raise


def test_task_passthrough():
    """Test to verify task parameters are passed through from submit_task to result processing"""
    
    # Create a temporary file for storing results
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as temp_file:
        results_file = temp_file.name
        
        try:
            # Create a partial function with our results file
            collector = partial(result_collector, results_file)
            
            # Set config directory for test
            config_dir = os.path.join(os.path.dirname(__file__), "test_configs/test_task_passthrough")
            ConfigLoader._set_directories([config_dir])
            
            # Create JobChain with parallel processing
            job_chain = JobChain(result_processing_function=collector)
            
            # Submit text processing tasks with unique identifiers
            submitted_tasks = []

            head_jobs = job_chain.get_job_names()
            
            for i, task in enumerate(test_tasks):
                submitted_tasks.append(task)
                logging.info(f"Submitting task: {task}")
                # Use a different graph for each task
                graph_num = i + 1
                job_chain.submit_task(task, job_name=f'text_processing_graph{graph_num}$$$$text_capitalize$$')
            
            # Mark completion and wait for processing
            job_chain.mark_input_completed()
            
            # Read results from file
            with open(results_file, 'r') as f:
                results = json.load(f)
            
            # Verify basic results count
            assert len(results) == len(test_tasks), f"Expected {len(test_tasks)} results, got {len(results)}"
            
            logging.debug("Verifying task pass-through")
            
            for result in results:
                # Verify task_pass_through exists and contains all original task data
                assert 'task_pass_through' in result, "Result missing task_pass_through field"
                task_pass_through = result['task_pass_through']
                
                assert 'task_id' in task_pass_through, "task_id not passed through"
                assert 'metadata' in task_pass_through, "metadata not passed through"
                
                matching_task = next(
                    (t for t in submitted_tasks if t['task_id'] == task_pass_through['task_id']), 
                    None
                )
                assert matching_task is not None, f"No matching submitted task for {task_pass_through['task_id']}"
                
                # Verify all metadata fields are preserved exactly
                assert task_pass_through['metadata'] == matching_task['metadata'], \
                    f"Metadata mismatch for task {task_pass_through['task_id']}"
                
                # Verify nested structures are preserved
                assert task_pass_through['metadata']['nested'] == matching_task['metadata']['nested'], \
                    f"Nested metadata mismatch for task {task_pass_through['task_id']}"
                
                # Verify arrays are preserved
                assert task_pass_through['metadata']['tags'] == matching_task['metadata']['tags'], \
                    f"Tags mismatch for task {task_pass_through['task_id']}"
                
                # Verify metrics are preserved
                assert task_pass_through['metadata']['nested']['metrics'] == matching_task['metadata']['nested']['metrics'], \
                    f"Metrics mismatch for task {task_pass_through['task_id']}"
                
                # Verify config is passed through if present
                if 'config' in matching_task:
                    assert 'config' in task_pass_through, "Config not passed through"
                    assert task_pass_through['config'] == matching_task['config'], \
                        f"Config mismatch for task {task_pass_through['task_id']}"
                
                # Verify processed text field exists
                assert 'processed_text' in result, "Result missing processed_text field"
                
        finally:
            # Clean up temporary file
            try:
                os.unlink(results_file)
            except Exception as e:
                logging.warning(f"Failed to delete temporary file {results_file}: {e}")


def test_multiple_task_submissions():
    """Test submitting each task through each head job multiple times"""
    
    # Create a temporary file for storing results
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as temp_file:
        results_file = temp_file.name
        
        try:
            # Create a partial function with our results file
            collector = partial(result_collector, results_file)
            
            # Set config directory for test
            config_dir = os.path.join(os.path.dirname(__file__), "test_configs/test_task_passthrough")
            ConfigLoader._set_directories([config_dir])
            
            # Create JobChain with parallel processing
            job_chain = JobChain(result_processing_function=collector)
            
            # Get all head jobs
            head_jobs = job_chain.get_job_names()
            
            # Submit text processing tasks with unique identifiers
            num_iterations = 2
            submitted_tasks = []

            # Submit each task through each head job multiple times
            for task in test_tasks:
                for head_job in head_jobs:
                    for iteration in range(num_iterations):
                        # Create a new task with unique ID for this iteration
                        task_copy = task.copy()
                        task_copy['task_id'] = f"{task['task_id']}_head{head_job}_iter{iteration}"
                        submitted_tasks.append(task_copy)
                        logging.info(f"Submitting task {task_copy['task_id']} to head job {head_job}")
                        job_chain.submit_task(task_copy, job_name=head_job)
            
            # Mark completion and wait for processing
            job_chain.mark_input_completed()
            
            # Read results from file
            with open(results_file, 'r') as f:
                results = json.load(f)
            
            # Calculate expected number of tasks
            expected_tasks = len(test_tasks) * len(head_jobs) * num_iterations
            assert len(results) == expected_tasks, f"Expected {expected_tasks} results, got {len(results)}"
            
            logging.debug("Verifying task pass-through for multiple submissions")
            
            for result in results:
                # Verify task_pass_through exists and contains all original task data
                assert 'task_pass_through' in result, "Result missing task_pass_through field"
                task_pass_through = result['task_pass_through']
                
                assert 'task_id' in task_pass_through, "task_id not passed through"
                assert 'metadata' in task_pass_through, "metadata not passed through"
                
                matching_task = next(
                    (t for t in submitted_tasks if t['task_id'] == task_pass_through['task_id']), 
                    None
                )
                assert matching_task is not None, f"No matching submitted task for {task_pass_through['task_id']}"
                
                # Verify metadata fields are preserved exactly
                assert task_pass_through['metadata'] == matching_task['metadata'], \
                    f"Metadata mismatch for task {task_pass_through['task_id']}"
                
                # Verify processed text field exists
                assert 'processed_text' in result, "Result missing processed_text field"
                
        finally:
            # Clean up temporary file
            try:
                os.unlink(results_file)
            except:
                pass


================================================
File: tests/test_configs/test_concurrency_by_returns/graphs.yaml
================================================
test_graph:
  A: {next: [B,C,D]}
  B: {next: [C]}
  C: {next: [E]}
  D: {next: [F]}
  E: {next: [G]}
  F: {next: [G]}
  G: {next: []}

================================================
File: tests/test_configs/test_concurrency_by_returns/jobs.yaml
================================================
A:
  type: ConcurrencyTestJob
  properties:
    test_inputs: []
    valid_return: "A"

B:
  type: ConcurrencyTestJob
  properties:
    test_inputs: [A]
    valid_return: "A.B"

C:
  type: ConcurrencyTestJob
  properties:
    test_inputs: [A,B]
    valid_return: "A.A.B.C"

D:
  type: ConcurrencyTestJob
  properties:
    test_inputs: [A]
    valid_return: "A.D"

E:
  type: ConcurrencyTestJob
  properties:
    test_inputs: [C]
    valid_return: "A.A.B.C.E"

F:
  type: ConcurrencyTestJob
  properties:
    test_inputs: [D]
    valid_return: "A.D.F"

G:
  type: ConcurrencyTestJob
  properties:
    test_inputs: [E,F]
    valid_return: "A.A.B.C.E.A.D.F.G"


================================================
File: tests/test_configs/test_concurrency_by_returns/jobs/concurrent_jobs.py
================================================
import asyncio
from typing import Any, Dict, Optional

import jobchain.jc_logging as logging
from jobchain.job import JobABC


class ConcurrencyTestJob(JobABC):
    def __init__(self, name: Optional[str] = None, properties: Dict[str, Any] = {}):
        super().__init__(name, properties)
        self.test_inputs: list = properties.get("test_inputs", [])
        self.valid_return: str = properties.get("valid_return", "")

    def get_random_sleep_duration(self) -> float:
        """Generate a random sleep duration between 0 and 1 second.
        
        The duration will be randomly selected from one of 10 equal deciles (0.0-0.1, 0.1-0.2, ..., 0.9-1.0)
        with equal probability for each decile.
        
        Returns:
            float: Sleep duration in seconds
        """
        import random
        decile = random.randint(0, 5)  # Choose a decile (0-5)
        base = decile * 0.1  # Base value for the chosen decile
        offset = random.random() * 0.1  # Random offset within the decile
        return base + offset

    async def run(self, inputs):
        await asyncio.sleep(self.get_random_sleep_duration())
        received_data = []
        if not self.is_head_job(): 
            for short_job_name in self.test_inputs: # self.test_input is guaranteed to be in the order it is loaded in
                data = self.get_input_from(inputs,short_job_name)
                if not data:
                    logging.error(f"Failed to get input from {short_job_name}")
                    raise Exception(f"Job {self.name} failed to get input from {short_job_name}")
                received_data.append(data['result']) # return from run() from parent job is a str it is converted to dict.
        task = self.get_task(inputs)
        await asyncio.sleep(self.get_random_sleep_duration())
        short_job_name = self.parse_job_name(self.name)
        received_data.append(f"{short_job_name}")
        return_data = ".".join(received_data)
        await asyncio.sleep(self.get_random_sleep_duration())
        if self.valid_return and return_data != self.valid_return:
            logging.error(f"Invalid return data: {return_data}")
            raise Exception(f"Job {self.name} returned invalid data: {return_data}, should have been {self.valid_return}")

        logging.info(f"Job {self.name} returned: {return_data} for task {task['task']}")
        return return_data


================================================
File: tests/test_configs/test_jc_config/graphs.yaml
================================================
four_stage_parameterized:
    read_file:
      next:
        - ask_llm
        - save_to_db
    ask_llm:
      next:
        - save_to_db
        - summarize
    save_to_db:
      next:
        - summarize
    summarize:
      next: []

three_stage:
    ask_llm_mini:
      next:
        - save_to_db2
        - summarize
    save_to_db2:
      next:
        - summarize
    summarize:
      next: []

three_stage_reasoning:
    ask_llm_reasoning:
      next:
        - save_to_db2
        - summarize
    save_to_db2:
      next:
        - summarize
    summarize:
      next: []

================================================
File: tests/test_configs/test_jc_config/jobs.yaml
================================================
ask_llm:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
    rate_limit:
      max_rate: 1
      time_period: 4

read_file:
  type: MockFileReadJob
  properties:
    filepath: "$filepath"

save_to_db:
  type: MockDatabaseWriteJob
  properties:
    database_url: "$database_url"
    table_name: "$table_name"

summarize:
  type: DummyJob
  properties: {}

ask_llm_mini:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
    rate_limit:
      max_rate: 1
      time_period: 4

ask_llm_reasoning:
  type: OpenAIJob
  properties:
    api:
      model: "o1-mini"
      temperature: 0.7
    rate_limit:
      max_rate: 1
      time_period: 4

save_to_db2:
  type: MockDatabaseWriteJob
  properties:
    database_url: "sqlite://user2:pass2@db2/mydb"
    table_name: "table_b"


================================================
File: tests/test_configs/test_jc_config/parameters.yaml
================================================
four_stage_parameterized: 
    params1:
        ask_llm:
          - model: "gpt-4o"
        read_file:
          - filepath: "./file1.txt"
        save_to_db:
          - database_url: "postgres://user1:pass1@db1/mydb"
            table_name: "table_a"
    params2:
        ask_llm:
          - model: "gpt-4o-mini"
        read_file:
          - filepath: "./file2.txt"
        save_to_db:
          - database_url: "sqlite://user2:pass2@db2/mydb"
            table_name: "table_b"

three_stage:
    params1:
        ask_llm_mini:
          - model: "gpt-4o-mini"

================================================
File: tests/test_configs/test_jc_config/jobs/mock_jobs.py
================================================
from typing import Any, Dict

from jobchain.job import JobABC


class MockFileReadJob(JobABC):
    async def run(self, inputs: Dict[str, Any]) -> Any:
        print(f"\nFileReadJob '{self.name}' reading file: {self.properties.get('filepath')}, inputs:{inputs}")
        file_content = f"Contents of {self.properties.get('filepath')}"
        return {self.name:file_content}

class MockDatabaseWriteJob(JobABC):
    async def run(self, inputs: Dict[str, Any]) -> Any:
        print(f"\nDatabaseWriteJob '{self.name}' writing to: {self.properties.get('database_url')}, table: {self.properties.get('table_name')}, inputs:{inputs}")
        result = f"Data written to table {self.properties.get('table_name')} on db {self.properties.get('database_url')} from {inputs}"
        return {self.name:result}
        
class DummyJob(JobABC):
    async def run(self, inputs: Dict[str, Any]) -> Any:
        print(f"\ndummy_job '{self.name}' with properties: {self.properties}, inputs: {inputs}")
        return {self.name:f"Ran function '{self.name}' with {inputs}"}
    

================================================
File: tests/test_configs/test_jc_config/jobs/mock_jobs2.py
================================================
from typing import Any, Dict

from jobchain.job import JobABC


class MockJob(JobABC):
    async def run(self, inputs: Dict[str, Any]) -> Any:
        print(f"\nMockJob '{self.name}' with properties: {self.properties}, inputs: {inputs}")
        return {self.name:f"Ran function '{self.name}' with {inputs}"}

================================================
File: tests/test_configs/test_jc_config_all/jobchain_all.yaml
================================================
graphs:
  four_stage_parameterized:
    read_file:
      next:
        - ask_llm
        - save_to_db
    ask_llm:
      next:
        - save_to_db
        - summarize
    save_to_db:
      next:
        - summarize
    summarize:
      next: []

  three_stage:
    ask_llm_mini:
      next:
        - save_to_db2
        - summarize
    save_to_db2:
      next:
        - summarize
    summarize:
      next: []

jobs:
  ask_llm:
    type: OpenAIJob
    properties:
      api:
        model: "$model"
        temperature: 0.7
      rate_limit:
        max_rate: 1
        time_period: 4

  read_file:
    type: MockFileReadJob
    properties:
      filepath: "$filepath"

  save_to_db:
    type: MockDatabaseWriteJob
    properties:
      database_url: "$database_url"
      table_name: "$table_name"

  summarize:
    type: DummyJob
    properties: {}

  ask_llm_mini:
    type: OpenAIJob
    properties:
      api:
        model: "$model"
        temperature: 0.7
      rate_limit:
        max_rate: 1
        time_period: 4

  save_to_db2:
    type: MockDatabaseWriteJob
    properties:
      database_url: "sqlite://user2:pass2@db2/mydb"
      table_name: "table_b"

parameters:
  four_stage_parameterized: 
    params1:
        ask_llm:
          - model: "gpt-4o"
        read_file:
          - filepath: "./file1.txt"
        save_to_db:
          - database_url: "postgres://user1:pass1@db1/mydb"
            table_name: "table_a"
    params2:
        ask_llm:
          - model: "gpt-4o-mini"
        read_file:
          - filepath: "./file2.txt"
        save_to_db:
          - database_url: "sqlite://user2:pass2@db2/mydb"
            table_name: "table_b"
  three_stage:
    params1:
        ask_llm_mini:
          - model: "gpt-4o-mini"

================================================
File: tests/test_configs/test_jc_config_all/jobs/mock_jobs.py
================================================
from typing import Any, Dict

from jobchain.job import JobABC


class MockFileReadJob(JobABC):
    async def run(self, inputs: Dict[str, Any]) -> Any:
        print(f"\nFileReadJob '{self.name}' reading file: {self.properties.get('filepath')}, inputs:{inputs}")
        file_content = f"Contents of {self.properties.get('filepath')}"
        return {self.name:file_content}

class MockDatabaseWriteJob(JobABC):
    async def run(self, inputs: Dict[str, Any]) -> Any:
        print(f"\nDatabaseWriteJob '{self.name}' writing to: {self.properties.get('database_url')}, table: {self.properties.get('table_name')}, inputs:{inputs}")
        result = f"Data written to table {self.properties.get('table_name')} on db {self.properties.get('database_url')} from {inputs}"
        return {self.name:result}
        
class DummyJob(JobABC):
    async def run(self, inputs: Dict[str, Any]) -> Any:
        print(f"\ndummy_job '{self.name}' with properties: {self.properties}, inputs: {inputs}")
        return {self.name:f"Ran function '{self.name}' with {inputs}"}
    

================================================
File: tests/test_configs/test_jc_config_all/jobs/mock_jobs2.py
================================================
from typing import Any, Dict

from jobchain.job import JobABC


class MockJob(JobABC):
    async def run(self, inputs: Dict[str, Any]) -> Any:
        print(f"\nMockJob '{self.name}' with properties: {self.properties}, inputs: {inputs}")
        return {self.name:f"Ran function '{self.name}' with {inputs}"}

================================================
File: tests/test_configs/test_jc_config_invalid/graphs.yaml
================================================
four_stage_parameterized:
    read_file:
      next:
        - nonexistent_job  # This job doesn't exist in jobs.yaml
        - save_to_db
    save_to_db:
      next:
        - another_missing_job  # This job doesn't exist either
    summarize:
      next: []

three_stage:
    ask_llm_mini:
      next:
        - undefined_job  # Another missing job
    save_to_db2:
      next:
        - summarize
    summarize:
      next: []


================================================
File: tests/test_configs/test_jc_config_invalid/jobs.yaml
================================================
save_to_db:
  type: MockDatabaseWriteJob
  properties:
    connection_string: "mock://localhost:1234"

save_to_db2:
  type: MockDatabaseWriteJob
  properties:
    connection_string: "mock://localhost:5678"

read_file:
  type: MockFileReadJob
  properties:
    file_path: "/path/to/file"

summarize:
  type: MockJob
  properties:
    test_param: "test_value"


================================================
File: tests/test_configs/test_jc_config_invalid/parameters.yaml
================================================
default_parameters:
  param1: value1
  param2: value2


================================================
File: tests/test_configs/test_jc_config_invalid_parameters/graphs.yaml
================================================
four_stage_parameterized:
    read_file:
      next:
        - ask_llm
        - save_to_db
    ask_llm:
      next:
        - save_to_db
        - summarize
    save_to_db:
      next:
        - summarize
    summarize:
      next: []

three_stage_parameterized:  # This graph has parameterized jobs but no parameters entry
    ask_llm_mini:
      next:
        - save_to_db2
        - summarize
    save_to_db2:
      next:
        - summarize
    summarize:
      next: []


================================================
File: tests/test_configs/test_jc_config_invalid_parameters/jobs.yaml
================================================
ask_llm:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
    rate_limit:
      max_rate: 1
      time_period: 4

read_file:
  type: MockFileReadJob
  properties:
    filepath: "$filepath"

save_to_db:
  type: MockDatabaseWriteJob
  properties:
    database_url: "$database_url"
    table_name: "$table_name"

summarize:
  type: DummyJob
  properties: {}

ask_llm_mini:
  type: OpenAIJob
  properties:
    api:
      model: "$model"  # This job is parameterized but its graph has no parameters entry
      temperature: 0.7
    rate_limit:
      max_rate: 1
      time_period: 4

save_to_db2:
  type: MockDatabaseWriteJob
  properties:
    database_url: "$database_url"  # This job is parameterized but its graph has no parameters entry
    table_name: "$table_name"


================================================
File: tests/test_configs/test_jc_config_invalid_parameters/parameters.yaml
================================================
four_stage_parameterized:  # Missing parameters for some jobs
    params1:  # Missing read_file parameters
        ask_llm:
          - model: "gpt-4o"
        save_to_db:
          - database_url: "postgres://user1:pass1@db1/mydb"
            table_name: "table_a"
    params2:
        ask_llm:
          - model: "gpt-4o-mini"
        save_to_db:
          - database_url: "sqlite://user2:pass2@db2/mydb"
            table_name: "table_b"


================================================
File: tests/test_configs/test_malformed_config/graphs.yaml
================================================
single_stage:
    ask_llm:
      next:[]


================================================
File: tests/test_configs/test_malformed_config/jobs.yaml
================================================
ask_llm:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
    rate_limit:
      max_rate: 10
      time_period: 1


================================================
File: tests/test_configs/test_malformed_config/parameters.yaml
================================================
single_stage:
    params1:
        ask_llm:
          - model: "gpt-4o"

================================================
File: tests/test_configs/test_malformed_config_jobs/graphs.yaml
================================================
single_stage:
    ask_llm:
      next: []


================================================
File: tests/test_configs/test_malformed_config_jobs/jobs.yaml
================================================
ask_llm:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
    rate_limit:
      max_rate: 10
      time_period


================================================
File: tests/test_configs/test_malformed_config_jobs/parameters.yaml
================================================
single_stage:
    params1:
        ask_llm:
          - model: "gpt-4o"

================================================
File: tests/test_configs/test_malformed_config_params/graphs.yaml
================================================
single_stage:
    ask_llm:
      next:[] # this is deliberately malformed, it should error.


================================================
File: tests/test_configs/test_malformed_config_params/jobs.yaml
================================================
ask_llm:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
    rate_limit:
      max_rate: 10
      time_period: 1


================================================
File: tests/test_configs/test_malformed_config_params/parameters.yaml
================================================
single_stage:
    params1:
        ask_llm:
          - model: "gpt-4o"
          key: 


================================================
File: tests/test_configs/test_multiple_heads/graphs.yaml
================================================
multi_head_demo:
    head_job_alpha:  # Head 1
        next: 
            - common_processor
    head_job_beta:  # Head 2
        next:
            - common_processor
    common_processor:  # Shared node
        next:
            - finalizer_job
    finalizer_job:  # Single tail
        next: []


================================================
File: tests/test_configs/test_multiple_heads/jobs.yaml
================================================
head_job_alpha:
    type: DataIngestionJob
    properties:
        source: "$input_path"
        batch: "$batch_size"

head_job_beta:
    type: DataSamplingJob 
    properties:
        source: "$input_path"
        rate: "$sampling_rate"

common_processor:
    type: ModelProcessorJob
    properties:
        model: "$model_name"
        validation_mode: "$validation"

finalizer_job:
    type: ResultArchiverJob
    properties:
        storage_url: "s3://results-bucket/prod"


================================================
File: tests/test_configs/test_multiple_heads/parameters.yaml
================================================
multi_head_demo:
    params1:
        head_job_alpha:
            - input_path: "/data/source_a"
              batch_size: 500
        head_job_beta:
            - input_path: "/data/source_b"
              sampling_rate: 0.25
        common_processor:
            - model_name: "ensemble_v3"
              validation: "strict"


================================================
File: tests/test_configs/test_multiple_heads/jobs/mock_jobs.py
================================================
import jobchain.jc_logging as logging
from jobchain.job import JobABC
from typing import Dict, Any, Union

logger = logging.getLogger(__name__)

class DataIngestionJob(JobABC):
    """Job for ingesting data from a source."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Processing data from {self.properties.get('source')} with batch size {self.properties.get('batch')}")
        result = {
            "source": self.properties.get('source'),
            "batch": self.properties.get('batch'),
            "records_processed": 1000,
            "status": "success"
        }
        logger.info(f"{self.name}: Completed data ingestion with result: {result}")
        return result

class DataSamplingJob(JobABC):
    """Job for sampling data from a source."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Sampling data from {self.properties.get('source')} at rate {self.properties.get('rate')}")
        result = {
            "source": self.properties.get('source'),
            "rate": self.properties.get('rate'),
            "samples_collected": 250,
            "status": "success"
        }
        logger.info(f"{self.name}: Completed data sampling with result: {result}")
        return result

class ModelProcessorJob(JobABC):
    """Job for processing data through a model."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Processing with model {self.properties.get('model')} in {self.properties.get('validation_mode')} mode")
        # Include inputs from upstream jobs in the result
        result = {
            "model": self.properties.get('model'),
            "validation": self.properties.get('validation_mode'),
            "accuracy": 0.92,
            "status": "success"
        }
        logger.info(f"{self.name}: Completed model processing with result: {result}")
        return result

class ResultArchiverJob(JobABC):
    """Job for archiving results."""
    async def run(self, task) -> Dict[str, Any]:
        """Execute the job on the given task."""
        logger.debug(f"{self.name}: Archiving results to {self.properties.get('storage_url')}")
        result = {
            "storage_url": self.properties.get('storage_url'),
            "archived_items": 3,
            "timestamp": "2025-02-26T19:30:00Z",
            "status": "success"
        }
        logger.info(f"{self.name}: Completed archiving with result: {result}")
        return result


================================================
File: tests/test_configs/test_pydantic_config/graphs.yaml
================================================
simple_pydantic:
    ask_llm:
      next: []



================================================
File: tests/test_configs/test_pydantic_config/jobs.yaml
================================================
ask_llm:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
      response_format: "UserProfile"
    rate_limit:
      max_rate: 4
      time_period: 10


================================================
File: tests/test_configs/test_pydantic_config/parameters.yaml
================================================
simple_pydantic: 
    params1:
        ask_llm:
          - model: "gpt-4o"
    params2:
        ask_llm:
          - model: "gpt-4o-mini"

================================================
File: tests/test_configs/test_pydantic_config/jobs/mock_jobs.py
================================================
from typing import Any, Dict

from jobchain.job import JobABC


class MockFileReadJob(JobABC):
    async def run(self, inputs: Dict[str, Any]) -> Any:
        print(f"\nFileReadJob '{self.name}' reading file: {self.properties.get('filepath')}, inputs:{inputs}")
        file_content = f"Contents of {self.properties.get('filepath')}"
        return {self.name:file_content}

class MockDatabaseWriteJob(JobABC):
    async def run(self, inputs: Dict[str, Any]) -> Any:
        print(f"\nDatabaseWriteJob '{self.name}' writing to: {self.properties.get('database_url')}, table: {self.properties.get('table_name')}, inputs:{inputs}")
        result = f"Data written to table {self.properties.get('table_name')} on db {self.properties.get('database_url')} from {inputs}"
        return {self.name:result}
        
class DummyJob(JobABC):
    async def run(self, inputs: Dict[str, Any]) -> Any:
        print(f"\ndummy_job '{self.name}' with properties: {self.properties}, inputs: {inputs}")
        return {self.name:f"Ran function '{self.name}' with {inputs}"}
    

================================================
File: tests/test_configs/test_pydantic_config/jobs/mock_jobs2.py
================================================
from typing import Any, Dict

from jobchain.job import JobABC


class MockJob(JobABC):
    async def run(self, inputs: Dict[str, Any]) -> Any:
        print(f"\nMockJob '{self.name}' with properties: {self.properties}, inputs: {inputs}")
        return {self.name:f"Ran function '{self.name}' with {inputs}"}

================================================
File: tests/test_configs/test_pydantic_config/jobs/pydantic.py
================================================
from pydantic import BaseModel
from typing import List, Optional

class UserProfile(BaseModel):
    """User profile data model"""
    username: str
    email: str
    full_name: str
    age: Optional[int] = None

class JobMetadata(BaseModel):
    """Job metadata model"""
    job_id: str
    status: str
    created_at: str
    updated_at: Optional[str] = None
    tags: List[str] = []

class TaskConfig(BaseModel):
    """Task configuration model"""
    task_name: str
    priority: int
    max_retries: int = 3
    timeout_seconds: int = 300
    dependencies: List[str] = []


================================================
File: tests/test_configs/test_single_job/graphs.yaml
================================================
single_stage:
    ask_llm:
      next: [] 


================================================
File: tests/test_configs/test_single_job/jobs.yaml
================================================
ask_llm:
  type: OpenAIJob
  properties:
    api:
      model: "$model"
      temperature: 0.7
    rate_limit:
      max_rate: 10
      time_period: 1


================================================
File: tests/test_configs/test_single_job/parameters.yaml
================================================
single_stage:
    params1:
        ask_llm:
          - model: "gpt-4o"

================================================
File: tests/test_configs/test_task_passthrough/graphs.yaml
================================================
text_processing_graph1:
    text_capitalize:
      next:
        - text_reverse
    text_reverse:
      next:
        - text_wrap
    text_wrap:
      next: []

text_processing_graph2:
    text_capitalize:
      next:
        - text_reverse
    text_reverse:
      next:
        - text_wrap
    text_wrap:
      next: []

text_processing_graph3:
    text_capitalize:
      next:
        - text_reverse
    text_reverse:
      next:
        - text_wrap
    text_wrap:
      next: []


================================================
File: tests/test_configs/test_task_passthrough/jobs.yaml
================================================
text_capitalize:
  type: TextCapitalizeJob
  properties: {}

text_reverse:
  type: TextReverseJob
  properties: {}

text_wrap:
  type: TextWrapJob
  properties: {}


================================================
File: tests/test_configs/test_task_passthrough/jobs/text_processing_jobs.py
================================================
import asyncio
from typing import Any, Dict, Optional

import jobchain.jc_logging as logging
from jobchain.job import JobABC


#Head Job in test
class TextCapitalizeJob(JobABC):
    """Capitalizes input text and adds stage info"""
    
    def __init__(self, name: Optional[str] = None, properties: Dict[str, Any] = {}):
        super().__init__(name, properties)
    
    async def run(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        task_id = inputs.get('task_id', 'unknown')
        logging.info(f"[TASK_TRACK] TextCapitalizeJob START task_id: {task_id}")
        logging.debug(f"TextCapitalizeJob full task: {inputs}")
        
        input_text = inputs.get('text', '')
        logging.debug(f"TextCapitalizeJob input text: {input_text}")
        
        # Simulate some processing time
        await asyncio.sleep(0.01)
        
        result = {
            'text': input_text.upper(),
            'processing_stage': 'capitalization'
        }
        logging.info(f"[TASK_TRACK] TextCapitalizeJob END task_id: {task_id}")
        logging.debug(f"TextCapitalizeJob result: {result}")
        return result

#Middle job in test
class TextReverseJob(JobABC):
    """Reverses the capitalized text and adds stage info"""
    
    def __init__(self, name: Optional[str] = None, properties: Dict[str, Any] = {}):
        super().__init__(name, properties)
    
    async def run(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        logging.debug(f"TextReverseJob full task: {inputs}")
        
        # Get task data from previous job
        input_data = next(iter(inputs.values())) if inputs else {}
        logging.debug(f"TextReverseJob task_data: {input_data}")
        
        # Check for task_pass_through key in task_data
        task = self.get_task()
        if not task:
            logging.error(f"[TASK_TRACK] TextReverseJob missing task_pass_through")
            raise KeyError("Required key 'task_pass_through' not found in task")
            
        input_text = input_data.get('text', '')
        logging.debug(f"TextReverseJob input text: {input_text}")
        
        # Simulate some processing time
        await asyncio.sleep(0.01)
        
        result = {
            'text': input_text[::-1],
            'processing_stage': 'reversal'
        }
        logging.info(f"[TASK_TRACK] TextReverseJob END")
        logging.debug(f"TextReverseJob result: {result}")
        return result


class TextWrapJob(JobABC):
    """Wraps the text in brackets and adds stage info"""
    
    def __init__(self, name: Optional[str] = None, properties: Dict[str, Any] = {}):
        super().__init__(name, properties)
    
    async def run(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        logging.debug(f"TextWrapJob full task: {inputs}")
        
        # Get task data from previous job
        input_data = next(iter(inputs.values())) if inputs else {}
        logging.debug(f"TextWrapJob task_data: {input_data}")
        
        # Check for task_pass_through key in task_data
        task = self.get_task()
        if not task:
            logging.error(f"[TASK_TRACK] TextWrapJob missing task_pass_through")
            raise KeyError("Required key 'task_pass_through' not found in task")
            
        input_text = input_data.get('text', '')
        logging.debug(f"TextWrapJob input text: {input_text}")
        
        # Simulate some processing time
        await asyncio.sleep(0.01)
        
        result = {
            'text': f"[{input_text}]",
            'processing_stage': 'wrapping'
        }
        logging.info(f"[TASK_TRACK] TextWrapJob END")
        logging.debug(f"TextWrapJob result: {result}")
        return result


================================================
File: tests/test_utils/__init__.py
================================================



================================================
File: tests/test_utils/simple_job.py
================================================
"""
Test utilities for JobChain tests.

This module contains simple job implementations that are used in tests.
These are not intended for production use.
"""
import asyncio
from typing import Any, Dict, Union

from jobchain import jc_logging as logging
from jobchain.job import JobABC, Task


class SimpleJob(JobABC):
    """A Job implementation that provides a simple default behavior."""
    
    async def run(self, task: Union[Dict[str, Any], Task]) -> Dict[str, Any]:
        """Run a simple job that logs and returns the task."""
        logging.info(f"Async JOB for {task}")
        await asyncio.sleep(1)  # Simulate network delay
        return {"task": task, "status": "complete"}


class SimpleJobFactory:
    """Factory class for creating Job instances with proper tracing."""
    
    @staticmethod
    def _load_from_file(params: Dict[str, Any]) -> JobABC:
        """Create a traced job instance from file configuration."""
        logging.info(f"Loading job with params: {params}")
        return SimpleJob("File Job")

    @staticmethod
    def _load_from_datastore(params: Dict[str, Any]) -> JobABC:
        """Create a traced job instance from datastore."""
        logging.info(f"Loading job from datastore with params: {params}")
        return SimpleJob("Datastore Job")

    @staticmethod
    def load_job(job_context: Dict[str, Any]) -> JobABC:
        """Load a job instance with proper tracing based on context."""
        load_type = job_context.get("type", "").lower()
        params = job_context.get("params", {})

        if load_type == "file":
            return SimpleJobFactory._load_from_file(params)
        elif load_type == "datastore":
            return SimpleJobFactory._load_from_datastore(params)
        else:
            raise ValueError(f"Unsupported job type: {load_type}")


